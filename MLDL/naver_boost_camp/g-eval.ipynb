{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation, retrieval 나누지 않고 한번에 진행용 프롬프트\n",
    "SYSTEM_PROMPT = \"\"\"You are an evaluation assistant for a RAG (Retrieval-Augmented Generation) system.\n",
    "Your task is to score the retrieved context(s) and the final answer according to the criteria provided below.\n",
    "\n",
    "### Input:\n",
    "- `question`: string = Field(description=\"A single user question\")\n",
    "- `ground_truth_context`: list = Field(description=\"Ground Truth (GT), usually one context\")\n",
    "- `context`: list = Field(description=\"list of retrieved contexts\")\n",
    "- `ground_truth_answer`: string = Field(description=\"The correct answer to the question\")\n",
    "- `answer`: string = Field(description=\"The final answer generated by the system\")\n",
    "\n",
    "### Output:\n",
    "- `retrieval`: list = Field(description=\"A list of 5 scores (0 or 1) for the retrieval evaluation criteria.\")\n",
    "- `generation`: list = Field(description=\"A list of 9 scores (0 or 1) for the generation evaluation criteria.\")\n",
    "\n",
    "You must return a JSON object with two keys: \"retrieval\" and \"generation\".\n",
    "- \"retrieval\" is a list of 5 scores (1 or 0).\n",
    "- \"generation\" is a list of 9 scores (1 or 0).\n",
    "No additional text or explanation is allowed.\n",
    "\n",
    "### Retrieval Criteria (1~5):\n",
    "1) Do any of the retrieved contexts show strong similarity to the Ground Truth? (1 or 0)\n",
    "2) Do the retrieved contexts collectively capture essential information from the Ground Truth? (1 or 0)\n",
    "3) Do the retrieved contexts sufficiently address the user’s query? (1 or 0)\n",
    "4) Are all retrieved contexts relevant to the Ground Truth or the user’s query? (1 or 0)\n",
    "5) Does the combined length and number of retrieved contexts remain reasonable without overwhelming the user with excessive or irrelevant details?  \n",
    "\n",
    "### Generation Criteria (6~14):\n",
    "6) Is the final answer clearly relevant to the question and reflective of the user’s intent? (1 or 0)\n",
    "7) Is the answer factually correct and free from unsupported or inaccurate information? (1 or 0)\n",
    "8) Does the answer include all essential points required by the question and the ground_truth_answer?\n",
    "9) Is the answer clear and concise? (1 or 0)\n",
    "10) Is the answer logically structured, consistent with the context, and free of contradictions? (1 or 0)\n",
    "11) Does the answer provide enough detail without being too excessive? (1 or 0)\n",
    "12) Does the answer provide any citations when referencing data or claims? (1 or 0)\n",
    "13) Is the answer presented in a suitable format (list, table, short text, etc.) for the question? (1 or 0)\n",
    "14) Does the answer offer helpful additional insights without deviating from factual correctness? (1 or 0)\n",
    "\n",
    "Output format (example):\n",
    "{{\n",
    "  \"retrieval\": [1, 0, 1, 1, 1],\n",
    "  \"generation\": [1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "HUMAN_PROMPT = \"\"\"Question: {query}\n",
    "Ground Truth: {ground_truth_context}\n",
    "Retrieved Context: {retrieved_context}\n",
    "Final Answer: {generated_answer}\n",
    "\n",
    "Evaluate the above according to the 14 criteria outlined in the system instructions.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate(\n",
    "    [(\"system\", SYSTEM_PROMPT), (\"human\", HUMAN_PROMPT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval 단계, generation 단계 각각 프롬프트 정의\n",
    "RETRIEVAL_SYSTEM_PROMPT = \"\"\"You are an evaluation assistant for a RAG (Retrieval-Augmented Generation) system.\n",
    "Your task is to score the *retrieved contexts* according to the criteria provided below.\n",
    "\n",
    "### Input:\n",
    "- `question`: string = Field(description=\"A single user question\")\n",
    "- `ground_truth_context`: list = Field(description=\"Ground Truth (GT), usually one context\")\n",
    "- `context`: list = Field(description=\"list of retrieved contexts\")\n",
    "\n",
    "### Output:\n",
    "- `retrieval`: list = Field(description=\"A list of 5 scores (0 or 1) for the retrieval evaluation criteria.\")\n",
    "\n",
    "You must return a JSON object with the key \"retrieval\".\n",
    "- \"retrieval\" is a list of 5 scores (1 or 0).\n",
    "No additional text or explanation is allowed.\n",
    "\n",
    "### Retrieval Criteria (1~5):\n",
    "1) Do any of the retrieved contexts show strong similarity to the Ground Truth? (1 or 0)\n",
    "2) Do the retrieved contexts collectively capture essential information from the Ground Truth? (1 or 0)\n",
    "3) Do the retrieved contexts sufficiently address the user’s query? (1 or 0)\n",
    "4) Are all retrieved contexts relevant to the Ground Truth or the user’s query? (1 or 0)\n",
    "5) Does the combined length and number of retrieved contexts remain reasonable without overwhelming the user with excessive or irrelevant details?\n",
    "\n",
    "Output format (example):\n",
    "{{\n",
    "  \"retrieval\": [1, 0, 1, 1, 1]\n",
    "}}\"\"\"\n",
    "\n",
    "GENERATION_SYSTEM_PROMPT = \"\"\"You are an evaluation assistant for a RAG (Retrieval-Augmented Generation) system.\n",
    "Your task is to score the *final answer* according to the criteria provided below.\n",
    "\n",
    "### Input:\n",
    "- `question`: string = Field(description=\"A single user question\")\n",
    "- `ground_truth_answer`: string = Field(description=\"The correct answer to the question\")\n",
    "- `answer`: string = Field(description=\"The final answer generated by the system\")\n",
    "\n",
    "### Output:\n",
    "- `generation`: list = Field(description=\"A list of 9 scores (0 or 1) for the generation evaluation criteria.\")\n",
    "\n",
    "You must return a JSON object with the key \"generation\".\n",
    "- \"generation\" is a list of 9 scores (1 or 0).\n",
    "No additional text or explanation is allowed.\n",
    "\n",
    "### Generation Criteria (1~9):\n",
    "1) Is the final answer clearly relevant to the question and reflective of the user’s intent? (1 or 0)\n",
    "2) Is the answer factually correct and free from unsupported or inaccurate information? (1 or 0)\n",
    "3) Does the answer include all essential points required by the question and the ground_truth_answer? (1 or 0)\n",
    "4) Is the answer clear and concise? (1 or 0)\n",
    "5) Is the answer logically structured, consistent with any provided context, and free of contradictions? (1 or 0)\n",
    "6) Does the answer provide enough detail without being too excessive? (1 or 0)\n",
    "7) Does the answer provide any citations when referencing data or claims? (1 or 0)\n",
    "8) Is the answer presented in a suitable format for the question? (1 or 0)\n",
    "9) Does the answer offer helpful additional insights without deviating from factual correctness? (1 or 0)\n",
    "\n",
    "Output format (example):\n",
    "{{\n",
    "  \"generation\": [1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
    "}}\"\"\"\n",
    "\n",
    "RETRIEVAL_HUMAN_PROMPT = \"\"\"Question: {question}\n",
    "Ground Truth: {ground_truth_context}\n",
    "Retrieved Context: {retrieved_context}\n",
    "\n",
    "Evaluate the above according to the 5 criteria (retrieval) outlined in the system instructions.\"\"\"\n",
    "\n",
    "GENERATION_HUMAN_PROMPT = \"\"\"Question: {question}\n",
    "Ground Truth Answer: {ground_truth_answer}\n",
    "Final Answer: {answer}\n",
    "\n",
    "Evaluate the above according to the 9 criteria (generation) outlined in the system instructions.\"\"\"\n",
    "\n",
    "\n",
    "RETRIEVAL_PROMPT_TEMPLATE = ChatPromptTemplate(\n",
    "    [(\"system\", RETRIEVAL_SYSTEM_PROMPT), (\"human\", RETRIEVAL_HUMAN_PROMPT)]\n",
    ")\n",
    "GENERETION_PROMPT_TEMPLATE = ChatPromptTemplate(\n",
    "    [(\"system\", GENERATION_SYSTEM_PROMPT), (\"human\", GENERATION_HUMAN_PROMPT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output format 정의\n",
    "class EvaluationResult(BaseModel):\n",
    "    retrieval: list = Field(\n",
    "        description=\"A list of 5 scores (0 or 1) for the retrieval evaluation criteria.\"\n",
    "    )\n",
    "    generation: list = Field(\n",
    "        description=\"A list of 9 scores (0 or 1) for the generation evaluation criteria.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    retrieval: list = Field(\n",
    "        description=\"A list of 5 scores (0 or 1) for the retrieval evaluation criteria.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GenerationResult(BaseModel):\n",
    "    generation: list = Field(\n",
    "        description=\"A list of 9 scores (0 or 1) for the generation evaluation criteria.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 정의\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval, Generation 한번에 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 정의\n",
    "chain = PROMPT_TEMPLATE | llm.with_structured_output(EvaluationResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"ground_truth_context\": \"The capital of France is Paris.\",\n",
    "        \"retrieved_context\": \"France: Paris\",\n",
    "        \"ground_truth_answer\": \"Paris\",\n",
    "        \"generated_answer\": \"The capital of France is Paris. \\nsource: (./data/france.md)\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval 단계, Generation 단계 각각 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval 단계, Generation 단계 chain 정의\n",
    "retrieval_chain = RETRIEVAL_PROMPT_TEMPLATE | llm.with_structured_output(\n",
    "    RetrievalResult\n",
    ")\n",
    "generation_chain = GENERETION_PROMPT_TEMPLATE | llm.with_structured_output(\n",
    "    GenerationResult\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_score_list = retrieval_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"ground_truth_context\": \"The capital of France is Paris.\",\n",
    "        \"retrieved_context\": \"France: Paris\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_score_list = generation_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"ground_truth_answer\": \"Paris\",\n",
    "        \"answer\": \"The capital of France is Paris. \\nsource: (./data/france.md)\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval 평가 결과: [1, 1, 1, 1, 1]\n",
      "generation 평가 결과: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"retrieval 평가 결과: {retrieval_score_list.retrieval}\")\n",
    "print(f\"generation 평가 결과: {generation_score_list.generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_chain_async(input_data):\n",
    "    \"\"\"\n",
    "    Execute retrieval and generation chains asynchronously.\n",
    "\n",
    "    Parameters:\n",
    "        input_data (dict): Input data containing question, contexts, and answers.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Retrieval and generation score lists.\n",
    "    \"\"\"\n",
    "    # retrieval 평가\n",
    "    retrieval_task = asyncio.create_task(\n",
    "        retrieval_chain.ainvoke(\n",
    "            {\n",
    "                \"question\": input_data[\"question\"],\n",
    "                \"ground_truth_context\": input_data[\"ground_truth_context\"],\n",
    "                \"retrieved_context\": input_data[\"retrieved_context\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # generation 평가\n",
    "    generation_task = asyncio.create_task(\n",
    "        generation_chain.ainvoke(\n",
    "            {\n",
    "                \"question\": input_data[\"question\"],\n",
    "                \"ground_truth_answer\": input_data[\"ground_truth_answer\"],\n",
    "                \"answer\": input_data[\"answer\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    retrieval_result = await retrieval_task\n",
    "    generation_result = await generation_task\n",
    "\n",
    "    return retrieval_result.retrieval, generation_result.generation\n",
    "\n",
    "\n",
    "async def calculate_score_async(retrieval_scores, generation_scores):\n",
    "    \"\"\"\n",
    "    Calculate the total score asynchronously based on retrieval and generation scores with their respective weights.\n",
    "\n",
    "    Parameters:\n",
    "        retrieval_scores (list): A list of 5 scores (0 or 1) for retrieval evaluation.\n",
    "        generation_scores (list): A list of 9 scores (0 or 1) for generation evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Total score out of 50.\n",
    "    \"\"\"\n",
    "    retrieval_weights = [5, 5, 4, 3, 3]\n",
    "    generation_weights = [5, 5, 5, 5, 3, 3, 2, 1, 1]\n",
    "\n",
    "    if len(retrieval_scores) != len(retrieval_weights):\n",
    "        raise ValueError(\"Retrieval scores must have exactly 5 elements.\")\n",
    "    if len(generation_scores) != len(generation_weights):\n",
    "        raise ValueError(\"Generation scores must have exactly 9 elements.\")\n",
    "\n",
    "    retrieval_total = sum(\n",
    "        score * weight for score, weight in zip(retrieval_scores, retrieval_weights)\n",
    "    )\n",
    "    print(f\"retrieval 결과: {retrieval_total}\")\n",
    "\n",
    "    generation_total = sum(\n",
    "        score * weight for score, weight in zip(generation_scores, generation_weights)\n",
    "    )\n",
    "    print(f\"generation 결과: {generation_total}\")\n",
    "\n",
    "    total_score = retrieval_total + generation_total\n",
    "\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval 결과: 20\n",
      "generation 결과: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "    \"question\": \"크래프톤의 글로벌 e-sports 대회 투자로 예상 지급수수료 812억 원은 전년 대비 몇 퍼센트 증가했나요?\",\n",
    "    \"ground_truth_context\": \"\"\"파일 제목: 크래프톤_교보증권_20240723.pdf\n",
    " 내용: 영업비용  중  지급수수료는  글로벌  e-sports  대회  투자로  812억원(YoY  +15.0%, \n",
    "\n",
    "매출  대비  14.7%)  예상,  인건비는  인력증가분을  반영하며  1,270억원(YoY \n",
    "\n",
    "+23.0%),  마케팅비는 PUBG PC  관련 마케팅 집행으로 YoY +123.0%  증가하나 \n",
    "\n",
    "신작  마케팅이  본격화되지  않아  매출  대비  3.5%  수준에  머무르는  193억원  추정. \n",
    "주식보상비용은 3월말 대비 주가 상승에 따라 213억원 발생 전망. \n",
    "\n",
    "투자의견 Buy, 목표주가 370,000원으로 상향 \n",
    "\n",
    "투자의견  Buy  유지,  목표주가를  370,000원(종전  330,000원)으로  상향.  목표주가 \n",
    "\n",
    "상향은  PUBG  PC  버전의  글로벌  트래픽이  높게  유지되고  있음을  반영해  2024년 \n",
    "\n",
    "지배주주순이익  추정치를  기존  대비  +6%  상향함에  따름.  차기  성장  동력  확보를 \n",
    "\n",
    "위한  공격적인  인력  채용,  e-sports에  대한  적극적인 투자가  이루어지고  있음에도 \n",
    "\n",
    "탑라인 성장 기반의 이익 성장이 이루어지고 있다는 점이 차별화되는 요소로 판단. \n",
    "\n",
    "PUBG IP가 언리얼엔진5 이관 및 콘솔 기반 출시를 준비하는 가운데 4Q24 ‘다크앤\n",
    "\n",
    "다커  모바일’  글로벌  출시,  쿠키런  인도  출시가  이루어져  2025년  그  수혜가  온기 \n",
    "\n",
    "반영될 것으로 예상하며, ‘inZOI’(라이프 시뮬레이션, PC/콘솔) 역시 1H25 정식 출\n",
    "\n",
    "시될 것으로 전망. ‘PUBG’, ‘다크앤다커 모바일’, ‘inZOI’ 3 종의 IP는 24년 8월 게\n",
    "\n",
    "임스컴에 참가하여 글로벌 관심도가 고양될 것으로 기대. \n",
    "\n",
    "Forecast earnings & Valuation \n",
    "\n",
    "12 결산(십억원) \n",
    "매출액(십억원) \n",
    "YoY(%) \n",
    "영업이익(십억원) \n",
    "OP 마진(%) \n",
    "순이익(십억원) \n",
    "EPS(원) \n",
    "YoY(%) \n",
    "PER(배) \n",
    "PCR(배) \n",
    "PBR(배) \n",
    "EV/EBITDA(배) \n",
    "ROE(%) \n",
    "\n",
    "2022.12 \n",
    "1,854  \n",
    "-1.7 \n",
    "752  \n",
    "40.\"\n",
    "\"\"\",\n",
    "    \"retrieved_context\": \"\"\"영업비용  중  지급수수료는  글로벌  e-sports  대회  투자로  812억원(YoY  +15.0%, \n",
    "\n",
    "매출  대비  14.7%)  예상,  인건비는  인력증가분을  반영하며  1,270억원(YoY \n",
    "\n",
    "+23.0%),  마케팅비는 PUBG PC  관련 마케팅 집행으로 YoY +123.0%  증가하나 \n",
    "\n",
    "신작  마케팅이  본격화되지  않아  매출  대비  3.5%  수준에  머무르는  193억원  추정.\"\"\",\n",
    "    \"ground_truth_answer\": \"글로벌 e-sports 대회 투자로 812억원의 예상 지급수수료는 YoY +15.0% 증가했습니다.\",\n",
    "    \"answer\": \"글로벌 e-sports 대회 투자로 예상 지급수수료는 +15.0% 증가 할것으로 보입니다\",\n",
    "}\n",
    "\n",
    "retrieval_score_list, generation_score_list = await execute_chain_async(\n",
    "    input_data=input_data\n",
    ")\n",
    "total_score = await calculate_score_async(\n",
    "    retrieval_score_list, generation_score_list\n",
    ")\n",
    "total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naver-boostcamp-autorag-AUsRp4k1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
