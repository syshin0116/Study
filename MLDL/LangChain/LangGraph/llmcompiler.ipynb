{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for asynchronous tasks\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM Compiler: Tool Setup ===\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "# Define a custom encoder for TavilySearchResults\n",
    "def tavily_encoder(tool: TavilySearchResults):\n",
    "    return {\n",
    "        \"name\": tool.name,\n",
    "        \"max_results\": tool.max_results,\n",
    "        \"description\": tool.description,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a Pydantic wrapper model for the tool\n",
    "class ToolWrapper(BaseModel):\n",
    "    tool: TavilySearchResults\n",
    "\n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            TavilySearchResults: tavily_encoder,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tavily_search_results_json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
    "ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
    "# $1 or ${1} -> 1\n",
    "ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "END_OF_PLAN = \"<END_OF_PLAN>\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "\n",
    "def _ast_parse(arg: str) -> Any:\n",
    "    try:\n",
    "        return ast.literal_eval(arg)\n",
    "    except:  # noqa\n",
    "        return arg\n",
    "\n",
    "\n",
    "def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> list[Any]:\n",
    "    \"\"\"Parse arguments from a string.\"\"\"\n",
    "    if args == \"\":\n",
    "        return ()\n",
    "    if isinstance(tool, str):\n",
    "        return ()\n",
    "    extracted_args = {}\n",
    "    tool_key = None\n",
    "    prev_idx = None\n",
    "    for key in tool.args.keys():\n",
    "        # Split if present\n",
    "        if f\"{key}=\" in args:\n",
    "            idx = args.index(f\"{key}=\")\n",
    "            if prev_idx is not None:\n",
    "                extracted_args[tool_key] = _ast_parse(\n",
    "                    args[prev_idx:idx].strip().rstrip(\",\")\n",
    "                )\n",
    "            args = args.split(f\"{key}=\", 1)[1]\n",
    "            tool_key = key\n",
    "            prev_idx = 0\n",
    "    if prev_idx is not None:\n",
    "        extracted_args[tool_key] = _ast_parse(\n",
    "            args[prev_idx:].strip().rstrip(\",\").rstrip(\")\")\n",
    "        )\n",
    "    return extracted_args\n",
    "\n",
    "\n",
    "def default_dependency_rule(idx, args: str):\n",
    "    matches = re.findall(ID_PATTERN, args)\n",
    "    numbers = [int(match) for match in matches]\n",
    "    return idx in numbers\n",
    "\n",
    "\n",
    "def _get_dependencies_from_graph(\n",
    "    idx: int, tool_name: str, args: Dict[str, Any]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Get dependencies from a graph.\"\"\"\n",
    "    if tool_name == \"join\":\n",
    "        return list(range(1, idx))\n",
    "    return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
    "\n",
    "\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "\n",
    "\n",
    "def instantiate_task(\n",
    "    tools: Sequence[BaseTool],\n",
    "    idx: int,\n",
    "    tool_name: str,\n",
    "    args: Union[str, Any],\n",
    "    thought: Optional[str] = None,\n",
    ") -> Task:\n",
    "    if tool_name == \"join\":\n",
    "        tool = \"join\"\n",
    "    else:\n",
    "        try:\n",
    "            tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
    "        except ValueError as e:\n",
    "            raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
    "    tool_args = _parse_llm_compiler_action_args(args, tool)\n",
    "    dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
    "\n",
    "    return Task(\n",
    "        idx=idx,\n",
    "        tool=tool,\n",
    "        args=tool_args,\n",
    "        dependencies=dependencies,\n",
    "        thought=thought,\n",
    "    )\n",
    "\n",
    "\n",
    "class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
    "    \"\"\"Planning output parser.\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
    "        texts = []\n",
    "        # TODO: Cleanup tuple state tracking here.\n",
    "        thought = None\n",
    "        for chunk in input:\n",
    "            # Assume input is str. TODO: support vision/other formats\n",
    "            text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
    "            for task, thought in self.ingest_token(text, texts, thought):\n",
    "                yield task\n",
    "        # Final possible task\n",
    "        if texts:\n",
    "            task, _ = self._parse_task(\"\".join(texts), thought)\n",
    "            if task:\n",
    "                yield task\n",
    "\n",
    "    def parse(self, text: str) -> List[Task]:\n",
    "        return list(self._transform([text]))\n",
    "\n",
    "    def stream(\n",
    "        self,\n",
    "        input: str | BaseMessage,\n",
    "        config: RunnableConfig | None = None,\n",
    "        **kwargs: Any | None,\n",
    "    ) -> Iterator[Task]:\n",
    "        yield from self.transform([input], config, **kwargs)\n",
    "\n",
    "    def ingest_token(\n",
    "        self, token: str, buffer: List[str], thought: Optional[str]\n",
    "    ) -> Iterator[Tuple[Optional[Task], str]]:\n",
    "        buffer.append(token)\n",
    "        if \"\\n\" in token:\n",
    "            buffer_ = \"\".join(buffer).split(\"\\n\")\n",
    "            suffix = buffer_[-1]\n",
    "            for line in buffer_[:-1]:\n",
    "                task, thought = self._parse_task(line, thought)\n",
    "                if task:\n",
    "                    yield task, thought\n",
    "            buffer.clear()\n",
    "            buffer.append(suffix)\n",
    "\n",
    "    def _parse_task(self, line: str, thought: Optional[str] = None):\n",
    "        task = None\n",
    "        if match := re.match(THOUGHT_PATTERN, line):\n",
    "            # Optionally, action can be preceded by a thought\n",
    "            thought = match.group(1)\n",
    "        elif match := re.match(ACTION_PATTERN, line):\n",
    "            # if action is parsed, return the task, and clear the buffer\n",
    "            idx, tool_name, args, _ = match.groups()\n",
    "            idx = int(idx)\n",
    "            task = instantiate_task(\n",
    "                tools=self.tools,\n",
    "                idx=idx,\n",
    "                tool_name=tool_name,\n",
    "                args=args,\n",
    "                thought=thought,\n",
    "            )\n",
    "            thought = None\n",
    "        # Else it is just dropped\n",
    "        return task, thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Union\n",
    "from langchain_core.messages import AIMessage, SystemMessage, ChatMessage, FunctionMessage, ToolMessage, AIMessageChunk, HumanMessageChunk, ChatMessageChunk, SystemMessageChunk, FunctionMessageChunk, ToolMessageChunk\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following {num_tools} types:\n",
    "# {tool_descriptions}\n",
    "# {num_tools}. join(): Collects and combines results from prior actions.\n",
    "\n",
    "# - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
    "# - join should always be the last action in the plan, and will be called in two scenarios:\n",
    "#   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
    "#   (b) if the answer cannot be determined in the planning phase before you execute the plans.\n",
    "\n",
    "# Guidelines:\n",
    "# - Each action described above contains input/output types and description.\n",
    "# - You must strictly adhere to the input and output types for each action.\n",
    "# - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
    "# - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
    "# - Each action MUST have a unique ID, which is strictly increasing.\n",
    "# - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
    "# - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join.\n",
    "# - Ensure the plan maximizes parallelizability.\n",
    "# - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
    "# - Never introduce new actions other than the ones provided.\n",
    "\n",
    "# {messages}\n",
    "\n",
    "# Remember, ONLY respond with the task list in the correct format! E.g.:\n",
    "# 1. tool_name(arg_name=args)\n",
    "# 2. another_tool(arg_name=args)\n",
    "# 3. join(input=$1, $2)\n",
    "# <END_OF_PLAN>\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))\n",
      "{'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))\n",
      "{'query': 'current temperature in New York'}\n",
      "---\n",
      "join\n",
      "()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "example_question = \"What's the difference in temperature between SF and NY?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"])\n",
    "    print(task[\"args\"])\n",
    "    # pprint(task[\"tool\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps\n",
    "                and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "runnable = joiner_prompt | llm.with_structured_output(\n",
    "    JoinOutputs, method=\"function_calling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# Visualize the compiled StateGraph as a Mermaid diagram\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = graph.stream(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(\n",
    "#                 content=\"What are the key differences in economic policies between the current governments of the United States, China, and Germany, and how have these policies impacted their respective GDP growth rates over the past three years?\"\n",
    "#             )\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"recursion_limit\": 100,\n",
    "#     },\n",
    "#     stream_mode=\"messages\",\n",
    "# )\n",
    "# for step in steps:\n",
    "#     print()\n",
    "#     pprint(step)\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "async def astream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: Dict[str, Any],\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable[[Dict[str, str]], None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 비동기 방식으로 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    매개변수:\n",
    "    - graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "    - inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "    - config (RunnableConfig): 실행 설정\n",
    "    - node_names (List[str], optional): 출력할 노드 이름 목록 (빈 리스트면 모든 노드 출력)\n",
    "    - callback (Callable[[Dict[str, str]], None], optional): 각 청크 처리를 위한 콜백 함수 (기본값: None)\n",
    "      콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    반환값:\n",
    "    - None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    async for chunk_msg, metadata in graph.astream(\n",
    "        inputs, config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 포함된 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            else:\n",
    "                # 노드가 변경될 때마다 헤더를 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"🔹 Node: \\033[1;34m{curr_node}\\033[0m 🔹\")\n",
    "                    print(\"=\" * 60)\n",
    "\n",
    "                # 콘텐츠를 보기 좋게 출력\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "                # if content.startswith(\"{\") or content.startswith(\"[\"):\n",
    "                #     try:\n",
    "                #         import json\n",
    "\n",
    "                #         parsed = json.loads(content)\n",
    "                #         formatted = json.dumps(parsed, indent=4, ensure_ascii=False)\n",
    "                #         print(formatted)\n",
    "                #     except (json.JSONDecodeError, TypeError):\n",
    "                #         print(content)\n",
    "                # else:\n",
    "                #     print(content)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 1,  # temporary thread ID for testing\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = State(\n",
    "    model=\"openai\",\n",
    "    messages=[\n",
    "        HumanMessage(\n",
    "            content=\"What are the key differences in economic policies between the current governments of the United States, China, and Germany, and how have these policies impacted their respective GDP growth rates over the past three years?\"\n",
    "        )\n",
    "    ],\n",
    "    steps=[],\n",
    "    answer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔹 Node: \u001b[1;34mplan_and_schedule\u001b[0m 🔹\n",
      "============================================================\n",
      "1. tavily_search_results_json(query=\"current economic policies of the United States 202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tavily_search_results_json(query=\"current economic policies of China 2023\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tavily_search_results_json(query=\"current economic policies of Germany 2023\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m astream_graph(graph, inputs, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "Cell \u001b[0;32mIn[30], line 29\u001b[0m, in \u001b[0;36mastream_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mLangGraph의 실행 결과를 비동기 방식으로 스트리밍하여 출력하는 함수입니다.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m- None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m prev_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk_msg, metadata \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m     30\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m ):\n\u001b[1;32m     32\u001b[0m     curr_node \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanggraph_node\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# node_names가 비어있거나 현재 노드가 node_names에 포함된 경우에만 처리\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-F0L5SJfm-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1874\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1874\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1875\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1876\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1877\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1878\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1879\u001b[0m     ):\n\u001b[1;32m   1880\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1881\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1882\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-F0L5SJfm-py3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:412\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    410\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m loop\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 412\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    413\u001b[0m         futures,\n\u001b[1;32m    414\u001b[0m         return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m    415\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, end_time \u001b[38;5;241m-\u001b[39m loop\u001b[38;5;241m.\u001b[39mtime()) \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/asyncio/tasks.py:418\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    417\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/asyncio/tasks.py:525\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    522\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/asyncio/futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/asyncio/tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/asyncio/futures.py:198\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[1;32m    197\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    }
   ],
   "source": [
    "await astream_graph(graph, inputs, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-F0L5SJfm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
