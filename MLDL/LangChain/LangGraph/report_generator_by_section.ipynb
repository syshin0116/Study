{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'NVIDIA Ï£ºÍ∞Ä ÌïòÎùΩ ÏõêÏù∏ 2023'},\n",
       "  'id': 'call_WcQyfUD0p4SKinQoxw6M3kTR',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "\n",
    "# Define a tool\n",
    "tavily_tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([tavily_tool])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\n",
    "    \"nvidia Ï£ºÍ∞Ä ÌïòÎùΩÏùò ÏõêÏù∏Ïù¥ Îêú ÎåÄÏÉÅ Ïù¥Î¶ÑÏù¥ Î≠êÏïº?Í∑∏Î¶¨Í≥† Í∑∏ ÎåÄÏÉÅÏóê ÎåÄÌïú Ï†ïÎ≥¥ ÏïåÎ†§Ï§ò\"\n",
    ")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_WcQyfUD0p4SKinQoxw6M3kTR', 'function': {'arguments': '{\"query\":\"NVIDIA Ï£ºÍ∞Ä ÌïòÎùΩ ÏõêÏù∏ 2023\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 103, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7bddc39b-0677-45b1-8a9f-3877e9eed1b0-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'NVIDIA Ï£ºÍ∞Ä ÌïòÎùΩ ÏõêÏù∏ 2023'}, 'id': 'call_WcQyfUD0p4SKinQoxw6M3kTR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 103, 'output_tokens': 29, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing_extensions import TypedDict\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# class SearchQury(BaseModel):\n",
    "#     search_query: str = Field(\n",
    "#         None, description=\"Query that is optimized for web search.\"\n",
    "#     )\n",
    "#     justification: str = Field(\n",
    "#         None, description=\"Why query is relevant to the user's request.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# class Reference(BaseModel):\n",
    "#     \"\"\"Model for a reference\"\"\"\n",
    "\n",
    "#     title: str = Field(description=\"The title of the reference.\")\n",
    "#     url: str = Field(description=\"The url of the reference.\")\n",
    "\n",
    "\n",
    "# # Graph state\n",
    "# class State(TypedDict):\n",
    "#     search_queries: list[SearchQury]\n",
    "#     combined_output: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "orchestrator_worker = orchestrator_worker_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "async def astream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: Dict[str, Any],\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable[[Dict[str, str]], None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraphÏùò Ïã§Ìñâ Í≤∞Í≥ºÎ•º ÎπÑÎèôÍ∏∞ Î∞©ÏãùÏúºÎ°ú Ïä§Ìä∏Î¶¨Î∞çÌïòÏó¨ Ï∂úÎ†•ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "\n",
    "    Îß§Í∞úÎ≥ÄÏàò:\n",
    "    - graph (CompiledStateGraph): Ïã§ÌñâÌï† Ïª¥ÌååÏùºÎêú LangGraph Í∞ùÏ≤¥\n",
    "    - inputs (dict): Í∑∏ÎûòÌîÑÏóê Ï†ÑÎã¨Ìï† ÏûÖÎ†•Í∞í ÎîïÏÖîÎÑàÎ¶¨\n",
    "    - config (RunnableConfig): Ïã§Ìñâ ÏÑ§Ï†ï\n",
    "    - node_names (List[str], optional): Ï∂úÎ†•Ìï† ÎÖ∏Îìú Ïù¥Î¶Ñ Î™©Î°ù (Îπà Î¶¨Ïä§Ìä∏Î©¥ Î™®Îì† ÎÖ∏Îìú Ï∂úÎ†•)\n",
    "    - callback (Callable[[Dict[str, str]], None], optional): Í∞Å Ï≤≠ÌÅ¨ Ï≤òÎ¶¨Î•º ÏúÑÌïú ÏΩúÎ∞± Ìï®Ïàò (Í∏∞Î≥∏Í∞í: None)\n",
    "      ÏΩúÎ∞± Ìï®ÏàòÎäî {\"node\": str, \"content\": str} ÌòïÌÉúÏùò ÎîïÏÖîÎÑàÎ¶¨Î•º Ïù∏ÏûêÎ°ú Î∞õÏäµÎãàÎã§.\n",
    "\n",
    "    Î∞òÌôòÍ∞í:\n",
    "    - None: Ìï®ÏàòÎäî Ïä§Ìä∏Î¶¨Î∞ç Í≤∞Í≥ºÎ•º Ï∂úÎ†•Îßå ÌïòÍ≥† Î∞òÌôòÍ∞íÏùÄ ÏóÜÏäµÎãàÎã§.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    async for chunk_msg, metadata in graph.astream(\n",
    "        inputs, config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_namesÍ∞Ä ÎπÑÏñ¥ÏûàÍ±∞ÎÇò ÌòÑÏû¨ ÎÖ∏ÎìúÍ∞Ä node_namesÏóê Ìè¨Ìï®Îêú Í≤ΩÏö∞ÏóêÎßå Ï≤òÎ¶¨\n",
    "        if not node_names or curr_node in node_names:\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            else:\n",
    "                # ÎÖ∏ÎìúÍ∞Ä Î≥ÄÍ≤ΩÎê† ÎïåÎßàÎã§ Ìó§ÎçîÎ•º Ï∂úÎ†•\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"üîπ Node: \\033[1;34m{curr_node}\\033[0m üîπ\")\n",
    "                    print(\"=\" * 60)\n",
    "\n",
    "                # ÏΩòÌÖêÏ∏†Î•º Î≥¥Í∏∞ Ï¢ãÍ≤å Ï∂úÎ†•\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "async def astream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: Dict[str, Any],\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable[[Dict[str, str]], None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraphÏùò Ïã§Ìñâ Í≤∞Í≥ºÎ•º ÎπÑÎèôÍ∏∞ Î∞©ÏãùÏúºÎ°ú Ïä§Ìä∏Î¶¨Î∞çÌïòÏó¨ Ï∂úÎ†•ÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "\n",
    "    Îß§Í∞úÎ≥ÄÏàò:\n",
    "    - graph (CompiledStateGraph): Ïã§ÌñâÌï† Ïª¥ÌååÏùºÎêú LangGraph Í∞ùÏ≤¥\n",
    "    - inputs (dict): Í∑∏ÎûòÌîÑÏóê Ï†ÑÎã¨Ìï† ÏûÖÎ†•Í∞í ÎîïÏÖîÎÑàÎ¶¨\n",
    "    - config (RunnableConfig): Ïã§Ìñâ ÏÑ§Ï†ï\n",
    "    - node_names (List[str], optional): Ï∂úÎ†•Ìï† ÎÖ∏Îìú Ïù¥Î¶Ñ Î™©Î°ù (Îπà Î¶¨Ïä§Ìä∏Î©¥ Î™®Îì† ÎÖ∏Îìú Ï∂úÎ†•)\n",
    "    - callback (Callable[[Dict[str, str]], None], optional): Í∞Å Ï≤≠ÌÅ¨ Ï≤òÎ¶¨Î•º ÏúÑÌïú ÏΩúÎ∞± Ìï®Ïàò (Í∏∞Î≥∏Í∞í: None)\n",
    "      ÏΩúÎ∞± Ìï®ÏàòÎäî {\"node\": str, \"content\": str} ÌòïÌÉúÏùò ÎîïÏÖîÎÑàÎ¶¨Î•º Ïù∏ÏûêÎ°ú Î∞õÏäµÎãàÎã§.\n",
    "\n",
    "    Î∞òÌôòÍ∞í:\n",
    "    - None: Ìï®ÏàòÎäî Ïä§Ìä∏Î¶¨Î∞ç Í≤∞Í≥ºÎ•º Ï∂úÎ†•Îßå ÌïòÍ≥† Î∞òÌôòÍ∞íÏùÄ ÏóÜÏäµÎãàÎã§.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    async for chunk_msg, metadata in graph.astream(\n",
    "        inputs, config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_namesÍ∞Ä ÎπÑÏñ¥ÏûàÍ±∞ÎÇò ÌòÑÏû¨ ÎÖ∏ÎìúÍ∞Ä node_namesÏóê Ìè¨Ìï®Îêú Í≤ΩÏö∞ÏóêÎßå Ï≤òÎ¶¨\n",
    "        if not node_names or curr_node in node_names:\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            else:\n",
    "                # ÎÖ∏ÎìúÍ∞Ä Î≥ÄÍ≤ΩÎê† ÎïåÎßàÎã§ Ìó§ÎçîÎ•º Ï∂úÎ†•\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"üîÑ Node: \\033[1;36m{curr_node}\\033[0m üîÑ\")\n",
    "                    print(\"=\" * 60)\n",
    "\n",
    "                # ÏøºÎ¶¨ Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ Ï∂úÎ†•\n",
    "                tool_calls = chunk_msg.additional_kwargs.get(\"tool_calls\", [])\n",
    "                if tool_calls:\n",
    "                    # print(f\"üîç Query: \", end=\"\")\n",
    "                    for tool_call in tool_calls:\n",
    "                        query = tool_call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "                        if query:\n",
    "\n",
    "                            print(f\"{query}\", end=\"\", flush=True)\n",
    "                else:\n",
    "                    # ÏøºÎ¶¨ Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ content Ï∂úÎ†•\n",
    "                    content = chunk_msg.content\n",
    "                    if content:\n",
    "                        print(f\"{content}\", end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 1,  # temporary thread ID for testing\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = State(\n",
    "    topic=\"Create a report on LLM scaling laws\",\n",
    "    sections = [],\n",
    "    completed_sections = [],\n",
    "    final_report = \"\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ Node: \u001b[1;36morchestrator\u001b[0m üîÑ\n",
      "============================================================\n",
      "{\"sections\":[{\"name\":\"Introduction to LLM Scaling Laws\",\"description\":\"This section will provide an overview of what LLM (Large Language Models) scaling laws are, their importance in the development of AI models, and how they influence model performance and efficiency.\"},{\"name\":\"Historical Context and Development\",\"description\":\"This section will cover the history and evolution of scaling laws in the context of language models, highlighting key research papers and breakthroughs that have shaped our current understanding.\"},{\"name\":\"Mathematical Foundation of Scaling Laws\",\"description\":\"This section will delve into the mathematical principles and theories that underpin scaling laws, including key formulas and models used to predict model performance based on size and computational resources.\"},{\"name\":\"Impact on Model Design and Architecture\",\"description\":\"This section will explore how scaling laws affect the design and architecture of language models, including considerations for parameter size, training data requirements, and computational resources.\"},{\"name\":\"Case Studies and Applications\",\"description\":\"This section will present case studies and real-world applications where scaling laws have been successfully applied, demonstrating their practical impact on developing state-of-the-art language models.\"},{\"name\":\"Challenges and Limitations\",\"description\":\"This section will discuss the challenges and limitations associated with applying scaling laws, such as diminishing returns, data efficiency, and the environmental impact of scaling large models.\"},{\"name\":\"Future Directions and Research Opportunities\",\"description\":\"This section will outline potential future research directions, innovations, and opportunities in the study and application of LLM scaling laws.\"},{\"name\":\"Conclusion\",\"description\":\"This section will summarize the key points discussed in the report, emphasizing the significance of scaling laws in advancing language model technology and their potential future implications.\"}]}\n",
      "============================================================\n",
      "üîÑ Node: \u001b[1;36mllm_call\u001b[0m üîÑ\n",
      "============================================================\n",
      "## Introduction## Case## Impact## Challenges to L and Limit## Mathematicalations\n",
      "\n",
      "LM Scaling## Historical Foundation of Context and Laws\n",
      "\n",
      " Studies and Applications\n",
      "\n",
      " Scaling LawsIn recent years, Development\n",
      "\n",
      "The exploration of scaling scaling\n",
      "\n",
      "Scaling laws on Model Design and Architecture\n",
      "\n",
      "Large Language Models ( laws inScaling laws language models## Future have emergedLLMs laws are has been as a have emerged instrumental in as a) are an essential Directions and Research Opportunities critical area of study shaping our concept in understanding how a cornerstone in the\n",
      "\n",
      "The the performance of modern understanding of critical factor study of artificial intelligence Large Language of machine development of how model learning models## Conclusion in shaping large-scale performance can Models\n",
      "\n",
      "In changes with the design ( language modelsThe application of scaling be improvedLLMs, fundamentally transforming how this report machines process size and. By laws in the development and architecture, we and generate understanding the computational resources. They of language of machine learning models offers substantial benefits, human language yet it is not without its relationships between have explored through changes in scale. The) and their scaling laws presents provide a. A challenges and the pivotal journey began a fertile models. critical concept framework for role that with the limitations. ground for underpinning scaling laws Key issues predicting how broader development future research the development of artificial and innovation of include these diminishing. As neural networks returns, models is these models, which play in adjustments in the advancement of language these factors continue to LLM model technology saw a model size impact the data efficiency scaling laws evolve and resurgence due efficiency and, which to increased. These, and expand in describe how accuracy of laws, computational power changes in models. which describe their capabilities and data availability in the environmental how model the late, several performance improves impact of the size, dataset 200 This section scaling large key areas0s of a with increased These laws provide insights computational resources model‚Äî.\n",
      "\n",
      "###specifically Early Develop, its into how model performance scales with changes in parameter size parameters, data, emerge as, dataset volume,ments\n",
      "\n",
      " and computational and data, have become aThe roots resources. of scaling cornerstone in promising directions explores the models.\n",
      "\n",
      "### laws can size, the development D and opportunities be tracediminishing Understanding these of large relationships is and computational resources‚Äîaffect its performance and capabilities back to for further Returns\n",
      "\n",
      ".\n",
      "\n",
      "### crucial for Understanding Scaling-scale AI exploration:\n",
      "\n",
      "One of developing efficient the primary Laws\n",
      "\n",
      " the pioneering challenges associated### Scaling laws mathematical principles work on and effective in the systems. with scaling1. neural network architectures and context of language models By systematically and computational laws is Understanding and LLM analyzing theses refer Optimizing the introduction laws, of backpropagation the phenomenon to the in the researchers have Scaling Laws of diminishing 198 been able.\n",
      "\n",
      "### Parameter Size0s and theories returns.\n",
      "\n",
      "Research Considerations. that However form empirical relationships As models\n",
      "\n",
      "One into the, it the basis grow in to predict the performance gains that wasn't until fundamental principles can be size, the introduction achieved as resources, governing L models grow of deep the incremental that dictate how improvements in AI models can be achieved by increasing learning frameworksLM scaling in size their scale gains in and complexity performance often and the laws of the primary aspects is. These of these scaling laws, alongside key formulas.\n",
      "\n",
      "One decrease. availability of laws are of the influenced by crucial for and models This means large datasets scaling laws key points formulated based that researchers optimizing model that facilitate that discussed is while on extensive these predictions initial could experiment increases is the experimentation and.\n",
      "\n",
      "### the exponential design and parameter size with larger analysis, Key Concepts relationship between models. in model researchers model size and revealing that performance. of language and The parameters can Initial efforts, dataset model performance lead to Future studiesories\n",
      "\n",
      " focused on tends to size, significant improvementsScaling laws and computation models. empirical observations could focus improve predict in capabilities are fundamentally on:\n",
      "\n",
      " that suggested rooted inably as- **, continual larger models statistical learning we scale tended to scaling results up threeRefining in progressively, and their collective impact on theory, Scaling Predictions key factors smaller performance:** Developing which seeks:\n",
      "\n",
      "1 enhancements. more accurate perform better, albeit with diminishing to understand. ** This is performance. As demonstrated, larger returns.\n",
      "\n",
      "Model Size the relationship engineers can models trained models that### Break between model particularly evident on extensivethrough Papers datasets tend complexity, (Parameters predict performance\n",
      "\n",
      "One data size to perform outcomes based, and of):** the when models better across approach or exceed the generalization seminal papers Increasing the performance. on scaling current state number of a wide that discussed parameters generally range of The principle-of-the enhances a tasks, parameters such-art benchmarks scaling in model's of Occ the context including language of deep, where ability toam's as model understanding, learning was further scaling size, translation, razor often better predict \"Understanding and generation training data understand and generate complex Deep Learning guides this language patterns. This As scaling Requires R. This understanding, expansion allowsethinking relationship under suggesting that for richer Generallines the yields minimal simpler models laws suggestization representations of tend to volume, and computation importance of\" by improvements relative generalize language intric continued investments Zhang et to the resources.\n",
      " better if- **acies and in computational resources invested, increasing the number they are infrastructure andEfficiency in data collection al. nuanced context (201 sufficiently expressive handling.\n",
      "\n",
      " of parameters Resource Util6)..\n",
      "\n",
      "### to push2. generally leads.\n",
      "\n",
      "Another Data Efficiencyization:** This study the boundaries **Dataset critical concept Investigating the performance of what to improved methods to Size highlighted:** the is the these models achieve greater As the surprising general model bias- performance efficiency in scaling,\n",
      "\n",
      "Datavariance tradeization capabilities potentially through of neural can achieve.\n",
      "\n",
      "Furthermoreoff, advanced algorithms networks, volume of that reduce efficiency is, the which describes computational overhead setting the training data and energy how increasing another critical report highlights stage for the potential model complexity limitation when increases, questioning the future implications can of these reduce limits, provided that other factors such of models scaling laws are bias but scaling.\n",
      "\n",
      " better able as dataset. As size and increase variance to and general capabilities of language models. models continueThe paradigm consumption.\n",
      "\n",
      "### 2.ize from This section applying scaling laws. Large models to grow computational power require vast diverse linguistic shift occurred Exploring Ethical and Soc, there explores a with the inputs. amounts of More extensive are adequately datasets provide is an. Scaling laws help quantify these series of advent of data to a broader effects, increasing needietal achieve optimal scaled. case studies the Transformer providing insights base of to address architecture, performance, challenges such Implications and real into how\n",
      "\n",
      "As as introduced by examples from energy LLM which the-world applicationss grow and model size the in scale consumption, This has acquisition and and influence, understanding model can and training Vaswani ethical considerations their ethical and where scaling societal impacts becomes processing of et al resulted in increasingly data can important, and learn,. Key. in areas for such data future research accessibility. include:\n",
      "\n",
      " thereby improving- ** be balanced laws have been successfully 201Bias and can be The scalability its accuracy Fairness:** Developing the development for optimal7. techniques to of these resource-intensive applied, and adaptability and time performance.\n",
      "\n",
      " This architecture identify, models also.\n",
      "\n",
      "3### Important highlighting their demonstrated unprecedented raises questions-consuming. mitigate, practical impact about their. ** scalability and and measure Moreover,Compute Resources biases that deployment in performance the assumption on developing Formulas and in real-world Models may be natural language state-of\n",
      "\n",
      "1. ** applications, that largerPower Law amplified as models scale of increasingly large models where trade:** Greater computational power-the-art.\n",
      "- processing tasks datasets will, leading ****:Account-offs betweenability and language models always lead, such to a A fundamental Transparency:** performance and to better enables more Exploring frameworks as GPT for ensuring.\n",
      "\n",
      "### flurry efficiency must mathematical model that L model performance be carefully sophisticated training of research Case Study processes, used inLMs-3 managed.\n",
      "\n",
      " allowing models to leverage scaling laws on scaling operate transpar increased parameters 1ently and laws.In conclusion is the and data and beyond: GPT are held power law does not accountable for, scaling effectively. The paper hold universally their laws offer outputs, which-3 Enhanced compute, expressed \"Attention in various a valuable as:\n",
      "\n",
      " is All by Open. In capabilities facilitate applications.\n",
      "\n",
      " framework for   \\###  possess billions more iterations You Need guiding theAI\n",
      "\n",
      "3.[\n",
      "  \" not many cases, finer Advancements development of only introduced P(n, models in Mult language models-tunedimodalOpenAI the transformer) ='s GPT Integrations may require but also C \\-3. By\n",
      "\n",
      "The understanding and serves astimes n integration of^\\alpha LLM leveraging these training, and ultimatelys with a prominent principles, careful tuning, better showcased the potential of scaling model\n",
      "   other modalities (e optimization of.g., the AI vision, example of audio) parameters to model parameters offers exciting opportunities for creating more \\]\n",
      "\n",
      " versatile and community can.\n",
      "\n",
      "### improve language the application comprehensive AI systems. achieve significant Importance of   where Future research understanding.\n",
      "\n",
      " could delve of scaling Scaling Laws \\(P### Evolution laws in into:\n",
      "\n",
      "- **Cross-Modal Learning of Scaling\n",
      "\n",
      "The breakthroughs,:** Investigating how language model importance of Laws\n",
      "\n",
      " scaling laws and additional LLM apply when data preprocessing ultimately leading development.In  to manage issues such combining text(n)\\) is scaling laws as to of parameters.\n",
      "\n",
      "However, simply increasing parameter data more size poses challenges, including over imbalance2020 with other, noise,fitting Building on powerful and and diminishing data types, Kaplan lies in the foundation versatile language and redundancy, potentially, their ability which et returns. al to guide technologies. leading to. published can inhibit laid by As we the efficient breakthroughs in the model its predecessors performance, \\(n a pivotal learning efficiency\\) is As a models fields such development and measure of and negatively, GPT paper grow,, as automated deployment of model size impact performance content creation-3 AI models \"Scaling.\n",
      "\n",
      "### look to or computational Laws for the future and interactive effort, they require careful tuning Environmental Impact leverages, it Neural Language and regular Models,\"\n",
      "\n",
      "The is crucial \\(C\\) which provided AI systems.\n",
      "- **Unified environmental impact. By understanding these relationships, researchers and to balance a formal is a Architectures innovation with:** Developing investigation into developers can responsibility, constant, a massive architecture with 175 how model of scaling make informed billion ensuring that parameters and \\ decisions about performance changes the benefits. The(\\ resource allocationalpha with scale\\) of these architectures that, balancing large models seamlessly integrate development process. They the trade advancements are is a multiple modalities is an and scale identified power was heavily efficiently across widely accessible-offs between scaling exponent increasingly pressing and ethically diverse tasks that determines-law relationships guided by model complexity concern. how performance aligned with.\n",
      "\n",
      "### scaling laws between model Training and 4, training scales with. Enh size, maintaining large societal values, which. duration, the amount-scale modelsancing Model and performance informed the of training size.\n",
      "\n",
      " Interpretability demand significant2. outcomes. decisions regarding **Law\n",
      "\n",
      "As data, computational resources Scaling laws LLM of D, which and compute model size offer a translate intoization to maintain performance gains without incurrings become excessive computational costs. Researchersiminishing, offering must and the strike a balance between model complexity blueprint for more complex a substantial energy predictive and efficiency Returns** predicting performance, enhancing: This amount of consumption and framework for, often gains from is often their interpret carbon emissions model scalingability is scaling up training data. This. This resources, critical for environmental footprint their effective required. paper was helping to deployment. raises ethical By adhering crucial in optimize model experimenting with sparsity Potential areas and sustainability providing a to these architecture and of research questions, theoretical underpin training strategies include:\n",
      "\n",
      " principles, particularly as and modularning for observed in.\n",
      "\n",
      "Moreover the AI- ** architectures to GPT- scaling laws the empiricalMechanistic, these community strives optimize resource, where observations gathered Interpretability laws highlight to balance3 achieved as models unprecedented performance innovation with:** Un the diminishingravel increase in ecological responsibilitying the size, the gains over the years.\n",
      "\n",
      "Subsequent research in performance in natural language processing tasks, decrease. internal processes of LLMs focused on from text returns associated with certain scaling aspects Mathem refining these to understand how they, prompting the exploration laws and of innovative completion to addressing challenges techniques to translation, associated with achieve further make decisions demonstrating the and generate outputs.\n",
      " usage.\n",
      "\n",
      "### Training Data Requirements\n",
      "\n",
      "Scaling laws indicate that larger models typically scaling, benefit from- ** advancements. such asInteractive AI transformative potential For instance. Efforts to mitigate this impact through more energy Systems:** training efficiency-efficient hardware of, scaling whileatically, and algorithm Creating systemsic innovations more extensive and diverse are ongoing and the laws in, increasing the yet training datasets the challenge remains significant environmental impact that allow users to enhancing model model size can enhance of larger this is capabilities.\n",
      "\n",
      " interactively to reach.\n",
      "\n",
      "In models. summary, performance, explore and The work while scaling understand model it also expressed through reasoning in laws have### Case demands more by Brown Study  real-time et al computational power their full potential. This has sub-linear.\n",
      "\n",
      "###. on and memory scaling,2: DeepMind led, which to driven remarkable advancements in machine where \\ learning may not an emphasis, they 5. Exp on curanding Applications always be pose notable‚Äôs G in Specializedopher\n",
      "\n",
      "(\\alpha feasible or Domains\n",
      "\n",
      "ating and challenges andDeepMind's GLLMs cost-effective utilizing GPT hold limitations. vast- < 3 in 202 Addressing0 further greatopher project. Thus exemplified the application promise for further illustrates, scaling specialized applications of scaling these issues the utility1\\ corpora across various of scaling).\n",
      "\n",
      "3 domains. requires a balanced approach laws. Future research, incorporating multilingual and Gopher could focus. ** multimodal that considers on:\n",
      "\n",
      ", a laws,- ** data to both theLearning Cur laws push 280Domain-S showcasing how-billion technological benefits- the AIpecific Models enrich the significantly largerparameter language and theves** community to:** Developing training process model, LLM broader implications seek alternative: These was designed to explore methods, of models. As tailored such as a result scaling. the role model compression Continued research, data of scaling to specific fields, such as models could achieve superior performance across a wide or fine range of and innovation healthcare, finance, tasks without-tuning are essential are graphical representations that task-specific in achieving or law, to preprocessing, human-like show the to overcoming fine-t, and maintain efficiency relationship between exploring their performance across quality control, anduning.\n",
      "\n",
      " these hurdles while scaling scaling dynamics### Current various tasks the size and ensuring in these.\n",
      "\n",
      "### curation Understanding\n",
      "\n",
      ". By Influence on contexts.\n",
      " have becomeToday, of the- ** analyzing scaling the sustainableReal-Time Model Performance scaling laws integral components laws, development of Adaptation and Efficiency training dataset DeepMind are a:** Investig\n",
      "\n",
      "LL AI technologies. of modelating cornerstone of howM scaling LLM language model laws significantlys can and the development.\n",
      "\n",
      " was able development,The interaction be adapted influence the guiding both performance of or fine performance and to optimize-tuned between model academic research a model the balance. The on-the and industry size and-fly asympt applications.otic nature data volume The continuous to respond efficiency of AI models. By adhering to of these to dynamic growth in is non these principles environments and curves often between model size and dataset diversity, developers-linear, model size evolving data follows a, from.\n",
      "\n",
      "###, resulting in significant necessitating power law can produce 6 GPT- improvements in. Collaborative2 to models that, indicating and Open GPT- model performance not only strategic decisions about data Research Initi4 and. This perform exceptionallyatives\n",
      "\n",
      " that performance beyond, well on augmentation andEncour case study embodies theaging collaborative improvements decrease complex language selection to principles outlined and open tasks but also do research efforts with larger maximize performance by scaling can accelerate so with gains. advancements in optimal resource datasets.\n",
      "\n",
      " the understanding underscores the Furthermore, importance of scaling laws usage. and application in creating4. of L models that laws, driving innovations in modelLM scaling This is **Compute architecture, training techniques, and the ethical implications of laws. Future initiatives not only could include crucial as optimization strategies-Optimal data collection, including.\n",
      "\n",
      "The:\n",
      "\n",
      "- excel in LLM **Open Scaling Laws historical development bias and**: individual tasks Datasets of scaling and Bench privacy concerns These laws but also laws reflectsmarks:**s are, must aim to demonstrate robust Establishing a broader deployed in shared datasets trend in generalization be addressed and benchmarks deep learning across diverse to ensure a wide: the to foster domains.\n",
      "\n",
      " reproducibility that scaled and comparative iterative process### Application identify the range of analysis across models are applications, of empirical from natural language processing 1 and machine translation to customer service studies.\n",
      " automation and discovery, content creation: Healthcare both performant- **.\n",
      "\n",
      "In theoretical formulationInterdiscip and responsible\n",
      "\n",
      "In most efficient, and the healthcare.\n",
      "\n",
      "### practical application use of summary, LLM scaling lawslinary Collaboration:** Prom computational resourcesoting partnerships between computer scientists, ethicists sector, scaling laws for, and training Computational Resources\n",
      "\n",
      "Scaling laws have domain experts profound implications to address. As have been models. provide essential multifaceted The compute for the challenges posed computational resources by L-optimal computational resourcesLM scaling insights into continue to instrumental in required to the systematic.\n",
      "\n",
      "In train and scaling law expand, the development conclusion, scaling of the study scaling laws AI models is often remain a deploy large language models vital area of research represented as and application of LLM scaling laws offer. The, presenting a framework for enhancing, promising exponential growth performance while of language managing resource of parameter:\n",
      "\n",
      "   further advancements sizes and constraints. \\[\n",
      " numerous opportunities for groundbreaking research and innovation. By focusing on optimizing As the in the data volumes capability and accessibility of field of scaling principles   L demands significant language models. AI continues, addressing computational power to evolve \\pro ethical considerations models for,, often and expanding application, these laws will domains, necessitating the researchpto N remain a specialized hardware pivotal factor community can medical applications^{-\\ harness the such as in steering full potential. For the development of L TPUsLMs instance, and GPUs to advance of increasingly models designed technology and. This society. to process sophisticated and capable language need for models. high-performance computing infrastructure has led to increased collaboration between AI researchers and hardware manufacturers.\n",
      "\n",
      "Eff and analyzeicient large volumes of medical literature and patient records benefit from scaling laws, which resource utilization guide the optimal sizing is crucial of models and datasets, prompting. This innovations in has led distributed training to advancements in areas such as, model parallelism automated diagnosis, and mixed precision, personalized techniques. medicine, These advancements and the aim to generation of reduce training medical reports time and, where energy consumption scaled models, making provide enhanced large-scale accuracy and models more reliability.\n",
      "\n",
      " accessible and sustainable. As a### Application result, the design 2 and architecture: Financial of language Services\n",
      "\n",
      " models areThe financial industry has increasingly influenced by the also embraced availability and scaling laws efficiency of to refine computational resources.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      " language modelsIn summary for tasks, scaling laws have like sentiment a profound analysis, impact on the design fraud detection and architecture, and of language models, market trend prediction. dictating By applying decisions about scaling laws parameter size, training data,, financial and computational institutions can resources. develop models As these models continue to grow in size and complexity that process vast datasets, ongoing research and efficiently, yielding insights innovation will that drive be essential to harness decision-making their full and strategy. The ability to scale models appropriately ensures that financial services can leverage comprehensive data analysis to maintain a competitive edge.\n",
      "\n",
      " potential while addressing the associated challenges. By understanding and leveraging scaling laws, the AI community can continue to advance the state-of-the-art in natural language processing.### Application 3: Education\n",
      "\n",
      "Educational technology has seen transformative changes through the application of scaling laws inbeta}\n",
      " language model development.   \\ Scaled models are]\n",
      "\n",
      "   now used where \\( to create adaptive learningL\\ platforms that cater to) is individual student needs, the loss providing personalized, \\( feedback and support.N\\ By adhering to scaling) is laws, educational tools have improved in delivering the computation (often accurate content measured in floating-point recommendations and facilitating more engaging learning operations), experiences, thereby enhancing and \\ educational outcomes(\\beta.\n",
      "\n",
      "###\\) Conclusion\n",
      "\n",
      " isThese case the scaling studies and exponent applications demonstrate reflecting how the profound loss decreases with increased impact that computation.\n",
      "\n",
      " scaling laws### Predict have onive Models the development\n",
      "\n",
      "Researchers and deployment often employ of state predictive models-of-the to forecast-art language the scaling models. behavior of By informing machine learning critical design systems. choices, One such scaling laws model is enable researchers and practitioners the neural scaling law to create models that are not only larger, which predicts the and more performance of neural networks based on powerful but parameters also, more dataset size efficient and, and adaptable to training compute. These real-world models are applications. invaluable for guiding the As the design and deployment of field continues large-scale to evolve AI systems, the, ensuring insights derived that resources are used from scaling effectively to achieve desired laws will performance levels remain pivotal.\n",
      "\n",
      "### Implications in shaping and Applications the future\n",
      "\n",
      "Understanding of language the mathematical model development foundation of scaling laws across diverse is crucial for advancing industries. artificial intelligence and machine learning. By leveraging these laws, researchers and practitioners can make informed decisions about model architectures, size, and training regimes to optimize performance. This knowledge is especially pertinent in the era of large-scale models, where resource allocation and efficiency are paramount.\n",
      "\n",
      "In conclusion, the mathematical underpinnings of scaling laws provide a robust framework for predicting model behavior in response to changes in size and computational resources. Mastery of these principles allows for the strategic development of AI systems that are both powerful and resource-efficient."
     ]
    }
   ],
   "source": [
    "await astream_graph(orchestrator_worker, inputs, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Introduction to LLM Scaling Laws\n",
       "\n",
       "Large Language Models (LLMs) are a cornerstone of modern artificial intelligence, fundamentally transforming how machines process and generate human language. A critical concept underpinning the development of these models is LLM scaling laws, which describe how changes in the size of a model‚Äîspecifically, its parameters, data, and computational resources‚Äîaffect its performance and capabilities.\n",
       "\n",
       "### Understanding Scaling Laws\n",
       "\n",
       "Scaling laws in the context of LLMs refer to the empirical relationships that dictate how improvements in AI models can be achieved by increasing their scale. These laws are formulated based on extensive experimentation and analysis, revealing that model performance tends to improve predictably as we scale up three key factors:\n",
       "\n",
       "1. **Model Size (Parameters):** Increasing the number of parameters generally enhances a model's ability to understand and generate complex language patterns. This expansion allows for richer representations of language intricacies and nuanced context handling.\n",
       "\n",
       "2. **Dataset Size:** As the volume of training data increases, models are better able to generalize from diverse linguistic inputs. More extensive datasets provide a broader base of examples from which the model can learn, thereby improving its accuracy and adaptability.\n",
       "\n",
       "3. **Compute Resources:** Greater computational power enables more sophisticated training processes, allowing models to leverage increased parameters and data effectively. Enhanced compute capabilities facilitate more iterations, finer-tuned training, and ultimately, better optimization of model parameters.\n",
       "\n",
       "### Importance of Scaling Laws\n",
       "\n",
       "The importance of LLM scaling laws lies in their ability to guide the efficient development and deployment of AI models. By understanding these relationships, researchers and developers can make informed decisions about resource allocation, balancing the trade-offs between model complexity, training duration, and performance outcomes. Scaling laws offer a blueprint for predicting performance gains from scaling up resources, helping to optimize model architecture and training strategies.\n",
       "\n",
       "Moreover, these laws highlight the diminishing returns associated with certain scaling aspects, prompting the exploration of innovative techniques to achieve further advancements. For instance, while increasing the model size can enhance performance, it also demands more computational power and memory, which may not always be feasible or cost-effective. Thus, scaling laws push the AI community to seek alternative methods, such as model compression or fine-tuning, to maintain efficiency while scaling.\n",
       "\n",
       "### Influence on Model Performance and Efficiency\n",
       "\n",
       "LLM scaling laws significantly influence the performance and efficiency of AI models. By adhering to these principles, developers can produce models that not only perform exceptionally well on complex language tasks but also do so with optimal resource usage. This is crucial as LLMs are deployed in a wide range of applications, from natural language processing and machine translation to customer service automation and content creation.\n",
       "\n",
       "In summary, LLM scaling laws provide essential insights into the systematic scaling of AI models, presenting a framework for enhancing performance while managing resource constraints. As the field of AI continues to evolve, these laws will remain a pivotal factor in steering the development of increasingly sophisticated and capable language models.\n",
       "\n",
       "---\n",
       "\n",
       "## Historical Context and Development\n",
       "\n",
       "The exploration of scaling laws in language models has been instrumental in shaping our understanding of how model performance can be improved through changes in scale. The journey began with the broader development of artificial neural networks, which saw a resurgence due to increased computational power and data availability in the late 2000s.\n",
       "\n",
       "### Early Developments\n",
       "\n",
       "The roots of scaling laws can be traced back to the pioneering work on neural network architectures and the introduction of backpropagation in the 1980s. However, it wasn't until the introduction of deep learning frameworks and the availability of large datasets that researchers could experiment with larger models. Initial efforts focused on empirical observations that suggested larger models tended to perform better, albeit with diminishing returns.\n",
       "\n",
       "### Breakthrough Papers\n",
       "\n",
       "One of the seminal papers that discussed scaling in the context of deep learning was \"Understanding Deep Learning Requires Rethinking Generalization\" by Zhang et al. (2016). This study highlighted the surprising generalization capabilities of neural networks, setting the stage for questioning the limits of scaling.\n",
       "\n",
       "The paradigm shift occurred with the advent of the Transformer architecture, introduced by Vaswani et al. in 2017. This architecture demonstrated unprecedented scalability and performance in natural language processing tasks, leading to a flurry of research on scaling laws. The paper \"Attention is All You Need\" not only introduced the transformer but also showcased the potential of scaling model parameters to improve language understanding.\n",
       "\n",
       "### Evolution of Scaling Laws\n",
       "\n",
       "In 2020, Kaplan et al. published a pivotal paper, \"Scaling Laws for Neural Language Models,\" which provided a formal investigation into how model performance changes with scale. They identified power-law relationships between model size, the amount of training data, and compute, offering a predictive framework for model scaling. This paper was crucial in providing a theoretical underpinning for the empirical observations gathered over the years.\n",
       "\n",
       "Subsequent research focused on refining these laws and addressing challenges associated with scaling, such as training efficiency and the environmental impact of larger models. The work by Brown et al. on GPT-3 in 2020 further exemplified the application of scaling laws, showcasing how significantly larger models could achieve superior performance across a wide range of tasks without task-specific fine-tuning.\n",
       "\n",
       "### Current Understanding\n",
       "\n",
       "Today, scaling laws are a cornerstone of language model development, guiding both academic research and industry applications. The continuous growth in model size, from GPT-2 to GPT-4 and beyond, embodies the principles outlined by scaling laws, driving innovations in model architecture, training techniques, and optimization strategies.\n",
       "\n",
       "The historical development of scaling laws reflects a broader trend in deep learning: the iterative process of empirical discovery, theoretical formulation, and practical application. As computational resources continue to expand, scaling laws remain a vital area of research, promising further advancements in the capability and accessibility of language models.\n",
       "\n",
       "---\n",
       "\n",
       "## Mathematical Foundation of Scaling Laws\n",
       "\n",
       "Scaling laws are an essential concept in understanding how the performance of machine learning models changes with size and computational resources. They provide a framework for predicting how adjustments in these factors impact the efficiency and accuracy of models. This section explores the mathematical principles and theories that form the basis of these scaling laws, alongside key formulas and models that facilitate these predictions.\n",
       "\n",
       "### Key Concepts and Theories\n",
       "\n",
       "Scaling laws are fundamentally rooted in statistical learning theory, which seeks to understand the relationship between model complexity, data size, and generalization performance. The principle of Occam's razor often guides this understanding, suggesting that simpler models tend to generalize better if they are sufficiently expressive.\n",
       "\n",
       "Another critical concept is the bias-variance tradeoff, which describes how increasing model complexity can reduce bias but increase variance. Scaling laws help quantify these effects, providing insights into how model size and training data can be balanced for optimal performance.\n",
       "\n",
       "### Important Formulas and Models\n",
       "\n",
       "1. **Power Law**: A fundamental mathematical model used in scaling laws is the power law, expressed as:\n",
       "\n",
       "   \\[\n",
       "   P(n) = C \\times n^\\alpha\n",
       "   \\]\n",
       "\n",
       "   where \\(P(n)\\) is the model performance, \\(n\\) is a measure of model size or computational effort, \\(C\\) is a constant, and \\(\\alpha\\) is a scaling exponent that determines how performance scales with size.\n",
       "\n",
       "2. **Law of Diminishing Returns**: This is often observed in scaling laws, where as models increase in size, the gains in performance decrease. Mathematically, this is expressed through sub-linear scaling, where \\(\\alpha < 1\\).\n",
       "\n",
       "3. **Learning Curves**: These are graphical representations that show the relationship between the size of the training dataset and the performance of a model. The asymptotic nature of these curves often follows a power law, indicating that performance improvements decrease with larger datasets.\n",
       "\n",
       "4. **Compute-Optimal Scaling Laws**: These laws aim to identify the most efficient use of computational resources for training models. The compute-optimal scaling law is often represented as:\n",
       "\n",
       "   \\[\n",
       "   L \\propto N^{-\\beta}\n",
       "   \\]\n",
       "\n",
       "   where \\(L\\) is the loss, \\(N\\) is the computation (often measured in floating-point operations), and \\(\\beta\\) is the scaling exponent reflecting how loss decreases with increased computation.\n",
       "\n",
       "### Predictive Models\n",
       "\n",
       "Researchers often employ predictive models to forecast the scaling behavior of machine learning systems. One such model is the neural scaling law, which predicts the performance of neural networks based on parameters, dataset size, and training compute. These models are invaluable for guiding the design and deployment of large-scale AI systems, ensuring that resources are used effectively to achieve desired performance levels.\n",
       "\n",
       "### Implications and Applications\n",
       "\n",
       "Understanding the mathematical foundation of scaling laws is crucial for advancing artificial intelligence and machine learning. By leveraging these laws, researchers and practitioners can make informed decisions about model architectures, size, and training regimes to optimize performance. This knowledge is especially pertinent in the era of large-scale models, where resource allocation and efficiency are paramount.\n",
       "\n",
       "In conclusion, the mathematical underpinnings of scaling laws provide a robust framework for predicting model behavior in response to changes in size and computational resources. Mastery of these principles allows for the strategic development of AI systems that are both powerful and resource-efficient.\n",
       "\n",
       "---\n",
       "\n",
       "## Impact on Model Design and Architecture\n",
       "\n",
       "Scaling laws have emerged as a critical factor in shaping the design and architecture of language models. These laws provide insights into how model performance scales with changes in parameter size, dataset volume, and computational resources. Understanding these relationships is crucial for developing efficient and effective language models.\n",
       "\n",
       "### Parameter Size Considerations\n",
       "\n",
       "One of the primary aspects influenced by scaling laws is the parameter size of language models. As scaling laws suggest, increasing the number of parameters generally leads to improved model performance, provided that other factors such as dataset size and computational power are adequately scaled. This has resulted in the development of increasingly large models, such as GPT-3 and beyond, which possess billions of parameters.\n",
       "\n",
       "However, simply increasing parameter size poses challenges, including overfitting and diminishing returns. As models grow, they require careful tuning and regularization to maintain performance gains without incurring excessive computational costs. Researchers must strike a balance between model complexity and efficiency, often experimenting with sparsity and modular architectures to optimize resource usage.\n",
       "\n",
       "### Training Data Requirements\n",
       "\n",
       "Scaling laws indicate that larger models typically benefit from more extensive and diverse training datasets to reach their full potential. This has led to an emphasis on curating and utilizing vast corpora, incorporating multilingual and multimodal data to enrich the training process. As a result, data preprocessing, quality control, and curation have become integral components of model development.\n",
       "\n",
       "The interaction between model size and data volume is non-linear, necessitating strategic decisions about data augmentation and selection to maximize performance gains. Furthermore, the ethical implications of data collection, including bias and privacy concerns, must be addressed to ensure that scaled models are both performant and responsible.\n",
       "\n",
       "### Computational Resources\n",
       "\n",
       "Scaling laws have profound implications for the computational resources required to train and deploy large language models. The exponential growth of parameter sizes and data volumes demands significant computational power, often necessitating specialized hardware such as TPUs and GPUs. This need for high-performance computing infrastructure has led to increased collaboration between AI researchers and hardware manufacturers.\n",
       "\n",
       "Efficient resource utilization is crucial, prompting innovations in distributed training, model parallelism, and mixed precision techniques. These advancements aim to reduce training time and energy consumption, making large-scale models more accessible and sustainable. As a result, the design and architecture of language models are increasingly influenced by the availability and efficiency of computational resources.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In summary, scaling laws have a profound impact on the design and architecture of language models, dictating decisions about parameter size, training data, and computational resources. As these models continue to grow in size and complexity, ongoing research and innovation will be essential to harness their full potential while addressing the associated challenges. By understanding and leveraging scaling laws, the AI community can continue to advance the state-of-the-art in natural language processing.\n",
       "\n",
       "---\n",
       "\n",
       "## Case Studies and Applications\n",
       "\n",
       "In recent years, scaling laws have emerged as a critical area of study in the development of large-scale language models. By understanding the relationships between model size, dataset size, and computational resources, researchers and engineers can better predict the performance and capabilities of language models. This section explores a series of case studies and real-world applications where scaling laws have been successfully applied, highlighting their practical impact on developing state-of-the-art language models.\n",
       "\n",
       "### Case Study 1: GPT-3 by OpenAI\n",
       "\n",
       "OpenAI's GPT-3 serves as a prominent example of the application of scaling laws in language model development. Building on the foundation laid by its predecessors, GPT-3 leverages a massive architecture with 175 billion parameters. The development process was heavily guided by scaling laws, which informed the decisions regarding model size and the amount of training data required. By adhering to these principles, GPT-3 achieved unprecedented performance in natural language processing tasks, from text completion to translation, demonstrating the transformative potential of scaling laws in enhancing model capabilities.\n",
       "\n",
       "### Case Study 2: DeepMind‚Äôs Gopher\n",
       "\n",
       "DeepMind's Gopher project further illustrates the utility of scaling laws. Gopher, a 280-billion-parameter language model, was designed to explore the role of scaling in achieving human-like performance across various tasks. By analyzing scaling laws, DeepMind was able to optimize the balance between model size and dataset diversity, resulting in significant improvements in model performance. This case study underscores the importance of scaling laws in creating models that not only excel in individual tasks but also demonstrate robust generalization across diverse domains.\n",
       "\n",
       "### Application 1: Healthcare\n",
       "\n",
       "In the healthcare sector, scaling laws have been instrumental in the development of language models for medical applications. For instance, models designed to process and analyze large volumes of medical literature and patient records benefit from scaling laws, which guide the optimal sizing of models and datasets. This has led to advancements in areas such as automated diagnosis, personalized medicine, and the generation of medical reports, where scaled models provide enhanced accuracy and reliability.\n",
       "\n",
       "### Application 2: Financial Services\n",
       "\n",
       "The financial industry has also embraced scaling laws to refine language models for tasks like sentiment analysis, fraud detection, and market trend prediction. By applying scaling laws, financial institutions can develop models that process vast datasets efficiently, yielding insights that drive decision-making and strategy. The ability to scale models appropriately ensures that financial services can leverage comprehensive data analysis to maintain a competitive edge.\n",
       "\n",
       "### Application 3: Education\n",
       "\n",
       "Educational technology has seen transformative changes through the application of scaling laws in language model development. Scaled models are now used to create adaptive learning platforms that cater to individual student needs, providing personalized feedback and support. By adhering to scaling laws, educational tools have improved in delivering accurate content recommendations and facilitating more engaging learning experiences, thereby enhancing educational outcomes.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "These case studies and applications demonstrate the profound impact that scaling laws have on the development and deployment of state-of-the-art language models. By informing critical design choices, scaling laws enable researchers and practitioners to create models that are not only larger and more powerful but also more efficient and adaptable to real-world applications. As the field continues to evolve, the insights derived from scaling laws will remain pivotal in shaping the future of language model development across diverse industries.\n",
       "\n",
       "---\n",
       "\n",
       "## Challenges and Limitations\n",
       "\n",
       "The application of scaling laws in the development of machine learning models offers substantial benefits, yet it is not without its challenges and limitations. Key issues include diminishing returns, data efficiency, and the environmental impact of scaling large models.\n",
       "\n",
       "### Diminishing Returns\n",
       "\n",
       "One of the primary challenges associated with scaling laws is the phenomenon of diminishing returns. As models grow in size, the incremental gains in performance often decrease. This means that while initial increases in model parameters can lead to significant improvements in capabilities, continual scaling results in progressively smaller performance enhancements. This is particularly evident when models approach or exceed the current state-of-the-art benchmarks, where further scaling yields minimal improvements relative to the resources invested.\n",
       "\n",
       "### Data Efficiency\n",
       "\n",
       "Data efficiency is another critical limitation when applying scaling laws. Large models require vast amounts of data to achieve optimal performance, and the acquisition and processing of such data can be resource-intensive and time-consuming. Moreover, the assumption that larger datasets will always lead to better model performance does not hold universally. In many cases, models may require careful tuning and additional data preprocessing to manage issues such as data imbalance, noise, and redundancy, which can inhibit learning efficiency and negatively impact performance.\n",
       "\n",
       "### Environmental Impact\n",
       "\n",
       "The environmental impact of scaling large models is an increasingly pressing concern. Training and maintaining large-scale models demand significant computational resources, which translate into substantial energy consumption and carbon emissions. This environmental footprint raises ethical and sustainability questions, particularly as the AI community strives to balance innovation with ecological responsibility. Efforts to mitigate this impact through more energy-efficient hardware and algorithmic innovations are ongoing, yet the challenge remains significant.\n",
       "\n",
       "In summary, while scaling laws have driven remarkable advancements in machine learning, they pose notable challenges and limitations. Addressing these issues requires a balanced approach that considers both the technological benefits and the broader implications of model scaling. Continued research and innovation are essential to overcoming these hurdles and ensuring the sustainable development of AI technologies.\n",
       "\n",
       "---\n",
       "\n",
       "## Future Directions and Research Opportunities\n",
       "\n",
       "The study of Large Language Models (LLMs) and their scaling laws presents a fertile ground for future research and innovation. As these models continue to evolve and expand in their capabilities, several key areas emerge as promising directions and opportunities for further exploration:\n",
       "\n",
       "### 1. Understanding and Optimizing Scaling Laws\n",
       "\n",
       "Research into the fundamental principles governing LLM scaling laws is crucial for optimizing model design and performance. Future studies could focus on:\n",
       "\n",
       "- **Refining Scaling Predictions:** Developing more accurate models that predict performance outcomes based on scaling parameters such as model size, training data volume, and computation resources.\n",
       "- **Efficiency in Resource Utilization:** Investigating methods to achieve greater efficiency in scaling, potentially through advanced algorithms that reduce computational overhead and energy consumption.\n",
       "\n",
       "### 2. Exploring Ethical and Societal Implications\n",
       "\n",
       "As LLMs grow in scale and influence, understanding their ethical and societal impacts becomes increasingly important. Key areas for future research include:\n",
       "\n",
       "- **Bias and Fairness:** Developing techniques to identify, mitigate, and measure biases that may be amplified as models scale.\n",
       "- **Accountability and Transparency:** Exploring frameworks for ensuring that LLMs operate transparently and are held accountable for their outputs in various applications.\n",
       "\n",
       "### 3. Advancements in Multimodal Integrations\n",
       "\n",
       "The integration of LLMs with other modalities (e.g., vision, audio) offers exciting opportunities for creating more versatile and comprehensive AI systems. Future research could delve into:\n",
       "\n",
       "- **Cross-Modal Learning:** Investigating how scaling laws apply when combining text with other data types, potentially leading to breakthroughs in fields such as automated content creation and interactive AI systems.\n",
       "- **Unified Architectures:** Developing architectures that seamlessly integrate multiple modalities and scale efficiently across diverse tasks.\n",
       "\n",
       "### 4. Enhancing Model Interpretability\n",
       "\n",
       "As LLMs become more complex, enhancing their interpretability is critical for their effective deployment. Potential areas of research include:\n",
       "\n",
       "- **Mechanistic Interpretability:** Unraveling the internal processes of LLMs to understand how they make decisions and generate outputs.\n",
       "- **Interactive AI Systems:** Creating systems that allow users to interactively explore and understand model reasoning in real-time.\n",
       "\n",
       "### 5. Expanding Applications in Specialized Domains\n",
       "\n",
       "LLMs hold great promise for specialized applications across various domains. Future research could focus on:\n",
       "\n",
       "- **Domain-Specific Models:** Developing LLMs tailored to specific fields, such as healthcare, finance, or law, and exploring their scaling dynamics in these contexts.\n",
       "- **Real-Time Adaptation:** Investigating how LLMs can be adapted or fine-tuned on-the-fly to respond to dynamic environments and evolving data.\n",
       "\n",
       "### 6. Collaborative and Open Research Initiatives\n",
       "\n",
       "Encouraging collaborative and open research efforts can accelerate advancements in the understanding and application of LLM scaling laws. Future initiatives could include:\n",
       "\n",
       "- **Open Datasets and Benchmarks:** Establishing shared datasets and benchmarks to foster reproducibility and comparative analysis across studies.\n",
       "- **Interdisciplinary Collaboration:** Promoting partnerships between computer scientists, ethicists, and domain experts to address multifaceted challenges posed by LLM scaling.\n",
       "\n",
       "In conclusion, the study and application of LLM scaling laws offer numerous opportunities for groundbreaking research and innovation. By focusing on optimizing scaling principles, addressing ethical considerations, and expanding application domains, the research community can harness the full potential of LLMs to advance technology and society.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this report, we have explored the pivotal role that scaling laws play in the advancement of language model technology. These laws, which describe how model performance improves with increased computational resources and data, have become a cornerstone in the development of large-scale AI systems. By systematically analyzing these laws, researchers have been able to predict the performance gains that can be achieved as models grow in size and complexity.\n",
       "\n",
       "One of the key points discussed is the exponential relationship between model size, dataset size, and computation, and their collective impact on performance. As demonstrated, larger models trained on extensive datasets tend to perform better across a wide range of tasks, including language understanding, translation, and generation. This relationship underlines the importance of continued investments in computational infrastructure and data collection to push the boundaries of what these models can achieve.\n",
       "\n",
       "Furthermore, the report highlights the potential future implications of these scaling laws. As models continue to grow, there is an increasing need to address challenges such as energy consumption, ethical considerations, and accessibility. The scalability of these models also raises questions about their deployment in real-world applications, where trade-offs between performance and efficiency must be carefully managed.\n",
       "\n",
       "In conclusion, scaling laws offer a valuable framework for guiding the development of language models. By understanding and leveraging these principles, the AI community can achieve significant breakthroughs, ultimately leading to more powerful and versatile language technologies. As we look to the future, it is crucial to balance innovation with responsibility, ensuring that the benefits of these advancements are widely accessible and ethically aligned with societal values."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(orchestrator_worker.get_state(config).values[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-F0L5SJfm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
