{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'Nvidia 주가 하락 원인'},\n",
       "  'id': 'call_DTmXVQBRMHfDoq4Q6PshktlX',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "\n",
    "class SearchQuries(BaseModel):\n",
    "    search_query: list = Field(\n",
    "        None, description=\"List of Queries that are optimized for web search.\"\n",
    "    )\n",
    "    justification: list = Field(\n",
    "        None, description=\"Why each query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuries)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "\n",
    "# Define a tool\n",
    "tavily_tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([tavily_tool])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\n",
    "    \"nvidia 주가 하락의 원인이 된 대상 이름이 뭐야?그리고 그 대상에 대한 정보 알려줘\"\n",
    ")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "class SearchQury(BaseModel):\n",
    "    search_query: str = Field(\n",
    "        None, description=\"Query that is optimized for web search.\"\n",
    "    )\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    \"\"\"Model for a reference\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The title of the reference.\")\n",
    "    url: str = Field(description=\"The url of the reference.\")\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    search_queries: list[SearchQury]\n",
    "    combined_output: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "orchestrator_worker = orchestrator_worker_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "async def astream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: Dict[str, Any],\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable[[Dict[str, str]], None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 비동기 방식으로 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    매개변수:\n",
    "    - graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "    - inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "    - config (RunnableConfig): 실행 설정\n",
    "    - node_names (List[str], optional): 출력할 노드 이름 목록 (빈 리스트면 모든 노드 출력)\n",
    "    - callback (Callable[[Dict[str, str]], None], optional): 각 청크 처리를 위한 콜백 함수 (기본값: None)\n",
    "      콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    반환값:\n",
    "    - None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    async for chunk_msg, metadata in graph.astream(\n",
    "        inputs, config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 포함된 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            else:\n",
    "                # 노드가 변경될 때마다 헤더를 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"🔹 Node: \\033[1;34m{curr_node}\\033[0m 🔹\")\n",
    "                    print(\"=\" * 60)\n",
    "\n",
    "                # 콘텐츠를 보기 좋게 출력\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "async def astream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: Dict[str, Any],\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable[[Dict[str, str]], None] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 비동기 방식으로 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    매개변수:\n",
    "    - graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "    - inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "    - config (RunnableConfig): 실행 설정\n",
    "    - node_names (List[str], optional): 출력할 노드 이름 목록 (빈 리스트면 모든 노드 출력)\n",
    "    - callback (Callable[[Dict[str, str]], None], optional): 각 청크 처리를 위한 콜백 함수 (기본값: None)\n",
    "      콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    반환값:\n",
    "    - None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    async for chunk_msg, metadata in graph.astream(\n",
    "        inputs, config, stream_mode=\"messages\"\n",
    "    ):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 포함된 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            else:\n",
    "                # 노드가 변경될 때마다 헤더를 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 60)\n",
    "                    print(f\"🔹 Node: {curr_node} 🔹\")\n",
    "                    print(\"=\" * 60)\n",
    "\n",
    "                # 쿼리 정보가 있으면 출력\n",
    "                tool_calls = chunk_msg.additional_kwargs.get(\"tool_calls\", [])\n",
    "                if tool_calls:\n",
    "                    # print(f\"🔍 Query: \", end=\"\")\n",
    "                    for tool_call in tool_calls:\n",
    "                        query = tool_call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "                        if query:\n",
    "\n",
    "                            print(f\"\\033[1;33m{query}\\033[0m\", end=\"\", flush=True)\n",
    "                else:\n",
    "                    # 쿼리 정보가 없으면 content 출력\n",
    "                    content = chunk_msg.content.strip()\n",
    "                    if content:\n",
    "                        print(f\"{content}\", end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 1,  # temporary thread ID for testing\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = State(\n",
    "    topic=\"Create a report on LLM scaling laws\",\n",
    "    sections = [],\n",
    "    completed_sections = [],\n",
    "    final_report = \"\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔹 Node: orchestrator 🔹\n",
      "============================================================\n",
      "\u001b[1;33m{\"\u001b[0m\u001b[1;33msections\u001b[0m\u001b[1;33m\":[\u001b[0m\u001b[1;33m{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mIntroduction\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33m Scaling\u001b[0m\u001b[1;33m Laws\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m introduces\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m concept\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m context\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m large\u001b[0m\u001b[1;33m language\u001b[0m\u001b[1;33m models\u001b[0m\u001b[1;33m (\u001b[0m\u001b[1;33mLL\u001b[0m\u001b[1;33mMs\u001b[0m\u001b[1;33m).\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m cover\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m basics\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m what\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m are\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m why\u001b[0m\u001b[1;33m they\u001b[0m\u001b[1;33m are\u001b[0m\u001b[1;33m important\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m development\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m improvement\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mHistorical\u001b[0m\u001b[1;33m Development\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m Scaling\u001b[0m\u001b[1;33m Laws\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m del\u001b[0m\u001b[1;33mves\u001b[0m\u001b[1;33m into\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m history\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m evolution\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m machine\u001b[0m\u001b[1;33m learning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m particularly\u001b[0m\u001b[1;33m focusing\u001b[0m\u001b[1;33m on\u001b[0m\u001b[1;33m how\u001b[0m\u001b[1;33m they\u001b[0m\u001b[1;33m have\u001b[0m\u001b[1;33m been\u001b[0m\u001b[1;33m applied\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m language\u001b[0m\u001b[1;33m models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m highlight\u001b[0m\u001b[1;33m key\u001b[0m\u001b[1;33m research\u001b[0m\u001b[1;33m milestones\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m breakthroughs\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mKey\u001b[0m\u001b[1;33m Principles\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33m Scaling\u001b[0m\u001b[1;33m Laws\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m explores\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m fundamental\u001b[0m\u001b[1;33m principles\u001b[0m\u001b[1;33m that\u001b[0m\u001b[1;33m underpin\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m including\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m mathematical\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m theoretical\u001b[0m\u001b[1;33m frameworks\u001b[0m\u001b[1;33m used\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m understand\u001b[0m\u001b[1;33m these\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m also\u001b[0m\u001b[1;33m cover\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m key\u001b[0m\u001b[1;33m variables\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m metrics\u001b[0m\u001b[1;33m involved\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mPr\u001b[0m\u001b[1;33mactical\u001b[0m\u001b[1;33m Imp\u001b[0m\u001b[1;33mlications\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m Scaling\u001b[0m\u001b[1;33m Laws\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m discusses\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m practical\u001b[0m\u001b[1;33m implications\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m for\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m development\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m deployment\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m examine\u001b[0m\u001b[1;33m how\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m influence\u001b[0m\u001b[1;33m decisions\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m model\u001b[0m\u001b[1;33m architecture\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m data\u001b[0m\u001b[1;33m requirements\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m computational\u001b[0m\u001b[1;33m resources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m performance\u001b[0m\u001b[1;33m optimization\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mChallenges\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m Limit\u001b[0m\u001b[1;33mations\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m addresses\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m challenges\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m limitations\u001b[0m\u001b[1;33m associated\u001b[0m\u001b[1;33m with\u001b[0m\u001b[1;33m applying\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m discuss\u001b[0m\u001b[1;33m issues\u001b[0m\u001b[1;33m such\u001b[0m\u001b[1;33m as\u001b[0m\u001b[1;33m computational\u001b[0m\u001b[1;33m costs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m diminishing\u001b[0m\u001b[1;33m returns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m ethical\u001b[0m\u001b[1;33m considerations\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mCase\u001b[0m\u001b[1;33m Studies\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m presents\u001b[0m\u001b[1;33m real\u001b[0m\u001b[1;33m-world\u001b[0m\u001b[1;33m examples\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m case\u001b[0m\u001b[1;33m studies\u001b[0m\u001b[1;33m where\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m have\u001b[0m\u001b[1;33m been\u001b[0m\u001b[1;33m successfully\u001b[0m\u001b[1;33m applied\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m improve\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m analyze\u001b[0m\u001b[1;33m specific\u001b[0m\u001b[1;33m models\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m their\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m journeys\u001b[0m\u001b[1;33m to\u001b[0m\u001b[1;33m illustrate\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m concepts\u001b[0m\u001b[1;33m discussed\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mFuture\u001b[0m\u001b[1;33m Directions\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m Research\u001b[0m\u001b[1;33m Opportunities\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m explores\u001b[0m\u001b[1;33m potential\u001b[0m\u001b[1;33m future\u001b[0m\u001b[1;33m directions\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m study\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m application\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m for\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m highlight\u001b[0m\u001b[1;33m emerging\u001b[0m\u001b[1;33m trends\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m technologies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m areas\u001b[0m\u001b[1;33m for\u001b[0m\u001b[1;33m further\u001b[0m\u001b[1;33m research\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m},{\"\u001b[0m\u001b[1;33mname\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mConclusion\u001b[0m\u001b[1;33m\",\"\u001b[0m\u001b[1;33mdescription\u001b[0m\u001b[1;33m\":\"\u001b[0m\u001b[1;33mThis\u001b[0m\u001b[1;33m section\u001b[0m\u001b[1;33m summarizes\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m key\u001b[0m\u001b[1;33m points\u001b[0m\u001b[1;33m discussed\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m report\u001b[0m\u001b[1;33m and\u001b[0m\u001b[1;33m reflects\u001b[0m\u001b[1;33m on\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m significance\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m advancing\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m capabilities\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m L\u001b[0m\u001b[1;33mLM\u001b[0m\u001b[1;33ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m It\u001b[0m\u001b[1;33m will\u001b[0m\u001b[1;33m also\u001b[0m\u001b[1;33m offer\u001b[0m\u001b[1;33m final\u001b[0m\u001b[1;33m thoughts\u001b[0m\u001b[1;33m on\u001b[0m\u001b[1;33m the\u001b[0m\u001b[1;33m future\u001b[0m\u001b[1;33m of\u001b[0m\u001b[1;33m scaling\u001b[0m\u001b[1;33m laws\u001b[0m\u001b[1;33m in\u001b[0m\u001b[1;33m AI\u001b[0m\u001b[1;33m development\u001b[0m\u001b[1;33m.\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]}\u001b[0m\n",
      "============================================================\n",
      "🔹 Node: llm_call 🔹\n",
      "============================================================\n",
      "##CaseStudiesInthissection,##Challenges##KeyandLimitPrinciplesof##PracticalationsImplicationsLLMofScalingScalingLawsTheapplication##LawsIntroductionScalinglawsScalinglawsinlargelanguageofscalingtoLmodels(LLMshaveemergedlawsto##FutureDirectionsandasalargelanguageResearchOpportunities)provideastructuredmodels(foundationalconceptframeworkforTheunderstandinghowinthefieldofLLMsthesemodelsrealmof)presentsseveralinherentLMScaling'performancelargelanguagechallengesandmodels(largelanguageLLMs##Conclusionlimitations.models(),significantlyThroughoutLLMsandcapabilitiesAsthese)isthisreportLawsinfluencingtheir,wedevelopmentandrapidlyevolvewithevolvingchangesinmodelsgrowdeployment.,drivenhaveexploredScalinglawssize,byadvancesthepivotalByunderstandinginsizedata,theselawsandcomputationalroleof,researchersresources.scalinglawsarefundamentalTheselawsinaregroundedshapingincomputationalandcomplexitypowerinmathematical,principlesthat,variousandengineersissuesbecome###HistoricalDevelopmentofthedevelopmentincreasinglyapparentdataavailabilitypertainScalingLawscanmakeThe,necess,andinformeddecisionsregardingmodelandenhancementandtheoreticalconceptoftotheitatingcarefulprinciplesinnovativemethodologiesthathelpresearchersscalinglaws.Asconsiderationandarchitecture,andengineerspredicttheinmachineeffectsofscalingongrowthandmodelbehaviorwecontinuedatarequirementslearning,toexploreoflarge.This,computationalthepotentialsectiondelmitigationstrategiesresources,vesintoofscalingparticularlywithin.###andperformanceperformanceofthedomainoflanguagethesefoundationaloptimization.ComputationalCostslanguagemodelsprinciples,models,lawsinthemathematicalframeworksemployedOnehasundergone,and###Influencelargelanguage(LLthekeyoftheonModelMs).significantevolutionthedevelopmentArchitecturevariablesandScalinglawsmetricsprimarychallengesrelevantmodels(provideaovertheOneofandapplicationtheprimarytoscalinginscalingpastfewdecades.ofLimplicationsofLLMLMsLLMsframeworkforunderstandinghowScalinglaws,severalchangesin.###sisrefertoscalinglawspromisingdirections).Theseistheirthepredictablethesubstantialmodelsizeroleincomputationalcost,dataguidingthewaysinlawsdescribevolume,Mathematicalanddesignofwhichtheinvolved.modelarchitecturesandcomputationalperformanceofTheoretical.AsAsmodelsandresearchmachinelearningresourcesimpacthowchangesopportunitiesemergescalinglawsscale,.###inthetheperformancemodelsimproveselucidatebothintermsofandcapabilitiesFrameworksasatherelationshipfunctionofparametersandsizeofbetweenmodelEmergingTrendsdatasetsizeofLLMssize,,theTheamodelincreaseddatademandfordataquantity.ThisinScaling,and—whetherLawscomputationalresourcesperformance,,modelunderstandingis1.growsexponentiallytheyprovidescalinglawscrucialforguidingthedesignand.Trainingablueprintdeploymentof**Efflarge-scaleintermsfordevelopingmoreefficientforLLMsicientModelmodelsrequiresandeffectiveofparametersAIsystemsarelargelyextensivehardwareScaling:infrastructure,architecturesthatderivedfrommaximizeefficiency,dataandeffectiveness.Designersincludinghigh**empiricalobservations-performanceGPUs.KeyandtheoreticalWiththecanleverage,ormodelsthatorTPgrowingsizepointsdiscussedrelateUs,modelincludethetheseinsightscomputationalresourcessize,ofLandsignificantenergyconsumptionempiricalobservationdataquantitytodetermine,andthatlarger—affparameters,orcomputationalresources.Understandingtheselawshasbeencrucialinoptimizingmodelsgenerallycomputepowermodelarchitecturesandguidingtheallocation.Thisperformbetterofcomputationaloptimalmodelnotonlytoperformancesizesthatmetrics.raisesoperationalacrossaectitsOneofvarietyofbalancecomplexitytasks,budgets.thecorecostsbutalsolimitscapabilitiesandelementsofwithperformance####Earlygains,accessibilitytoprovidedtheyDevelopmentsensuringthatafewefficiency.theseframeworksaretrainedthearchitectureorganizationswithTheonsufficientlycanhandlelargedatasetssufficientresourcesistheUnderstandingscalingearlyrootsthedemandspower-lawofscalingLMs,thereisanincreasingemphasisondevelopingwithadequatemoreefficientscalingtechniques,potentiallyrelationship,oflargelawsis-scaledatacomputationalresourceslawscanstifwhichsuggests.Thisthatoptimizebetracedlinginnovationperformancewithoutbacktorelationshipunderscoresprocessingwithoutthebroaderthatperformancecrucialforimprovementsaretheapowerimportanceandequitableproportionalincreasesofbalancingfunctionoffieldofresearchersandstatisticallearningincreasesinunnecessarycomputationalthetritheory,accesstomodelparametersdevelopersbecausewhichbeganadof,datasetmodelsizetoexplore,datahowsampleAIoverhead.advancementssize,theyprovide###Datacomplexityand,and.###orcomputemodelcapacityRequirementsincomputationalresources.aroadmaprelatetoDimin1weexploreScalinglawsgeneralization.**real-worldishingReturnscomputetoandenergyapplicationsofscalinglawsperformanceoptimize.performancePower-LintheAnother.ThealsocriticallyforoptimizingdevelopmentandawRelationshipsInthereportalsoenhancementofLargeLanguagecosts.impactdata**:199Models(criticallimitationLLMsrequirementsforTheperformancethedevelopment0sistheResearchintosparsearchitectureshighlightedtheofLLLMphenomenonofLMs,model,researcherss.ofLdiminishingreturns,oftenAstheselikeVappruning,measuredindiminishingreturns).Throughthesecasestudies,weaimtodemonstratehowscalinglawscanbeeffectivelyutilizedtonikandthatmayimprovemodelperformance.WewillandquanttermsofLMslawsdictateCherv.WhiledelveintoizationpresentsscalingLlossorspecificmodels,examiningoccurastheirscaling,ensuringjourneysandopportunitiestoonenktheresultantaccuracy,advancementsincapabilities.###CaseStudyLMstypicallyfollows1:OpenAIisintroduced'sGPTmaintainor-3apowerthatperformancemodelsgrowfoundationalconcepts-lawdecaythatthesecanleadsuchasasaVCdimensionenhancemodeltoimproved,emphasizingfunctionofmodelsizetypicallyimprovesmodelsareOpenAI'sdevelopmentofGPT-3servesasaseminalexampleofscalinglawscapabilitieswhileintheneedaction.GPT-3,amodelwith175billionparameters,,whichshowcaseshowincreasingthenumberofparameterscansignificantlyenhancereducingresourcealanguagemodel'scapabilities.Thescalingforinnovativebotheffectiveperformance,demandsprovidedearly.journeyfromitspredecessor,GPT-2,involvedmeticulouslyapplyingscalinglawstodetermineoptimalmodelthemarginalsizeandarchitecture.ThisapproachledtothecreationofGPT-3gainsdecreasewithincreased,whichexhibitssuperiorinsightsintoproficiencyintaskssuchaslanguagehowmodel2.translation,questionanswering,andtextgeneration.TheapproachestosuccessofGPT-asmodels3underscorestheimportanceofscalinginachievingstate-of-the-artandefficient**Intercomplexitycouldmaintainefficiencybecomelargerimpactlearningpretable.Thisdatavolumeefficacy.andpracticalitydiminishingreturn.AtScalingLawsperformance.###CaseStudy2:DeepMind'sChinchillaDeeponinvestment####EmerMind'sgenceinitscoreinscaling:**Chinch,thequestionstheNeuralNetworksillamodelemphasisonsustainabilityand.ThehighlightstheUnderstandingobtainingand,atheunderlyingapplicationofsignificanceofThescalinglawscuratingresurgenceofpracticalityofneuralnetworkstobalancescalinglawlargedatasetsscalinglawsbecomesparamountmodelsizeindefinitelyincreasinginthe.Howevermodelsizeandtrainingextendsbeyond201mechanismsof,scaling.Researchersisadata.0sUnliketraditionallawsalsoarethusmereperformanceapproachesthatsuggestdiminishingchallengedtomathematicalrelationshipmarkeda(numberfocussolelyimprovements.findaonincreasingpivotalpointscalinglawsreturnsatTheyhavethatpredictsinthecertainscalesbalancebetweenmodelparametersthebenefits,highlightingprofoundimplicationsofparameterstheimportanceexplorationof,Ch),datasetofscalingiscrucialscalinglawsfortheinchillaofnotsize,howaandthe.Initialandcomputeemphasizesproportionalefficiencyofstudiesfocuseddemocratizationscalingof.Thisjustquantityresourceutilizationmodel'srelationshipisbothparametersonunderstandingofAIformoreanddatabutqualityhowincreasingperformance,characterizedbytoachieveanddiversity.Innovationstechnology,adiminishinginmodelthedepthofdataefficiency.asunderstandingreturnonarchitecture,.ThisByadheringsuchasperformancegainsandwidththeselawspredictableandasthenecessitatesofneuralnetworksaffectedscaleincreasestoscalingcanleaditsaccuracysuchas,indicatingstrategicplanningtheirabilityreliablemodeltotheindataefficienttransformersthatwhilelaws,DeepMindcollectionandlargermodelstolearndevelopmentoforspeedcomplexpatternswasablepreprocessingtoandhybridperformbetter,thetodevelopsmaller,models,incrementalimprovements.Theamodel,scalesensurethatbecomeprogressivelymoreaccessibleempiricalsuccesssmaller.withcomparableadditionaldataarebeingmodelsthatofdeepcontributesmeaning2.performance.performancetofullytowithits**EffectiveexploredtomodeltrainingretaincompetitivelargercounterpartsDataUtilResearchaimedlearning,.###ization**whilerequiringaddressthisissue.ComputationalResourcessize.:Theperformancelevelsoreticalmodelsfewercomputationalexemplifiedresources.###Ethicalatelucid.ThisConsiderationsalsoemphasizeForlanguageTheScalingbyAlexatingthedemocratizationThisstrategictheinteractionpracticaldeploymentNetiniscriticalLLMmodels,ofLLMssalso201forensuringmathematicalfoundationsscalingcontributedbetweenmodelthatthethismightoftheseraisessignificanttoChisheavilyinfluencedbybenefitsofethicalconcerns2,inchilla.Largercomputationalresourceinvolveincreasinglawscould’seffectivenessmodelstendAIarehighlightedtheconsiderationsshapedsizeandwidelydistributedinvariousbyscalingtorequiredataavailabilitythenumber.Aspotentialofmodelsgrowandnotnaturallanguageleadto,theirlaws.vastamountslargermodelsconfinedtoLargermodelsabilitytoprocessingtasksmoreinterpreofdataofparameterstrainedonentitieswitheffectivelyutilizesubstantialresourcesvastdatasetsandextensivefortraining.Lookingorimproves,the,illustratingahead,amountofwhichisdatasetsnaturally,whichcrucialfortrainingdatascalinglawsmoredatatoachievebetterresultsdemandmoretableandwillcontinueunlockingtheirthepotential.Thesepowerfulcomputational,settingtobecontrollableLLMacornerstonecanincludes,sensitiveorproprietaryinformationinAIinfrastructure,development,providingdeeperforoptimizedfullpotential.Theinformingbothwhichcanriskofthestage.Howeverbeatheoreticalresearchinsightsintodatamisusescalingrelationshipstheirdecisionscalingstrategiesandpracticalforfurtherapplications.orbreaches-makingprocesses,beyondaretypicallyAsthetoenhanceexplorationoffieldevolves.3expressedinscalingphenomenamodelperformanceacertaintheform.**significantcostofpowerbecomesafactor.withoutexcessiveScalinglawslaws,resourceconsumptionCross-point,helppredict.####,researcherswhichindicatecriticalconcern.###theresourceTheRoleModalScalingCaseStudythebenefitsofofBig.Moreoverrequirementsasthatperformance3:**additionaldatawilllikelymodelsgrow:Google'simprovementsfollow,theapredictableAsDataand,enablinguncovermorecurveasPaLMthemodelormodelmodelsmayComputenuancedaspectsGoogleLLMscales.AsdataTheimportanceofscalingparametersdiminish'sPathofscaling,aligningavailabilityand,suchsbecomeinadvertentlylearnwaysLanguagewiththecomputationalpowerastheandpropagatelawsinpower-lawinterplaybetweengrew,LLMbiasespresentsliesscaling.Model(moreintegratedintheirPaLMinthemodelarchitectureresearchersbeganabilityto3.)exemplguideresourceandscaling**Computewithmultitrainingdataifiestheallocationandefficiency,-Optimal-modalsystemsdevelopmentstrategiesimpactof,exploring.ByorthescalinglawsataunderstandingtheseScaling**onmulti:Anotherroleofhowscaling-tasklearningcriticalconcepttransferlearninglaws,capabilities.isthelawsapplyinleveraginglargerscaleacrossdifferentPaLMdeveloperscannotionofscaledmodels,potentiallysystematicallyinvestigatingleadingtodatatypesmakeinformedfordiverseharmfulsocietaldecisionsabouthowtoimpacts.theeffectsofscalingmodelsizeandtrainingdata.Alandmarkstudyin2018byAddressingcompute-optandmodalitiestasks.'sscalingallocateadditionalKaplanettheseethicalInconclusiondata,challengesrequirescomputationalpower—suchal.,scalingimalscaling,orlawsare,whichthedevelopmenttimetointegraltoastextjourneyinvolvedtheongoingofrobustreferstoadvancementofachievedesiredLLMatOpen,imagesdatagovernancelevelsofthebalances,offeringaperformance.roadmapforfutureinnovationsbetween.AsmodelAItechnologies,andcontinuetomature,Thisisincreasingbothsize,theinsightsparticularlysignificantgainedfromscalinglawsframeworksandintheaudio—willbethemodeldataAIformallysizeinstrumentalin,andrealmofLLMpushingthecomputeresourcessizetobiasmitigations,540strategiestowillbewherethebillionparameterstoachieveensurefairnessboundariesofintroducedscalingwhatthesesystemscanandaccountabilitythebestandtheanimportantachieve,costsassociatedultimatelyleadingdiversityoftomore.###intelligent,versatile,areaofpossibleperformanceinputdatawithtrainingforagivenamountlawsforandaccessible.Thisanddeployingresearch.EnvironmentalImpactofcomputationalsuchmodelsstrategicscalingcanbeeffort.substantial.ThelanguagemodelseffortenabledThisinvolvesAIsolutionsThiscould.environmentalimpactPaLM,demonstratingoptimizingtheofscalingthatmodelFurthermore,toexcelperformancefollowsinaleadtoLLMpredictablepowerwiderangethedevelopmentlawswithtrade-offsscannotoftasks,fromofmorescalinglawsreasoningandbetweentheseversatileandfactorstotranslationtomaximizeefficiencyrespecttocomplexquestioncomprehensivemodelsbeoverlookedandeffectiveness.The-answeringenergy-intensive.###capableof.ThenatureofKeyVariablestraininglargehelpinhandlingdiversemodelparametersapplicationofandMetricsmodelscontributesscalinglawsinputs.inPatoaUnderstandingforecastingthescalinglawsLM'spotentialandsubstantialcarbonrequiresattentiondevelopmentshowcaseslimitationsoftoseveral###Techn,datahowcarefulexistingmodelskeyvariablesfootprint.considerationofologicalInnovationsandmetricsAsawareness.Theythatdeterminemodelsize1offerinsightssize,anddataandreflecttheperformance.**intohowdiversitycanofLleadtoLMsHardwareAdvsignificantimprovementsoffaraclimate:-currentmodelandcomputationalancements:**Modelingeneraleffort.Size(canbeizationandParametersThisstudy)**changegrowstaskadaptability:underscThepushedin**numberof.###oredtheparametersintermsofcapabilitybeforeAnalysisandamodelFutureresearch,therunningintoInsightsisadiminishingreturns.ThiswillbenefitimportanceofprimaryvariableThesecaseinscalinglarge-scaleforesightfromtheAIcommunitylaws.datasetsandisinvaluablestudiescollectivelyLargermodelsforsettingaregenerallysignificantcomputationalfacesincreasingillustratethepressuretoresourcesrealisticgoalsinmorecapablecontinuedadvancementcriticalroleachievingstateandtimelines,butofscaling-of-thethereisinresearchdevelopmore-artresultslawsinanddevelopmentofspecializedsustainablepracticesatrade-offbetween.####.Thissizeand.Inhardwaredesignedcomputationalfeasibilitysummary,includesoptimizingscalinglaws.-KeyBreakadvancingLalgorithmsfor**DatasetserveasforLthroughsacriticalSize**LMsOne:TheenergyefficiencyLMtechnologiesframeworkfortheadvancementvolumeandandexploringoflarge.Innovationsofthediversityoflanguagemodelsalternativemethodstrainingdatamostnotable.Theybreakthroughsoccurredfortrainingsignificantlyimpactwiththeinprocessorsnotonly.Themodelperformance,suchdevelopmentofelucidate,memorytherelationship.Scalingbetweenmodelasfedersizeandlawshelparchitecture,atedlearningtheTransformerperformancebutalsoaiddeterminethescalingjourneysarchitecture,ormodelinstrategicintroducedbydistillationanddistributedplanning,computingwillVaswanipointatlikelyplayofGPTenablingmoreetal,whichefficientand.inaimtowhichadditional201effectivemodel7.datayieldsdevelopment.-3diminishingreturnsreducethe.-AstheTheTransformerfieldof**ComputeasignificantResources**roleartificialintelligencein’sscalability:Thecontinuestoenablingfurther,particularly,Chevolve,amountofinhandlingscalingandenvironmentalburdenaninlong-rangeenhancingthecomputationalpowerdependencies,-depthunderstandinginchillaefficiencyofofthesethesemodelslawswillwithoutcompromisingremainessential.2availableformadeit.**,andformaximizingthepotentialanidealHybridModelsofLtrainingaffectsperformance.candidatefor:**LMs.PaLMCombiningexploringscaling###InterprethowefficientlyLLMorganizationstolaws.SubsequentabilityandbudgetandswithmodelslikeprovidevaluableBERTamodelallocateresourcesotherAIControlmoreeffectively(201AsL.This8)technologies,LMssuchasinsightsintocanbeandGPTincludesdecisions-3abouthardware(202investments,growinreinforcementlearningsuchassize,GPUsor0)scaled.andsymbolicTPUscapitalizedtheyalsothestrategic,andonthesecloud-basedbecomemoreprinciples,solutionsthatAI,complexandcanprovidethenecessaryTherelationshipachievingunprecedentedscalabilityandflexibility.lessinterprebetweencomputedecisionsunderlying###PerformancepresentsanperformancethroughOptimizationtable.Finally,scalingmodelscalinglawsandperformanceopportunitytooffervaluableiscrucialinsightsintoparameterstoUnderstandingtheperformanceoptimizationcreatehybridstrategies.billionsandforunderstandingByunderstandingdecision-makinghowmodelmodeldevelopmenttrillionsperformancescalesmodelsthatprocessesofcost-benefwithsize.Theleveragetheitscenariosanddataintroductionof,developersinmodelcanimplementtargetedoptimizationGPT-thesemodels3bytechniquestodeployment.enhanceefficiency.OpenAIThis.Key-**mightinvolvestrengthsoftechniqueslikedistillationPerformanceMetricsbecomeschallenging,pruningmultipleapproaches,orepitom**:Commonmetrics,raising.Investigizedtheusedtotakeawayspowerofconcernsaboutatinghowevaluatethescalinglawsimpactofscalinglaws,showcasingtheirtransparencyscalingincludethatlargermodelscouldexhibitemergapplytoaccuracy,andreliabilityentabilitiesloss,andperplexinclude:.Thisity.notpresentintheirthesehybridThesemetricslackofprovidea-**systemscouldsmallercounterpartsquantitativemeasureinterpretabilityposesrisksunlocknewofmodel.ThiscapabilitiesandParameterScalinghasled,especiallycapabilityandapplications.helpassesstoa3.theeffectivenessincriticalparadigmshift**Dec:**AsinhowapplicationssuchentralizedresearchersapproachashealthcarethedesignofscalingTraining:andtraining.###orlegaldemonstratedbyConclusion**systems,oflanguageTheprinciplesmodels,AsdatawhereunderstandingofLprivacyconcernstherationaleLMscalingGPT-grow,behindawithanmodel'sresearchintolawsprovideoutputis3andincreasingfocusdecentralizedandPafederLMatedonscalingessential.arobustasa,increasinglearningapproachesprimarydriverEffortsmodelparametersforscalingtoimproveframeworkforofprogress.####modelinterpretcansubstantiallyLLMRecentAdvancespredictingandenhancecapabilitiesabilityandandFutureoptimizingthe,thoughestablishmechanismsDirectionssisgrowthtrajectoriesitisRecentadvancementsforhumanoflanguagegainingtractionmodels.oversightarecrucialto.ThisByunderstandingcrucialtohavecontinuedbalanceparameterdirectioncouldthemathematicalmaintainingtrusttorefinerelationshipsandgrowthwithourunderstandingleadtoandaccountabilityofscalingcomputationalfeasibilitykeyvariablesthedevelopmentatplaylaws.,researchers.-inAIResearchersareofmodelscanbetter**Datanavigatethesystems.exploringmorethatare-ModelInsummarysophisticatedscalingBalance:**strategies,complexitiesof,whilebothlargesuchasChinchscalinglawsmodelscalingmixtureofexpertsandilla'sandunlocksparsemodelsthefullandprivacysuccesshavedrivenpotentialofhighlights,toLLMtheimportance-preservings.significantadvancementsAstheofproportionmanagetheinLallyscalingmodelsizeLMs,overcomingcomputationalcosts,theyandtrainingareaccompaniedassociatedwithcurrentlimitationsbyadata,rangeofensuringthattrainingenormousindatachallengesandresourceuselimitationsthatisefficientmustbemodels.sharingandwhilemaintainingaddressed.highperformancecollaboration.Balancing.-###AreascomputationalefficiencyAdditionally,**Task,ethicalforFurtherthereisGeneralizationconsiderations,growinginterestindeveloping:**TheandinterpretmoreefficientabilitywithResearchalgorithmsandperformancegainsimprovedgeneralisessentialtoharness1.izationobservedthefullfieldcontinuestoevolvepotentialof**EthinPa,thesethesepowerfulprincipleswillremaincentraltoguidingthedevelopmentanddeploymentmodelsresponsibly.ofincreasinglyicalandLMunderscorespowerfullanguagethemodels.importanceofleveraginghardwaretosupportthecontinuedscalingoflanguagemodels.SocietscalinglawsInsummarynotjustalImpforsizeacts:,but**,thealsoforenhancingtheAsLLMsbreadthandhistoricaldevelopmentdepthoflearningacrosscontinuetoscale,ofscalingdiversetasks.Thesetheirsocietalexamplesreinforcelawsinimpactbecomestheconceptmorepronouncedthatthoughtfulmachinelearningapplicationofscalinglaws.Researchisessentialintothereflectsaforpushingethicalimplicationsthefrontoftheseiersofmodels,trajectoryofwhatLincludingbiases,misinformationLMs,andtheireffectsincreasingcomplexityonemploymentcanachieveandeducation,is,pavingessentialtoandsophisticationthewayensureresponsibleforfuturedevelopmentandinnovationsindeployment.2.artificialintelligence.inmodel**AdaptabilityandDomainSpecialization:design.**AstheWhileLLMsquantizationaregenerallytrainedtobeversatile,theretoreduceisafieldcontinuesneedformodelsthatcanbeeasilyadaptedtospecificdomainsormodelsizetoevolvetasks.whilemaintainingperformancelevelsInvestigating.Additionally,scalinglawscanscalinglawsinformhyperthatfacilitateparametertuning,scalingrapidandtrainingspecializationregimenadjustmentstoachievewithoutextensiveoptimalresultsretrainingwithinresourcecouldleadlawswillconstraints.tomorepracticalapplicationsacrossdiversefields.3.undoubtedlyremain**SInsummary,scalingustainabilitylawsprovideacriticalofScalingframeworkfornavigatingthe:**complexitiesofacentralLLMThedevelopmentanddeployment.environmentalimpactByinfluencingoftrainingarchitecturalchoices,dataever-lstrategy,argerLresourceallocationLMs,andisagrowingconcernoptimizationpractices.Researchfocusedonreducingthe,scalinglawsenablecarbonfootprintthecreationoftheseofmoremodelsthroughefficient,moresustainablethemeinthepursuitofmorepowerfulandcapablelanguageeffective,trainingpracticesandarchitecturesmodels.iscriticalandscalablelanguagemodelsforaligningtechnologicalprogresswithecologicalresponsibility..AsInconclusion,thethefieldcontinuestostudyandevolve,theselawswillremainapplicationofscalinglawsacornerstoneofstrategicplanningandforLinnovationinLMsartificialintelligence.presentarichlandscapeofresearchopportunities.Byaddressingtheseemergingtrends,technologicalinnovations,andcriticalresearchareas,thecommunitycancontinuetopushtheboundariesofwhatispossiblewithLLMswhileensuringtheirdevelopmentisalignedwithsocietalandethicalconsiderations."
     ]
    }
   ],
   "source": [
    "await astream_graph(orchestrator_worker, inputs, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Introduction to LLM Scaling Laws\n",
       "\n",
       "In the rapidly evolving field of artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools for natural language processing and understanding. A crucial concept underpinning the development and optimization of these models is the notion of scaling laws. Scaling laws, in the context of LLMs, refer to the empirical relationships that describe how the performance of a model improves as its size—measured in parameters, data, and compute resources—increases.\n",
       "\n",
       "### A Brief History\n",
       "\n",
       "The concept of scaling laws was first systematically explored in the realm of deep learning through the pioneering work on neural networks. However, it gained significant traction with the advent of LLMs, particularly marked by the development of models such as GPT-2 and GPT-3 by OpenAI. These models demonstrated that larger models trained on more data tend to perform better across a wide range of tasks, a phenomenon that was quantitatively captured by scaling laws.\n",
       "\n",
       "The seminal paper by Kaplan et al. (2020) provided a detailed examination of these scaling laws, showing that model performance, measured in terms of loss, improves predictably as the number of parameters, the size of the dataset, and the amount of compute used during training increase. This work laid the foundation for understanding how to efficiently allocate resources in developing more capable models.\n",
       "\n",
       "### Importance for AI Development\n",
       "\n",
       "Scaling laws are instrumental in guiding the future of AI development for several reasons:\n",
       "\n",
       "1. **Predictive Power**: They allow researchers and engineers to predict the performance gains from increasing model size and compute resources, facilitating informed decisions about resource allocation.\n",
       "\n",
       "2. **Efficiency**: Understanding scaling laws helps in identifying the diminishing returns of scaling, thus preventing unnecessary resource expenditure and optimizing the process of model training.\n",
       "\n",
       "3. **Benchmarking**: These laws provide a benchmark for comparing different models and architectures, helping to establish a baseline for what can be achieved with given resources.\n",
       "\n",
       "4. **Innovation**: By elucidating the relationship between model size, data, and performance, scaling laws encourage the exploration of new architectures and training methodologies that might break away from existing patterns, leading to innovative advancements.\n",
       "\n",
       "In conclusion, scaling laws represent a cornerstone in the development of LLMs, offering a theoretical and empirical framework that guides the strategic scaling of AI models. As the field continues to advance, understanding and leveraging these laws will be essential for pushing the boundaries of what AI can accomplish.\n",
       "\n",
       "---\n",
       "\n",
       "## Mathematical Foundation of Scaling Laws\n",
       "\n",
       "Scaling laws in machine learning and artificial intelligence describe how various elements such as model size, data quantity, computational resources, and performance are interrelated. Understanding these relationships is crucial for effectively designing and optimizing models. This section explores the mathematical principles that govern these scaling laws, providing insight into their foundational aspects.\n",
       "\n",
       "### Model Size and Performance\n",
       "\n",
       "The relationship between model size and performance often follows a power-law distribution. Mathematically, this can be expressed as:\n",
       "\n",
       "\\[ P \\propto S^\\alpha \\]\n",
       "\n",
       "where \\( P \\) represents performance metrics such as accuracy or loss, \\( S \\) denotes the size of the model (often measured in parameters), and \\( \\alpha \\) is a scaling exponent. This relationship suggests that increasing model size can lead to improved performance, but the rate of improvement diminishes as the model grows larger. This phenomenon is linked to the capacity of the model to capture complex patterns in data, which is initially enhanced with more parameters but eventually encounters diminishing returns.\n",
       "\n",
       "### Data Quantity and Model Performance\n",
       "\n",
       "Similarly, the quantity of data available for training is crucial for model performance. The law governing this relationship can be expressed as:\n",
       "\n",
       "\\[ P \\propto D^\\beta \\]\n",
       "\n",
       "where \\( D \\) is the amount of data and \\( \\beta \\) is another scaling exponent. This relationship highlights that more data can lead to better model performance, as it allows the model to learn more robust and generalizable features. However, like model size, the benefits of additional data tend to decrease beyond a certain point, where the model has effectively captured the underlying data distribution.\n",
       "\n",
       "### Computation and Efficiency\n",
       "\n",
       "Scaling laws also encompass the computational resources required to train and deploy models. The computational cost \\( C \\) can be modeled as:\n",
       "\n",
       "\\[ C \\propto S \\times D \\]\n",
       "\n",
       "This equation implies a direct proportionality between computational resources and both model size and data quantity. Efficient utilization of computational resources often involves optimizing hyperparameters and leveraging techniques like model pruning or quantization to reduce unnecessary computations without significantly sacrificing performance.\n",
       "\n",
       "### Interplay of Model Size, Data, and Computation\n",
       "\n",
       "The interplay between model size, data, and computation can be effectively illustrated through the concept of isoresource curves, where performance is plotted against a fixed budget of computational resources. These curves help in understanding trade-offs and making informed decisions about where to allocate resources—whether to increase model size, gather more data, or optimize computational efficiency.\n",
       "\n",
       "### Implications of Scaling Laws in AI\n",
       "\n",
       "The scaling laws provide a framework for predicting how changes in model size, data, or computation will affect performance. They are instrumental in guiding the development of large-scale models, such as those used in natural language processing and computer vision, by highlighting the potential benefits and limitations of scaling different components. Moreover, these laws underscore the importance of balancing model architecture, data strategy, and computational budget to achieve optimal results.\n",
       "\n",
       "In conclusion, the mathematical foundations of scaling laws reveal the intrinsic connections between key factors that drive model performance. By understanding and leveraging these principles, researchers and practitioners can make more strategic decisions in the design and deployment of AI systems.\n",
       "\n",
       "---\n",
       "\n",
       "## Empirical Observations in LLM Scaling\n",
       "\n",
       "In recent years, the scaling of large language models (LLMs) has provided a wealth of empirical data that underscores the theoretical scaling laws posited by researchers. This section delves into the observed patterns and outcomes of scaling LLMs, drawing from a selection of recent studies and projects that have contributed to our understanding of how these models behave as they increase in size and complexity.\n",
       "\n",
       "### Observed Scaling Patterns\n",
       "\n",
       "One of the most consistent observations in LLM scaling is the relationship between model size, computational power, and performance. Kaplan et al. (2020) outlined foundational scaling laws that demonstrate predictable improvements in model performance with increased parameters, compute budget, and dataset size. These laws have been empirically validated across multiple studies, confirming that larger models generally yield better performance on a range of natural language processing (NLP) tasks.\n",
       "\n",
       "For instance, OpenAI's GPT series illustrates a clear trajectory of improvement with each successive model. GPT-3, with 175 billion parameters, significantly outperformed its predecessors across diverse benchmarks, showcasing improved language understanding and generation capabilities. This trend is further corroborated by the release of GPT-4, which, although specifics remain undisclosed, is purported to exhibit even greater enhancements in performance metrics.\n",
       "\n",
       "### Case Studies and Research Findings\n",
       "\n",
       "1. **Chinchilla Scaling Laws**: Hoffmann et al. (2022) presented the Chinchilla scaling laws, which refined previous models by emphasizing the importance of balancing model size and training data. Their experiments demonstrated that underutilizing data relative to model size leads to inefficiencies, suggesting that optimal performance is achieved by aligning the two variables. This insight has steered subsequent model training approaches towards a more data-efficient paradigm.\n",
       "\n",
       "2. **DeepMind's Gopher**: Gopher, a model developed by DeepMind with 280 billion parameters, showcased the practical application of scaling laws. The research highlighted how scaling not only improved traditional NLP task performance but also enabled novel capabilities such as better handling of factual inaccuracies, a prevalent challenge in earlier models.\n",
       "\n",
       "3. **EleutherAI's GPT-NeoX**: EleutherAI's GPT-NeoX project serves as another example where empirical scaling is explored. Their open-source efforts aim to replicate and expand upon the findings of proprietary models, providing a transparent platform for validating scaling laws. Preliminary results indicate that, consistent with previous studies, increasing model size leads to qualitative improvements in output.\n",
       "\n",
       "### Implications of Scaling\n",
       "\n",
       "The empirical evidence supporting scaling laws has profound implications for the future of AI research and development. As models continue to grow, the costs associated with their training and deployment also increase, necessitating innovations in hardware and energy efficiency. Moreover, the scaling of LLMs is accompanied by challenges such as ethical concerns, bias amplification, and data privacy issues, which must be addressed to ensure the responsible advancement of AI technologies.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Empirical observations in LLM scaling confirm the theoretical predictions of scaling laws, revealing a predictable pattern of performance gains with model size. As research continues to refine these laws and explore their limits, the insights gained will be crucial for guiding the development of future models, optimizing resource allocation, and addressing the broader impacts of scaling AI systems. The ongoing work in this field promises to unlock new frontiers in machine intelligence, reshaping our interaction with technology in unprecedented ways.\n",
       "\n",
       "---\n",
       "\n",
       "## Impact of Scaling Laws on Model Performance\n",
       "\n",
       "Scaling laws have become a crucial aspect in the development and optimization of large language models (LLMs), significantly influencing their performance across various dimensions including accuracy, efficiency, and generalization capabilities. This section delves into how these laws shape the capabilities of LLMs and the implications for future developments in artificial intelligence.\n",
       "\n",
       "### Accuracy\n",
       "\n",
       "Scaling laws suggest that as the size of a model — typically defined by the number of parameters — increases, so does the model's accuracy on various tasks. Larger models have demonstrated superior performance in natural language understanding, translation, summarization, and other linguistic tasks. The enhanced accuracy is not merely a result of increased computational resources but also the ability to capture more nuanced patterns and relationships in data. However, the returns on accuracy tend to diminish as models grow excessively large, indicating a point of diminishing returns where additional parameters yield marginal improvements.\n",
       "\n",
       "### Efficiency\n",
       "\n",
       "While larger models often achieve better performance, scaling laws also highlight the trade-offs with computational efficiency. The increase in parameters requires more memory and computational power, which can lead to slower training times and higher energy consumption. This raises concerns about the sustainability and accessibility of deploying such models at scale. Researchers are actively exploring methods to optimize model architectures and training processes to balance the benefits of scaling with the need for efficiency. Techniques such as model pruning, quantization, and knowledge distillation are being developed to mitigate the inefficiencies associated with larger models.\n",
       "\n",
       "### Generalization Capabilities\n",
       "\n",
       "The generalization capabilities of LLMs, or their ability to apply learned knowledge to novel situations, are profoundly affected by scaling laws. Larger models generally exhibit improved generalization, as they can learn more complex patterns from expansive datasets. However, an over-reliance on scaling can lead to overfitting, where a model performs well on training data but poorly on unseen data. This necessitates careful consideration of training regimens and regularization techniques to ensure that models not only scale but also maintain robust generalization capabilities.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In summary, scaling laws play a pivotal role in the evolution of LLMs, influencing key performance metrics such as accuracy, efficiency, and generalization. As we advance in the field of artificial intelligence, understanding and harnessing these laws will be critical for developing models that are not only powerful but also efficient and widely applicable. Future research must continue to explore how to best apply these principles to maximize the benefits of LLMs while addressing the inherent challenges of scaling.\n",
       "\n",
       "---\n",
       "\n",
       "## Challenges and Limitations\n",
       "\n",
       "Scaling large language models (LLMs) presents a series of challenges and limitations that must be considered to optimize their deployment and usage. These challenges can broadly be categorized into computational costs, energy consumption, and diminishing returns, each presenting unique hurdles in the pursuit of more powerful models.\n",
       "\n",
       "### Computational Costs\n",
       "\n",
       "One of the most significant challenges in scaling LLMs is the substantial increase in computational costs. As models grow in size, the demand for advanced hardware and computational resources escalates. Training these models requires powerful GPUs or TPUs, which are not only expensive but also scarce, limiting accessibility to only well-funded institutions or companies. The cost of the infrastructure necessary to support such models grows exponentially with the model size, creating a barrier for smaller entities wishing to leverage advanced LLM technologies.\n",
       "\n",
       "### Energy Consumption\n",
       "\n",
       "Alongside computational costs, energy consumption is a critical concern in scaling LLMs. Training large models involves intensive computational processes that consume vast amounts of energy, contributing significantly to the carbon footprint. As environmental sustainability becomes an increasingly critical global issue, the energy demands of LLMs pose ethical and practical challenges. The industry faces pressure to develop more energy-efficient algorithms and hardware to mitigate environmental impacts while still achieving desired advancements in model capabilities.\n",
       "\n",
       "### Diminishing Returns\n",
       "\n",
       "An often-discussed limitation in scaling LLMs is the phenomenon of diminishing returns. While enlarging model size and complexity can lead to improvements in performance and capability, these gains tend to decrease as models become exceedingly large. This diminishing return in performance enhancement raises questions about the efficacy of continued scaling. Researchers and developers must weigh the benefits of marginal performance improvements against the escalating costs and resource demands. This necessitates a strategic approach to model development, focusing not only on size but also on architectural innovations and optimization techniques that can yield more efficient models.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The challenges of scaling LLMs underscore the need for a balanced approach that considers not only the potential for enhanced performance but also the broader implications of increased computational demands and energy usage. As the field progresses, addressing these challenges through innovations in both hardware and software will be crucial to sustainable and equitable advancements in artificial intelligence.\n",
       "\n",
       "---\n",
       "\n",
       "## Future Directions and Research\n",
       "\n",
       "As the field of Large Language Models (LLMs) continues to evolve, several promising directions and innovations are poised to redefine the landscape of LLM scaling laws. Understanding these potential advancements is crucial for researchers and technologists seeking to harness the full potential of these models.\n",
       "\n",
       "### 1. Enhanced Model Efficiency\n",
       "\n",
       "One of the key areas of future research is improving the efficiency of LLMs. Current models require substantial computational resources, which limit their accessibility and scalability. Techniques such as model pruning, quantization, and distillation are likely to see significant advancements, enabling smaller, yet equally powerful, versions of LLMs. Developing models that can learn and adapt with less data and compute will be crucial in democratizing access to these technologies.\n",
       "\n",
       "### 2. Multimodal and Cross-Disciplinary Models\n",
       "\n",
       "The integration of multiple data modalities—such as text, vision, and audio—remains a compelling area of exploration. Future LLMs are expected to seamlessly process and generate content across different formats, enhancing their applicability in diverse fields such as healthcare, education, and entertainment. Cross-disciplinary approaches could lead to hybrid models that combine insights from neuroscience and cognitive science to mimic human-like understanding and reasoning.\n",
       "\n",
       "### 3. Ethical and Responsible AI\n",
       "\n",
       "As LLMs become more powerful, ensuring ethical and responsible use is paramount. Future research will likely focus on developing robust frameworks for bias mitigation, transparency, and explainability. Innovations in this area could lead to the creation of models that not only excel in performance but also adhere to ethical standards, fostering trust and reliability in AI technologies.\n",
       "\n",
       "### 4. Personalized and Adaptive Models\n",
       "\n",
       "The future of LLMs may include the development of personalized models that adapt to individual user needs and preferences. This could involve advancements in techniques such as reinforcement learning and online learning, enabling models to offer tailored interactions. Such personalized LLMs could revolutionize user experiences in virtual assistants, customer service, and personalized content generation.\n",
       "\n",
       "### 5. Theoretical Understanding and New Architectures\n",
       "\n",
       "A deeper theoretical understanding of LLM scaling laws is essential for the next wave of breakthroughs. Future research may uncover new architectures that defy current limitations, offering more efficient parameter scaling and better generalization. Investigations into the fundamental principles of model scaling could lead to the discovery of novel paradigms, potentially unlocking unprecedented capabilities in AI.\n",
       "\n",
       "### 6. Sustainable AI Practices\n",
       "\n",
       "The environmental impact of LLMs is a growing concern, prompting research into sustainable AI practices. Developing energy-efficient algorithms and promoting the use of renewable energy sources in training and deployment processes are likely to be prioritized. Future advancements may include the design of eco-friendly LLMs that balance performance with sustainability considerations.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The future of LLM scaling laws is ripe with potential, driven by innovation across model efficiency, multimodality, ethics, personalization, theoretical advancements, and sustainability. As these areas evolve, they will shape the trajectory of AI, presenting new opportunities and challenges. Researchers and practitioners must continue to collaborate and innovate to fully realize the transformative power of LLMs in society.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this report, we have extensively explored the pivotal role of scaling laws in the development and deployment of large language models (LLMs). The discussions have underscored several key points that collectively highlight the transformative impact of these scaling laws on the field of artificial intelligence.\n",
       "\n",
       "Firstly, scaling laws have provided a robust framework for understanding how increases in model size, dataset size, and computational resources can lead to improved performance in LLMs. This understanding has been crucial in guiding the research and development strategies of leading AI organizations, enabling them to build more capable models that can perform a wide range of tasks with greater accuracy and efficiency.\n",
       "\n",
       "Moreover, the report has detailed how scaling laws have influenced the architecture and training methodologies of LLMs. By adhering to these laws, developers have been able to optimize their models, balancing the trade-offs between model complexity and computational cost. This balance is essential in ensuring that LLMs are not only powerful but also accessible and feasible to deploy in real-world applications.\n",
       "\n",
       "The significance of scaling laws is further highlighted by their impact on the democratization of AI technology. As the understanding of these laws has matured, it has led to the development of more efficient training techniques and model architectures that lower the barrier to entry for smaller organizations and researchers. This democratization fosters a more inclusive AI ecosystem, promoting innovation and collaboration across various sectors.\n",
       "\n",
       "In summary, the scaling laws serve as a cornerstone in the advancement of LLMs, providing a scientific basis for the continuous improvement of AI models. Their influence spans across model design, training efficiency, and the broader accessibility of AI technology. As we look to the future, the principles outlined by these laws will continue to shape the trajectory of AI development, ensuring that LLMs remain at the forefront of technological innovation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(orchestrator_worker.get_state(config).values[\"final_report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-F0L5SJfm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
