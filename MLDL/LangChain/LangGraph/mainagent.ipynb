{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgentState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMainAgent\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprp_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser_idx\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mMainAgent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_documents_simple\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: \u001b[43mAgentState\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[1;32m     28\u001b[0m     query \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m     relevant_documents_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchroma_handler_public\u001b[38;5;241m.\u001b[39msearch_db(\n\u001b[1;32m     30\u001b[0m         query, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectordb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m}, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AgentState' is not defined"
     ]
    }
   ],
   "source": [
    "class MainAgent:\n",
    "    def __init__(self, user_idx, prp_idx, model_type=\"public\", params={}):\n",
    "        self.user_idx = user_idx\n",
    "        self.prp_idx = prp_idx\n",
    "        self.model_type = model_type\n",
    "        self.params = params\n",
    "        self.chroma_handler_public = ChromaHandler(\n",
    "            user_idx=user_idx, prp_idx=prp_idx, file_type=\"public\"\n",
    "        )\n",
    "        self.chroma_handler_private = (\n",
    "            ChromaHandler(user_idx=user_idx, prp_idx=prp_idx, file_type=\"private\")\n",
    "            if model_type == \"private\"\n",
    "            else None\n",
    "        )\n",
    "        self.query_handler = QueryHandler()\n",
    "        self.prompt_handler = PromptHandler()\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        if self.model_type == \"public\":\n",
    "            return ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "        elif self.model_type == \"private\":\n",
    "            return PrivateLLMModel()  # 추후 개발될 private LLM 모델 클래스\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
    "\n",
    "    def retrieve_documents_simple(self, state: AgentState) -> AgentState:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        relevant_documents_db = self.chroma_handler_public.search_db(\n",
    "            query, {\"vectordb\": \"query\"}, k=2\n",
    "        )\n",
    "        return {\"query\": query, \"documents\": list(relevant_documents_db)}\n",
    "\n",
    "    def make_context(self, state: AgentState) -> AgentState:\n",
    "        context_info = self._make_context(\n",
    "            self.chroma_handler_public,\n",
    "            self.chroma_handler_private,\n",
    "            state[\"query\"],\n",
    "            {\"vectordb\": \"query\", \"summary\": \"summary\", \"search\": \"search\"},\n",
    "        )\n",
    "        return {\"query\": state[\"query\"], \"context_info\": context_info}\n",
    "\n",
    "    def create_draft(self, state: AgentState) -> AgentState:\n",
    "        context_info = state[\"context_info\"]\n",
    "        toc_list = eval(self.params[\"toc_list\"])\n",
    "        summarized_history = []\n",
    "        previous_contents = eval(self.params[\"previous_content\"])\n",
    "        draft_content = \"\"\n",
    "\n",
    "        for idx, toc_idx in enumerate(toc_list):\n",
    "            category = self._get_category(toc_idx)\n",
    "            context = context_info[\"context\"]\n",
    "            source = context_info[\"source\"]\n",
    "            prompt_type, temperature = self._get_prompt_details(category)\n",
    "\n",
    "            if context and source:\n",
    "                content = self._generate_content(\n",
    "                    toc_idx,\n",
    "                    context,\n",
    "                    summarized_history,\n",
    "                    previous_contents,\n",
    "                    prompt_type,\n",
    "                    temperature,\n",
    "                )\n",
    "                content = self._evaluate_and_refine_content(category, toc_idx, content)\n",
    "                draft_content += content\n",
    "\n",
    "                if idx != len(toc_list) - 1:\n",
    "                    summarized_history.append(self._summarize_history(content))\n",
    "\n",
    "        return {\"messages\": draft_content}\n",
    "\n",
    "    def synthesize_response_default(\n",
    "        self, state: AgentState, config: RunnableConfig\n",
    "    ) -> AgentState:\n",
    "        prompt_template = \"\"\"\n",
    "        Task: Generate a comprehensive answer based on search results.\n",
    "        Topic: Provided search results\n",
    "        Language: Korean\n",
    "        Length: 100 words or less\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", prompt_template),\n",
    "                (\"human\", \"{query}\"),\n",
    "                (\"system\", \"{context}\"),\n",
    "            ]\n",
    "        )\n",
    "        response_synthesizer = prompt | self.model\n",
    "        synthesized_response = response_synthesizer.stream(\n",
    "            {\n",
    "                \"query\": state[\"query\"],\n",
    "                \"context\": self._format_docs(state[\"context_info\"][\"documents\"]),\n",
    "            },\n",
    "            config,\n",
    "        )\n",
    "        return {\"messages\": list(synthesized_response)}\n",
    "\n",
    "    def save_content(self):\n",
    "        pass\n",
    "\n",
    "    def update_proposal(self):\n",
    "        pass\n",
    "\n",
    "    def export_docx(self):\n",
    "        pass\n",
    "\n",
    "    def export_pdf(self):\n",
    "        pass\n",
    "\n",
    "    def track_status(self):\n",
    "        pass\n",
    "\n",
    "    def _make_context(\n",
    "        self,\n",
    "        chroma_handler_public: ChromaHandler,\n",
    "        chroma_handler_private: ChromaHandler,\n",
    "        query: str,\n",
    "        prompt: dict,\n",
    "    ) -> dict:\n",
    "        source = dict(vectorDB=[], search=[])\n",
    "        context_info = dict(context={}, source=source)\n",
    "\n",
    "        public_docs = chroma_handler_public.search_db(query, prompt[\"vectordb\"], k=2)\n",
    "        private_docs = []\n",
    "        if chroma_handler_private:\n",
    "            private_docs = chroma_handler_private.search_db(\n",
    "                query, prompt[\"vectordb\"], k=2\n",
    "            )\n",
    "\n",
    "        all_docs = public_docs + private_docs\n",
    "\n",
    "        if all_docs:\n",
    "            summarized_chroma_docs = chroma_handler_public.summarize_context(\n",
    "                all_docs, prompt[\"summary\"]\n",
    "            )\n",
    "            context_info[\"context\"][\"vectorDB\"] = summarized_chroma_docs[\"context\"]\n",
    "            context_info[\"source\"][\"vectorDB\"] = summarized_chroma_docs[\"source\"]\n",
    "\n",
    "        search_docs = chroma_handler_public.search_web(query, prompt[\"search\"], k=3)\n",
    "        context_info[\"context\"][\"search\"] = search_docs[\"context\"]\n",
    "        context_info[\"source\"][\"search\"] = search_docs[\"source\"]\n",
    "\n",
    "        return context_info\n",
    "\n",
    "    def _get_category(self, toc_idx: int) -> str:\n",
    "        category_query = self.query_handler.make_category_query(toc_idx)\n",
    "        category_prompt = self.prompt_handler.make_category_prompt(category_query)\n",
    "        category_chatgpt = ChatGPTHandler(\n",
    "            prompt=category_prompt, temperature=0, model_kwargs={\"top_p\": 0.1}\n",
    "        )\n",
    "        category = category_chatgpt.generate()\n",
    "        return category\n",
    "\n",
    "    def _get_prompt_details(self, category: str) -> tuple:\n",
    "        if category == \"계획\":\n",
    "            return \"high\", 0.6\n",
    "        return \"low\", 0\n",
    "\n",
    "    def _generate_content(\n",
    "        self,\n",
    "        toc_idx: int,\n",
    "        context: dict,\n",
    "        summarized_history: list,\n",
    "        previous_contents: list,\n",
    "        prompt_type: str,\n",
    "        temperature: float,\n",
    "    ) -> str:\n",
    "        prompt = self.prompt_handler.make_draft_prompt(\n",
    "            toc_idx,\n",
    "            self.params[\"user_msg\"],\n",
    "            context,\n",
    "            \"\\n\".join(summarized_history),\n",
    "            previous_contents[toc_idx],\n",
    "            prompt_type,\n",
    "        )\n",
    "        draft_chatgpt = ChatGPTHandler(\n",
    "            prompt=prompt, model_name=\"gpt-4o\", temperature=temperature\n",
    "        )\n",
    "        content = draft_chatgpt.generate()\n",
    "        return content\n",
    "\n",
    "    def _evaluate_and_refine_content(\n",
    "        self, category: str, toc_idx: int, content: str\n",
    "    ) -> str:\n",
    "        if category != \"계획\":\n",
    "            agent_handler = AgentHandler(\n",
    "                toc_idx, category, content, self.chroma_handler_public\n",
    "            )\n",
    "            evaluation = agent_handler.start()\n",
    "            final_prompt = self.prompt_handler.make_final_prompt(content, evaluation)\n",
    "            final_chatgpt = ChatGPTHandler(\n",
    "                prompt=final_prompt, model_name=\"gpt-4o\", temperature=0\n",
    "            )\n",
    "            content = final_chatgpt.generate()\n",
    "        return content\n",
    "\n",
    "    def _summarize_history(self, content: str) -> str:\n",
    "        summarized_history_prompt = self.prompt_handler.make_summarized_history_prompt(\n",
    "            content\n",
    "        )\n",
    "        summarized_chatgpt = ChatGPTHandler(\n",
    "            prompt=summarized_history_prompt, temperature=0, model_kwargs={\"top_p\": 0.1}\n",
    "        )\n",
    "        summarized_content = summarized_chatgpt.generate()\n",
    "        return summarized_content\n",
    "\n",
    "    def _format_docs(self, docs: Sequence[Document]) -> str:\n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_string = f\"<doc id='{i+1}'>{doc.page_content}</doc>\"\n",
    "            formatted_docs.append(doc_string)\n",
    "        return \"\\n\".join(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MainAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_graph\u001b[39m(agent: \u001b[43mMainAgent\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StateGraph:\n\u001b[1;32m      2\u001b[0m     workflow \u001b[38;5;241m=\u001b[39m StateGraph(AgentState)\n\u001b[1;32m      3\u001b[0m     workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mretrieve_documents_simple)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MainAgent' is not defined"
     ]
    }
   ],
   "source": [
    "def create_graph(agent: MainAgent) -> StateGraph:\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"retriever\", agent.retrieve_documents_simple)\n",
    "    workflow.add_node(\"context_maker\", agent.make_context)\n",
    "    workflow.add_node(\"draft_creator\", agent.create_draft)\n",
    "    workflow.add_node(\"response_synthesizer\", agent.synthesize_response_default)\n",
    "    workflow.set_conditional_entry_point(route_to_retriever)\n",
    "    workflow.add_conditional_edges(\"retriever\", route_to_context_maker)\n",
    "    workflow.add_conditional_edges(\"context_maker\", route_to_draft_creator)\n",
    "    workflow.add_conditional_edges(\"draft_creator\", route_to_response_synthesizer)\n",
    "    workflow.add_edge(\"response_synthesizer\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "def route_to_retriever(state: AgentState) -> Literal[\"retriever\"]:\n",
    "    return \"retriever\"\n",
    "\n",
    "\n",
    "def route_to_context_maker(state: AgentState) -> Literal[\"context_maker\"]:\n",
    "    return \"context_maker\"\n",
    "\n",
    "\n",
    "def route_to_draft_creator(state: AgentState) -> Literal[\"draft_creator\"]:\n",
    "    return \"draft_creator\"\n",
    "\n",
    "\n",
    "def route_to_response_synthesizer(\n",
    "    state: AgentState, config: RunnableConfig\n",
    ") -> Literal[\"response_synthesizer\"]:\n",
    "    return \"response_synthesizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
