{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비디오에 데이터 증강 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_video(vid, tfms):\n",
    "    seed = random.randint(0,99999)\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list, tfms):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return frames, label\n",
    "        else:\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        for _ in range(CFG['VIDEO_LENGTH']):\n",
    "            _, img = cap.read()\n",
    "            frames.append(img)\n",
    "        frames = aug_video(frames, tfms=self.tfms)\n",
    "        return torch.FloatTensor(np.array(frames)).permute(3, 0, 1, 2)\n",
    "    \n",
    "def aug_video(vid, tfms):\n",
    "    seed = random.randint(0,99999)\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))\n",
    "\n",
    "tfms = A.Compose([\n",
    "            A.Resize(width=CFG['IMG_SIZE'], height=CFG['IMG_SIZE']),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize()\n",
    "            ], p=1)\n",
    "\n",
    "train_dataset = CustomDataset(train['video_path'].values, train['label'].values, tfms=tfms)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2076a387d2eeee7d098e4cccefb973249bebb4fa144919220ce31f67b8bf2cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
