{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### MMDetection 설치","metadata":{"id":"c7XRP9bwSfJu"}},{"cell_type":"code","source":"!pip install mmcv-full\n!git clone https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"id":"xJkO1dPo1-2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 런타임->런타임 다시 시작 후 아래 수행. \nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv","metadata":{"id":"KbAnpNg9Q8dG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PASCAL VOC 2007 데이터 세트 다운로드","metadata":{"id":"tjsrX8SFSnK7"}},{"cell_type":"code","source":"!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n!tar -xvf VOCtrainval_06-Nov-2007.tar > /dev/null 2>&1","metadata":{"id":"rKHMaXju2EOE","executionInfo":{"status":"ok","timestamp":1623899395941,"user_tz":-540,"elapsed":23623,"user":{"displayName":"권철민","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03917677622451543916"}},"outputId":"8f39b2a6-0ae8-4884-e3d2-33c1e3c7ff01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MMDetection은 Mask RCNN을 학습하기 위해서는 COCO 포맷을 가장 선호\n* CocoDataset으로 지정해야만, evaluation 시 mask evaluation 정보 제공.(2021년 6월 기준) https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html\n* Pascal voc 포맷을 Coco 포맷으로 변환할 수 있는 유틸리티를 활용하여 데이터 변환\nhttps://github.com/ISSResearch/Dataset-Converters\n* Dataset converter 패키지가 opencv를 3.4로 downgrade함에 유의","metadata":{"id":"-rvSmtd4SyIW"}},{"cell_type":"code","source":"import cv2\nprint(cv2.__version__)","metadata":{"id":"KJg7Mj7rYIlG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ISSResearch/Dataset-Converters.git\n","metadata":{"id":"4KQXbBV7wBjO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd Dataset-Converters; pip install -r requirements.txt","metadata":{"id":"rk8lq3PNjsig","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/coco_output\n!cd Dataset-Converters;python convert.py --input-folder /kaggle/working/VOCdevkit/VOC2007 --output-folder /kaggle/working/coco_output \\\n                  --input-format VOCSEGM --output-format COCO --copy","metadata":{"id":"2aDFbNkjWQmh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install opencv-python==4.1.2.30","metadata":{"id":"h7xdA8F_ZT3E","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 생성된 Coco Annotation json 파일 살펴 보기","metadata":{"id":"Wh0LZFGwbB_5"}},{"cell_type":"code","source":"!sudo apt-get install jq","metadata":{"id":"TROJ8YQuaOTp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!jq . /kaggle/working/coco_output/annotations/train.json > output.json ","metadata":{"id":"uX7X1MoSaOXM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -200 output.json","metadata":{"id":"OhzkICHXaOaa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail -200 output.json","metadata":{"id":"Jg1nUKl6aOdC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!grep -n 'annotations' output.json","metadata":{"id":"_zTNFwsSakv6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -1600 output.json | tail -400 ","metadata":{"id":"8hp4Ufz7apHW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained 모델 다운로드 및 Config, Dataset설정.","metadata":{"id":"dBldbcCSbUj3"}},{"cell_type":"code","source":"# pretrained weight 모델을 다운로드 받기 위해서 mmdetection/checkpoints 디렉토리를 만듬. \n!cd mmdetection; mkdir checkpoints","metadata":{"id":"0bEeppPQG_sr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -O /kaggle/working/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r101_fpn_1x_coco/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth","metadata":{"id":"KwcBRmjdQDLQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -lia /kaggle/working/mmdetection/checkpoints","metadata":{"id":"Ifz2eS2lBqMJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정. \nconfig_file = '/kaggle/working/mmdetection/configs/mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\ncheckpoint_file = 'kaggle/working/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'","metadata":{"id":"3uTqmO5EQDOC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.coco import CocoDataset\n\n@DATASETS.register_module(force=True)\nclass VOCDataset(CocoDataset):\n    CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n               'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n               'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n               'tvmonitor')","metadata":{"id":"WKPZrx6HQDQe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmcv import Config\n\ncfg = Config.fromfile(config_file)\nprint(cfg.pretty_text)","metadata":{"id":"VrPweafhQDTd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmdet.apis import set_random_seed\n\n# dataset에 대한 환경 파라미터 수정. \ncfg.dataset_type = 'VOCDataset'\ncfg.data_root = '/kaggle/working/coco_output/'\n\n# train, val, test dataset에 대한 type, data_root, ann_file, img_prefix 환경 파라미터 수정. \ncfg.data.train.type = 'VOCDataset'\ncfg.data.train.data_root = '/kaggle/working/coco_output/'\ncfg.data.train.ann_file = 'annotations/train.json'\ncfg.data.train.img_prefix = 'train'\n\ncfg.data.val.type = 'VOCDataset'\ncfg.data.val.data_root = '/kaggle/working/coco_output/'\ncfg.data.val.ann_file = 'annotations/val.json'\ncfg.data.val.img_prefix = 'val'\n\n\n# class의 갯수를 pascal voc로 설정.  수정. \ncfg.model.roi_head.bbox_head.num_classes = 20\ncfg.model.roi_head.mask_head.num_classes = 20\n\n# pretrained 모델\ncfg.load_from = '/kaggle/working/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'\n\n# 학습 weight 파일로 로그를 저장하기 위한 디렉토리 설정. \ncfg.work_dir = './tutorial_exps'\n\n# 학습율 변경 환경 파라미터 설정. \ncfg.optimizer.lr = 0.02 / 8\ncfg.lr_config.warmup = None\ncfg.log_config.interval = 10\n\n# CocoDataset의 경우 metric을 bbox로 설정해야 함.(mAP아님. bbox로 설정하면 mAP를 iou threshold를 0.5 ~ 0.95까지 변경하면서 측정)\ncfg.evaluation.metric = ['bbox', 'segm']\ncfg.evaluation.interval = 12\ncfg.checkpoint_config.interval = 12\n\n# 두번 config를 로드하면 lr_config의 policy가 사라지는 오류로 인하여 설정. \ncfg.lr_config.policy='step'\n# Set seed thus the results are more reproducible\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)","metadata":{"id":"udEntY3Gj8VP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cfg.pretty_text)","metadata":{"id":"KXbuUXRyj8Yt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\n\n# train용 Dataset 생성. \ndatasets = [build_dataset(cfg.data.train)]","metadata":{"id":"wd2Bik8Fj8cD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\nmodel.CLASSES = datasets[0].CLASSES\nprint(model.CLASSES)","metadata":{"id":"u7YF5h7Hj8ft","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os.path as osp\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n# epochs는 config의 runner 파라미터로 지정됨. 기본 12회 \ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","metadata":{"id":"VgxxeD9Jj8ia","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습된 모델을 이용하여 단일 이미지와 Video Inference 수행. ","metadata":{"id":"IEm1hGrzdV0o"}},{"cell_type":"code","source":"from mmdet.apis import show_result_pyplot\n\ncheckpoint_file = '/kaggle/working/tutorial_exps/epoch_12.pth'\n\n# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용. \nmodel_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')\n# BGR Image 사용 \nimg = cv2.imread('/kaggle/working/VOCdevkit/VOC2007/JPEGImages/000007.jpg')\n#model_ckpt.cfg = cfg\n\nresult = inference_detector(model_ckpt, img)\nshow_result_pyplot(model_ckpt, img, result, score_thr=0.5)","metadata":{"id":"IS78SaUh5giE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/data\n!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg","metadata":{"id":"-zQONtMS6bRn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/working/data/beatles01.jpg')\n#model_ckpt.cfg = cfg\n\nresult = inference_detector(model_ckpt, img)\nshow_result_pyplot(model_ckpt, img, result, score_thr=0.5)","metadata":{"id":"nEAdcLjA6bgt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Video Inference 수행\n* get_detected_img()함수를 이용하여 Inference 수행. ","metadata":{"id":"jcX5PLi37cy2"}},{"cell_type":"code","source":"!wget -O /kaggle/working/data/London_Street.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/London_Street.mp4?raw=true","metadata":{"id":"4UKrr1Hfj8nx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nlabels_to_names_seq =  {0:'aeroplane', 1:'bicycle', 2:'bird', 3:'boat', 4:'bottle', 5:'bus', 6:'car',\n               7:'cat', 8:'chair', 9:'cow', 10:'diningtable', 11:'dog', 12:'horse',\n               13:'motorbike', 14:'person', 15:'pottedplant', 16:'sheep', 17:'sofa', 18:'train',\n               19:'tvmonitor'}\n  \ncolors = list(\n    [[0, 255, 0],\n     [0, 0, 255],\n     [255, 0, 0],\n     [0, 255, 255],\n     [255, 255, 0],\n     [255, 0, 255],\n     [80, 70, 180],\n     [250, 80, 190],\n     [245, 145, 50],\n     [70, 150, 250]] )","metadata":{"id":"OuOty1BK7da2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성. \n# 이미 inference 시 mask boolean값이 들어오므로 mask_threshold 값을 필요하지 않음. \ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n    # 인자로 들어온 image_array를 복사. \n    draw_img = img_array.copy()\n    bbox_color=(0, 255, 0)\n    text_color=(0, 0, 255)\n\n    # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음. \n    # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list. \n    results = inference_detector(model, img_array)\n    bbox_results = results[0]\n    seg_results = results[1]\n\n    # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화 \n    # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n    # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐. \n    for result_ind, bbox_result in enumerate(bbox_results):\n        # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행. \n        if len(bbox_result) == 0:\n            continue\n    \n        mask_array_list = seg_results[result_ind]\n    \n        # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출. \n        for i in range(len(bbox_result)):\n            # 좌상단, 우하단 좌표 추출. \n            if bbox_result[i, 4] > score_threshold:\n                left = int(bbox_result[i, 0])\n                top = int(bbox_result[i, 1])\n                right = int(bbox_result[i, 2])\n                bottom = int(bbox_result[i, 3])\n                caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], bbox_result[i, 4])\n                cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n                cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n                # masking 시각화 적용. class_mask_array는 image 크기 shape의  True/False값을 가지는 2차원 array\n                class_mask_array = mask_array_list[i]\n                # 원본 image array에서 mask가 True인 영역만 별도 추출. \n                masked_roi = draw_img[class_mask_array]\n                #color를 임의 지정\n                #color_index = np.random.randint(0, len(colors)-1)\n                # color를 class별로 지정\n                color_index = result_ind % len(colors)\n                color = colors[color_index]\n                # apply_mask()함수를 적용시 수행 시간이 상대적으로 오래 걸림. \n                #draw_img = apply_mask(draw_img, class_mask_array, color, alpha=0.4)\n                # 원본 이미지의 masking 될 영역에 mask를 특정 투명 컬러로 적용\n                draw_img[class_mask_array] = ([0.3*color[0], 0.3*color[1], 0.3*color[2]] + 0.6 * masked_roi).astype(np.uint8)\n                \n                if is_print:\n                    print(caption)\n  \n    return draw_img\n","metadata":{"id":"qgNcOvwm7QbQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n    \n    cap = cv2.VideoCapture(input_path)\n\n    codec = cv2.VideoWriter_fourcc(*'XVID')\n\n    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n\n    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size) \n\n    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('총 Frame 갯수:', frame_cnt)\n    btime = time.time()\n    while True:\n        hasFrame, img_frame = cap.read()\n        if not hasFrame:\n            print('더 이상 처리할 frame이 없습니다.')\n            break\n        stime = time.time()\n        img_frame = get_detected_img(model, img_frame,  score_threshold=score_threshold,is_print=False)\n        if do_print:\n            print('frame별 detection 수행 시간:', round(time.time() - stime, 4))\n        vid_writer.write(img_frame)\n    # end of while loop\n\n    vid_writer.release()\n    cap.release()\n\n    print('최종 detection 완료 수행 시간:', round(time.time() - btime, 4))","metadata":{"id":"rOnTeurI78Ny","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"do_detected_video(model_ckpt, '/kaggle/working/data/London_Street.mp4', '/kaggle/working/London_Street_out01.mp4', score_threshold=0.4, do_print=True)","metadata":{"id":"g3vr964M8EGl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UPJra-WN8IyA"},"execution_count":null,"outputs":[]}]}