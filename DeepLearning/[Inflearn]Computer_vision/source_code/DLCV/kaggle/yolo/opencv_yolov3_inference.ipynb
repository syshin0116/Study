{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### OpenCV Darknet Yolo를 이용하여 이미지Object Detection\n* yolo와 tiny-yolo 를 이용하여 Object Detection","metadata":{"id":"ZVRgTwfuY4TV"}},{"cell_type":"markdown","source":"#### 입력 이미지로 사용될 이미지 다운로드/보기","metadata":{"id":"WiAh0hGQ_LWC"}},{"cell_type":"code","source":"!mkdir /kaggle/working/data\n!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg","metadata":{"id":"8mqbLhc-Y4TY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Darknet Yolo사이트에서 coco로 학습된 Inference모델와 환경파일을 다운로드 받은 후 이를 이용해 OpenCV에서 Inference 모델 생성\n\n* https://pjreddie.com/darknet/yolo/ 에 다운로드 URL 있음.\n* pretrained 모델은 wget https://pjreddie.com/media/files/yolov3.weights 에서 다운로드\n* pretrained 모델을 위한 환경 파일은 https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg 에서 다운로드\n* wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true -O ./yolov3.cfg\n* readNetFromDarknet(config파일, weight파일)로 config파일 인자가 weight파일 인자보다 먼저 옴. 주의 필요. \n\n* tiny yolo의 pretrained된 weight파일은 wget https://pjreddie.com/media/files/yolov3-tiny.weights 에서 download 가능. \n* config 파일은 wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg?raw=true -O ./yolov3-tiny.cfg 로 다운로드","metadata":{"id":"4UhN_QQjY4Tm"}},{"cell_type":"code","source":"### coco 데이터 세트로 pretrained 된 yolo weight 파일과 config 파일 다운로드하여 /kaggle/working/pretrained 디렉토리 아래에 저장. \n!mkdir ./pretrained\n!echo \"##### downloading pretrained yolo/tiny-yolo weight file and config file\"\n!wget -O /kaggle/working/pretrained/yolov3.weights https://pjreddie.com/media/files/yolov3.weights\n!wget -O /kaggle/working/pretrained/yolov3.cfg https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true \n\n!wget -O /kaggle/working/pretrained/yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights\n!wget -O /kaggle/working/pretrained/yolov3-tiny.cfg https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg?raw=true\n\n!ls /kaggle/working/pretrained\n","metadata":{"id":"xJEpALOjaqhI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### readNetFromDarknet(config파일, weight파일)을 이용하여 yolo inference network 모델을 로딩","metadata":{"id":"jwh5Qtx0Y4Tr"}},{"cell_type":"code","source":"import os\nimport cv2\n\nweights_path = '/kaggle/working/pretrained/yolov3.weights'\nconfig_path =  '/kaggle/working/pretrained/yolov3.cfg'\n#config 파일 인자가 먼저 옴. \ncv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)","metadata":{"id":"7SyFlTzWY4Tu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### COCO class id와 class 명 매핑","metadata":{"id":"mYJysoTaY4T1"}},{"cell_type":"code","source":"labels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }","metadata":{"id":"P8DLZ_A2Y4T2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3개의 scale Output layer에서 결과 데이터 추출","metadata":{"id":"x2p5o3ppY4UE"}},{"cell_type":"code","source":"layer_names = cv_net_yolo.getLayerNames()\nprint('### yolo v3 layer name:', layer_names)\nprint('final output layer id:', cv_net_yolo.getUnconnectedOutLayers())\nprint('final output layer name:', [layer_names[i[0] - 1] for i in cv_net_yolo.getUnconnectedOutLayers()])","metadata":{"id":"38bbQIdYAxA6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#전체 Darknet layer에서 13x13 grid, 26x26, 52x52 grid에서 detect된 Output layer만 filtering\nlayer_names = cv_net_yolo.getLayerNames()\noutlayer_names = [layer_names[i[0] - 1] for i in cv_net_yolo.getUnconnectedOutLayers()]\nprint('output_layer name:', outlayer_names)\n\nimg = cv2.imread('./data/beatles01.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# 로딩한 모델은 Yolov3 416 x 416 모델임. 원본 이미지 배열을 사이즈 (416, 416)으로, BGR을 RGB로 변환하여 배열 입력\ncv_net_yolo.setInput(cv2.dnn.blobFromImage(img, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False))\n\n# Object Detection 수행하여 결과를 cvOut으로 반환 \ncv_outs = cv_net_yolo.forward(outlayer_names)\nprint('cv_outs type:', type(cv_outs), 'cv_outs의 내부 원소개수:', len(cv_outs))\nprint(cv_outs[0].shape, cv_outs[1].shape, cv_outs[2].shape)\nprint(cv_outs)\n","metadata":{"id":"N8a87BcFY4UF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3개의 scale output layer에서 Object Detection 정보를 모두 수집. \n* center와 width,height좌표는 모두 좌상단, 우하단 좌표로 변경. ","metadata":{"id":"RPNgIBSoY4UN"}},{"cell_type":"code","source":"import numpy as np\n\n# 원본 이미지를 네트웍에 입력시에는 (416, 416)로 resize 함. \n# 이후 결과가 출력되면 resize된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\nrows = img.shape[0]\ncols = img.shape[1]\n\nconf_threshold = 0.5\nnms_threshold = 0.4\n\n# bounding box의 테두리와 caption 글자색 지정\ngreen_color=(0, 255, 0)\nred_color=(0, 0, 255)\n\nclass_ids = []\nconfidences = []\nboxes = []\n\n# 3개의 개별 output layer별로 Detect된 Object들에 대해서 Detection 정보 추출 및 시각화 \nfor ix, output in enumerate(cv_outs):\n    print('output shape:', output.shape)\n    # feature map에 있는 anchor 갯수만큼 iteration하면서 Detected 된 Object 추출.(13x13x3, 26x26x3, 52x52x3)\n    for jx, detection in enumerate(output):\n        # class score는 detetection배열에서 5번째 이후 위치에 있는 값. \n        class_scores = detection[5:]\n        # class_scores배열에서 가장 높은 값을 가지는 값이 class confidence, 그리고 그때의 위치 인덱스가 class id\n        class_id = np.argmax(class_scores)\n        confidence = class_scores[class_id]\n\n        # confidence가 지정된 conf_threshold보다 작은 값은 제외 \n        if confidence > conf_threshold:\n            print('ix:', ix, 'jx:', jx, 'class_id', class_id, 'confidence:', confidence)\n            # detection은 scale된 좌상단, 우하단 좌표를 반환하는 것이 아니라, detection object의 중심좌표와 너비/높이를 반환\n            # 원본 이미지에 맞게 scale 적용 및 좌상단, 우하단 좌표 계산\n            center_x = int(detection[0] * cols)\n            center_y = int(detection[1] * rows)\n            width = int(detection[2] * cols)\n            height = int(detection[3] * rows)\n            left = int(center_x - width / 2)\n            top = int(center_y - height / 2)\n            # 3개의 개별 output layer별로 Detect된 Object들에 대한 class id, confidence, 좌표정보를 모두 수집\n            class_ids.append(class_id)\n            confidences.append(float(confidence))\n            boxes.append([left, top, width, height])","metadata":{"id":"Ny1mZ8JsY4UP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NMS를 이용하여 각 Output layer에서 Detected된 Object의 겹치는 Bounding box를 제외. ","metadata":{"id":"jLp5YthuY4UU"}},{"cell_type":"code","source":"conf_threshold = 0.5\nnms_threshold = 0.4\nidxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n\nidxs","metadata":{"id":"TGH3xLopY4UV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idxs.flatten()","metadata":{"id":"GyZwFco-KlYO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.","metadata":{"id":"WuDutomPY4Ua"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# cv2의 rectangle()은 인자로 들어온 이미지 배열에 직접 사각형을 업데이트 하므로 그림 표현을 위한 별도의 이미지 배열 생성. \ndraw_img = img.copy()\n\n# NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.\nif len(idxs) > 0:\n    for i in idxs.flatten():\n        box = boxes[i]\n        left = box[0]\n        top = box[1]\n        width = box[2]\n        height = box[3]\n        # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n        caption = \"{}: {:.4f}\".format(labels_to_names_seq[class_ids[i]], confidences[i])\n        #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n        cv2.rectangle(draw_img, (int(left), int(top)), (int(left+width), int(top+height)), color=green_color, thickness=2)\n        cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)\n        print(caption)\n\nimg_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 12))\nplt.imshow(img_rgb)\n","metadata":{"id":"lsHrDsEkY4Ub","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 단일 이미지를 Yolo로 detect하는 get_detected_img() 함수 생성. ","metadata":{"id":"jsFeIbSOY4Uh"}},{"cell_type":"code","source":"def get_detected_img(cv_net, img_array, conf_threshold, nms_threshold, is_print=True):\n    \n    # 원본 이미지를 네트웍에 입력시에는 (416, 416)로 resize 함. \n    # 이후 결과가 출력되면 resize된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n    rows = img_array.shape[0]\n    cols = img_array.shape[1]\n    \n    draw_img = img_array.copy()\n    \n    #전체 Darknet layer에서 13x13 grid, 26x26, 52x52 grid에서 detect된 Output layer만 filtering\n    layer_names = cv_net.getLayerNames()\n    outlayer_names = [layer_names[i[0] - 1] for i in cv_net.getUnconnectedOutLayers()]\n    \n    # 로딩한 모델은 Yolov3 416 x 416 모델임. 원본 이미지 배열을 사이즈 (416, 416)으로, BGR을 RGB로 변환하여 배열 입력\n    cv_net.setInput(cv2.dnn.blobFromImage(img_array, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False))\n    start = time.time()\n    # Object Detection 수행하여 결과를 cvOut으로 반환 \n    cv_outs = cv_net.forward(outlayer_names)\n    layerOutputs = cv_net.forward(outlayer_names)\n    # bounding box의 테두리와 caption 글자색 지정\n    green_color=(0, 255, 0)\n    red_color=(0, 0, 255)\n\n    class_ids = []\n    confidences = []\n    boxes = []\n\n    # 3개의 개별 output layer별로 Detect된 Object들에 대해서 Detection 정보 추출 및 시각화 \n    for ix, output in enumerate(cv_outs):\n        # Detected된 Object별 iteration\n        for jx, detection in enumerate(output):\n            scores = detection[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n            # confidence가 지정된 conf_threshold보다 작은 값은 제외 \n            if confidence > conf_threshold:\n                #print('ix:', ix, 'jx:', jx, 'class_id', class_id, 'confidence:', confidence)\n                # detection은 scale된 좌상단, 우하단 좌표를 반환하는 것이 아니라, detection object의 중심좌표와 너비/높이를 반환\n                # 원본 이미지에 맞게 scale 적용 및 좌상단, 우하단 좌표 계산\n                center_x = int(detection[0] * cols)\n                center_y = int(detection[1] * rows)\n                width = int(detection[2] * cols)\n                height = int(detection[3] * rows)\n                left = int(center_x - width / 2)\n                top = int(center_y - height / 2)\n                # 3개의 개별 output layer별로 Detect된 Object들에 대한 class id, confidence, 좌표정보를 모두 수집\n                class_ids.append(class_id)\n                confidences.append(float(confidence))\n                boxes.append([left, top, width, height])\n    \n    # NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.\n    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n    if len(idxs) > 0:\n        for i in idxs.flatten():\n            box = boxes[i]\n            left = box[0]\n            top = box[1]\n            width = box[2]\n            height = box[3]\n            # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n            caption = \"{}: {:.4f}\".format(labels_to_names_seq[class_ids[i]], confidences[i])\n            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n            cv2.rectangle(draw_img, (int(left), int(top)), (int(left+width), int(top+height)), color=green_color, thickness=2)\n            cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)\n\n    if is_print:\n        print('Detection 수행시간:',round(time.time() - start, 2),\"초\")\n    return draw_img","metadata":{"id":"_RH-zcYKY4Ui","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport time\nimport os\n\n# image 로드 \nimg = cv2.imread('/kaggle/working/data/beatles01.jpg')\n\nweights_path = '/kaggle/working/pretrained/yolov3.weights'\nconfig_path =  '/kaggle/working/pretrained/yolov3.cfg'\n\n# darknet yolo pretrained 모델 로딩\ncv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n    \n","metadata":{"id":"U3zyudBZY4Uo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_threshold = 0.5\nnms_threshold = 0.4\n# Object Detetion 수행 후 시각화 \ndraw_img = get_detected_img(cv_net_yolo, img, conf_threshold=conf_threshold, nms_threshold=nms_threshold, is_print=True)\n\nimg_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(img_rgb)","metadata":{"id":"uMJQefNJGJRR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tiny Yolo로 Object Detection 수행하기. ","metadata":{"id":"BqIoa11rY4VB"}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport time\nimport os\n\n# image 로드 \nimg = cv2.imread('/kaggle/working/data/beatles01.jpg')\n\nweights_path = '/kaggle/working/pretrained/yolov3-tiny.weights'\nconfig_path =  '/kaggle/working/pretrained/yolov3-tiny.cfg'\n\n# darknet tiny yolo pretrained 모델 로딩\ncv_net_yolo_tiny = cv2.dnn.readNetFromDarknet(config_path, weights_path)","metadata":{"id":"VC5ngVUfY4VC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_threshold = 0.5\nnms_threshold = 0.4\n# Object Detetion 수행 후 시각화 \ndraw_img = get_detected_img(cv_net_yolo_tiny, img, conf_threshold=conf_threshold, nms_threshold=nms_threshold, is_print=True)\n\nimg_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(img_rgb)","metadata":{"id":"-0odKbPoGf_N","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"C1WL6DSMN0Kq"},"execution_count":null,"outputs":[]}]}