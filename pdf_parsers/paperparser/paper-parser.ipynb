{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "app_path = os.path.abspath(os.path.join(notebook_dir, \"../\"))\n",
    "if app_path not in sys.path:\n",
    "    sys.path.insert(0, app_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from graphparser.state import GraphState\n",
    "import graphparser_backup.core as parser_core\n",
    "import graphparser_backup.pdf as pdf\n",
    "\n",
    "\n",
    "# 문서 분할\n",
    "split_pdf_node = pdf.SplitPDFFilesNode(batch_size=10)\n",
    "\n",
    "# Layout Analyzer\n",
    "layout_analyze_node = parser_core.LayoutAnalyzerNode(os.environ.get(\"UPSTAGE_API_KEY\"))\n",
    "\n",
    "# 페이지 요소 추출\n",
    "page_element_extractor_node = parser_core.ExtractPageElementsNode()\n",
    "\n",
    "# 이미지 자르기\n",
    "image_cropper_node = parser_core.ImageCropperNode()\n",
    "\n",
    "# 테이블 자르기\n",
    "table_cropper_node = parser_core.TableCropperNode()\n",
    "\n",
    "# 페이지별 텍스트 추출\n",
    "extract_page_text = parser_core.ExtractPageTextNode()\n",
    "\n",
    "# 페이지별 요약\n",
    "page_summary_node = parser_core.CreatePageSummaryNode(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 이미지 요약\n",
    "image_summary_node = parser_core.CreateImageSummaryNode(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 테이블 요약\n",
    "table_summary_node = parser_core.CreateTableSummaryNode(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 테이블 Markdown 추출\n",
    "table_markdown_extractor = parser_core.TableMarkdownExtractorNode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangGraph을 생성\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 정의합니다.\n",
    "workflow.add_node(\"split_pdf_node\", split_pdf_node)\n",
    "workflow.add_node(\"layout_analyzer_node\", layout_analyze_node)\n",
    "workflow.add_node(\"page_element_extractor_node\", page_element_extractor_node)\n",
    "workflow.add_node(\"image_cropper_node\", image_cropper_node)\n",
    "workflow.add_node(\"table_cropper_node\", table_cropper_node)\n",
    "workflow.add_node(\"extract_page_text_node\", extract_page_text)\n",
    "workflow.add_node(\"page_summary_node\", page_summary_node)\n",
    "workflow.add_node(\"image_summary_node\", image_summary_node)\n",
    "workflow.add_node(\"table_summary_node\", table_summary_node)\n",
    "workflow.add_node(\"table_markdown_node\", table_markdown_extractor)\n",
    "\n",
    "# 각 노드들을 연결합니다.\n",
    "workflow.add_edge(\"split_pdf_node\", \"layout_analyzer_node\")\n",
    "workflow.add_edge(\"layout_analyzer_node\", \"page_element_extractor_node\")\n",
    "workflow.add_edge(\"page_element_extractor_node\", \"image_cropper_node\")\n",
    "workflow.add_edge(\"page_element_extractor_node\", \"table_cropper_node\")\n",
    "workflow.add_edge(\"page_element_extractor_node\", \"extract_page_text_node\")\n",
    "workflow.add_edge(\"image_cropper_node\", \"page_summary_node\")\n",
    "workflow.add_edge(\"table_cropper_node\", \"page_summary_node\")\n",
    "workflow.add_edge(\"extract_page_text_node\", \"page_summary_node\")\n",
    "workflow.add_edge(\"page_summary_node\", \"image_summary_node\")\n",
    "workflow.add_edge(\"page_summary_node\", \"table_summary_node\")\n",
    "workflow.add_edge(\"image_summary_node\", END)\n",
    "workflow.add_edge(\"table_summary_node\", \"table_markdown_node\")\n",
    "workflow.add_edge(\"table_markdown_node\", END)\n",
    "\n",
    "workflow.set_entry_point(\"split_pdf_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        Image(app.get_graph(xray=True).draw_mermaid_png())\n",
    "    )  # 실행 가능한 객체의 그래프를 mermaid 형식의 PNG로 그려서 표시합니다. xray=True는 추가적인 세부 정보를 포함합니다.\n",
    "except:\n",
    "    # 이 부분은 추가적인 의존성이 필요하며 선택적으로 실행됩니다.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 링크에서 데이터를 다운로드 받습니다.\n",
    "- [샘플 데이터 링크](https://www.innovation.go.kr/ucms/bbs/B0000051/view.do?nttId=3801&menuNo=300145&searchType=&pageIndex=&searchType=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 설정 파라미터 ###\n",
    "FILEPATH = \"../data/sample_pdf.pdf\"\n",
    "LANGUAGE = \"Korean\"\n",
    "### 설정 파라미터 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100, configurable={\"thread_id\": \"research-paper\"}\n",
    ")\n",
    "\n",
    "# AgentState 객체를 활용하여 질문을 입력합니다.\n",
    "inputs = GraphState(filepath=FILEPATH, language=LANGUAGE)\n",
    "\n",
    "# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "try:\n",
    "    for output in app.stream(inputs, config=config):\n",
    "        # 출력된 결과에서 키와 값을 순회합니다.\n",
    "        for key, value in output.items():\n",
    "            # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "            pprint.pprint(f\"Output from node '{key}':\")\n",
    "            pprint.pprint(\"---\")\n",
    "            # 출력 값을 예쁘게 출력합니다.\n",
    "            pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        # 각 출력 사이에 구분선을 추가합니다.\n",
    "        pprint.pprint(\"\\n---\\n\")\n",
    "except GraphRecursionError as e:\n",
    "    pprint.pprint(f\"Recursion limit reached: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphparser.layout_utils import save_state, load_state\n",
    "\n",
    "# 상태 저장\n",
    "save_state(state, FILEPATH)\n",
    "\n",
    "# 상태 불러오기\n",
    "loaded_state = load_state(FILEPATH)\n",
    "loaded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = loaded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"image_summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state[\"image_summary\"][24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_tag_content(content, tag):\n",
    "    pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
    "    match = re.search(pattern, content, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_non_tag_content(content, tag):\n",
    "    pattern = rf\"<{tag}>.*?</{tag}>\"\n",
    "    result = re.sub(pattern, \"\", content, flags=re.DOTALL)\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image, Table 에서 추출된 데이터 Vector DB 생성을 위한 문서 생성\n",
    "\n",
    "- Title, Summary, Entities 는 임베딩 검색에 걸리기 위한 문서로 생성\n",
    "- hypothetical_questions 는 임베딩 검색에 걸리기 위한 문서로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in state[\"image_summary\"]:\n",
    "    print(\n",
    "        state[\"image_summary\"][i],\n",
    "        end=\"\\n=============================================================================================================================================================\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extract_tag_content(state[\"image_summary\"][24], \"hypothetical_questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extract_non_tag_content(state[\"image_summary\"][24], \"hypothetical_questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"table_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML 파싱을 위해 import 문을 추가합니다\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def convert_to_markdown_table(table_summary):\n",
    "    html = \"<table>\\n\"\n",
    "\n",
    "    # table_summary가 문자열인 경우를 처리합니다\n",
    "    if isinstance(table_summary, str):\n",
    "        # XML 파싱을 사용하여 문자열에서 데이터를 추출합니다\n",
    "        root = ET.fromstring(table_summary)\n",
    "        for child in root:\n",
    "            html += f\"  <tr>\\n    <th>{child.tag}</th>\\n    <td>\"\n",
    "\n",
    "            if child.tag in [\"entities\", \"data_insights\"]:\n",
    "                html += \"<ul>\\n\"\n",
    "                for item in child.text.strip().split(\"\\n- \"):\n",
    "                    if item.strip():\n",
    "                        html += f\"      <li>{item.strip()}</li>\\n\"\n",
    "                html += \"    </ul>\"\n",
    "            elif child.tag == \"hypothetical_questions\":\n",
    "                html += \"<ol>\\n\"\n",
    "                for item in child.text.strip().split(\"\\n\"):\n",
    "                    if item.strip():\n",
    "                        html += f\"      <li>{item.strip()}</li>\\n\"\n",
    "                html += \"    </ol>\"\n",
    "            else:\n",
    "                html += child.text.strip()\n",
    "\n",
    "            html += \"</td>\\n  </tr>\\n\"\n",
    "    else:\n",
    "        # 기존의 딕셔너리 처리 로직을 유지합니다\n",
    "        for key, value in table_summary.items():\n",
    "            html += f\"  <tr>\\n    <th>{key}</th>\\n    <td>\"\n",
    "\n",
    "            if key in [\"entities\", \"data_insights\"]:\n",
    "                html += \"<ul>\\n\"\n",
    "                for item in value.split(\"\\n- \"):\n",
    "                    if item.strip():\n",
    "                        html += f\"      <li>{item.strip()}</li>\\n\"\n",
    "                html += \"    </ul>\"\n",
    "            elif key == \"hypothetical_questions\":\n",
    "                html += \"<ol>\\n\"\n",
    "                for item in value.split(\"\\n\"):\n",
    "                    if item.strip():\n",
    "                        html += f\"      <li>{item.strip()}</li>\\n\"\n",
    "                html += \"    </ol>\"\n",
    "            else:\n",
    "                html += value\n",
    "\n",
    "            html += \"</td>\\n  </tr>\\n\"\n",
    "\n",
    "    html += \"</table>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state[\"table_summary\"][31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown_table = convert_to_markdown_table(state[\"table_summary\"][31])\n",
    "# print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"table_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(content, metadata):\n",
    "    \"\"\"\n",
    "    문서 객체를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        content (str): 문서의 내용\n",
    "        metadata (dict): 문서의 메타데이터\n",
    "\n",
    "    Returns:\n",
    "        Document: 생성된 문서 객체\n",
    "    \"\"\"\n",
    "    return Document(page_content=content, metadata=metadata)\n",
    "\n",
    "\n",
    "def process_image_element(element, state, page_number):\n",
    "    \"\"\"\n",
    "    이미지 요소를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        element (dict): 이미지 요소 정보\n",
    "        state (dict): 현재 상태\n",
    "        page_number (str): 페이지 번호\n",
    "\n",
    "    Returns:\n",
    "        tuple: 마크다운 문자열과 문서 객체 리스트\n",
    "    \"\"\"\n",
    "    image_id = element[\"id\"]\n",
    "    image_summary = state[\"image_summary\"][image_id]\n",
    "    image_path = state[\"images\"][image_id]\n",
    "    image_path_md = f\"![{image_path}]({image_path})\"\n",
    "\n",
    "    # image_summary_md = convert_to_markdown_table(image_summary)\n",
    "    markdown = f\"{image_path_md}\"\n",
    "\n",
    "    image_summary_clean = extract_non_tag_content(\n",
    "        image_summary, \"hypothetical_questions\"\n",
    "    )\n",
    "\n",
    "    docs = [\n",
    "        create_document(\n",
    "            image_summary_clean,\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "                \"page\": page_number,\n",
    "                \"source\": state[\"filepath\"],\n",
    "                \"id\": image_id,\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    hypo_docs = []\n",
    "\n",
    "    hypothetical_questions = extract_tag_content(\n",
    "        image_summary, \"hypothetical_questions\"\n",
    "    )\n",
    "    if hypothetical_questions != None:\n",
    "        hypo_docs.append(\n",
    "            create_document(\n",
    "                hypothetical_questions,\n",
    "                {\n",
    "                    \"type\": \"hypothetical_questions\",\n",
    "                    \"image\": image_path,\n",
    "                    \"summary\": image_summary_clean,\n",
    "                    \"page\": page_number,\n",
    "                    \"source\": state[\"filepath\"],\n",
    "                    \"id\": image_id,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return markdown, docs, hypo_docs\n",
    "\n",
    "\n",
    "def process_table_element(element, state, page_number):\n",
    "    \"\"\"\n",
    "    테이블 요소를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        element (dict): 테이블 요소 정보\n",
    "        state (dict): 현재 상태\n",
    "        page_number (str): 페이지 번호\n",
    "\n",
    "    Returns:\n",
    "        tuple: 마크다운 문자열과 문서 객체\n",
    "    \"\"\"\n",
    "    table_id = element[\"id\"]\n",
    "    table_summary = state[\"table_summary\"][table_id]\n",
    "    table_markdown = state[\"table_markdown\"][table_id]\n",
    "    table_path = state[\"tables\"][table_id]\n",
    "    table_path_md = f\"![{table_path}]({table_path})\"\n",
    "\n",
    "    # table_summary_md = convert_to_markdown_table(table_summary)\n",
    "    markdown = f\"{table_path_md}\\n{table_markdown}\"\n",
    "\n",
    "    table_summary_clean = extract_non_tag_content(\n",
    "        table_summary, \"hypothetical_questions\"\n",
    "    )\n",
    "\n",
    "    docs = [\n",
    "        create_document(\n",
    "            table_summary_clean,\n",
    "            {\n",
    "                \"type\": \"table\",\n",
    "                \"table\": table_path,\n",
    "                \"markdown\": table_markdown,\n",
    "                \"page\": page_number,\n",
    "                \"source\": state[\"filepath\"],\n",
    "                \"id\": table_id,\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    hypo_docs = []\n",
    "\n",
    "    hypothetical_questions = extract_tag_content(\n",
    "        table_summary, \"hypothetical_questions\"\n",
    "    )\n",
    "    if hypothetical_questions != None:\n",
    "        hypo_docs.append(\n",
    "            create_document(\n",
    "                hypothetical_questions,\n",
    "                {\n",
    "                    \"type\": \"hypothetical_questions\",\n",
    "                    \"table\": table_path,\n",
    "                    \"summary\": table_summary_clean,\n",
    "                    \"markdown\": table_markdown,\n",
    "                    \"page\": page_number,\n",
    "                    \"source\": state[\"filepath\"],\n",
    "                    \"id\": table_id,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return markdown, docs, hypo_docs\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def html_to_markdown(html: str) -> str:\n",
    "    \"\"\"\n",
    "    - <h1> 태그나 font-size:22px인 경우 -> H1\n",
    "    - font-size:20px -> H2\n",
    "    - 그 외 -> 일반 텍스트\n",
    "    - <footer>, <header> 태그는 특별히 로그성 메시지로만 남기고 본문에는 추가하지 않는 예시\n",
    "    - <br> 태그는 공백으로 치환하여 자연스러운 연결\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    markdown_lines = []\n",
    "\n",
    "    # 폰트 사이즈를 보고 Heading 레벨을 결정하는 함수\n",
    "    def get_heading_level_from_font_size(font_size: int) -> int:\n",
    "        if font_size == 22:\n",
    "            return 1  # H1\n",
    "        elif font_size == 20:\n",
    "            return 2  # H2\n",
    "        # 다른 폰트 크기는 모두 일반 텍스트\n",
    "        return 0\n",
    "\n",
    "    for child in soup.contents:\n",
    "        # 텍스트 노드거나 태그명이 없으면 건너뜀\n",
    "        if not hasattr(child, \"name\") or child.name is None:\n",
    "            text = child.get_text(strip=True)\n",
    "            if text:\n",
    "                markdown_lines.append(text)\n",
    "            continue\n",
    "\n",
    "        tag_name = child.name.lower()\n",
    "        style_attr = child.get(\"style\", \"\")\n",
    "        match = re.search(r\"font-size:\\s*(\\d+)px\", style_attr)\n",
    "        font_size = int(match.group(1)) if match else 0\n",
    "\n",
    "        # <br> 태그를 공백으로 치환\n",
    "        for br in child.find_all(\"br\"):\n",
    "            br.replace_with(\" \")\n",
    "\n",
    "        # 요소 텍스트 추출\n",
    "        text_content = \" \".join(child.stripped_strings).strip()\n",
    "        if not text_content:\n",
    "            continue\n",
    "\n",
    "        # <h1> 태그인지, 혹은 font-size=22인지 검사\n",
    "        if tag_name == \"h1\":\n",
    "            heading_level = 1\n",
    "        else:\n",
    "            heading_level = get_heading_level_from_font_size(font_size)\n",
    "\n",
    "        # footer, header 태그 처리 (예: 내용은 넣지 않고 로그만 찍는 예시)\n",
    "        if tag_name in [\"footer\", \"header\"]:\n",
    "            # footer나 header 내용은 본문에 포함하지 않는 예시\n",
    "            # 필요하다면 별도 로직으로 markdown_lines.append(...)\n",
    "            # print(f\"[LOG] footer/header tag found: {text_content}\")\n",
    "            continue\n",
    "\n",
    "        # Heading 레벨에 따라 Markdown 변환\n",
    "        if heading_level == 1:\n",
    "            # H1\n",
    "            markdown_lines.append(f\"# {text_content}\")\n",
    "        elif heading_level == 2:\n",
    "            # H2\n",
    "            markdown_lines.append(f\"## {text_content}\")\n",
    "        else:\n",
    "            # 일반 문단\n",
    "            markdown_lines.append(text_content + \"\\n\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "\n",
    "def process_text_element(element: dict) -> str:\n",
    "    \"\"\"\n",
    "    텍스트 요소(예: element[\"content\"][\"html\"])를 위 로직으로 Markdown 변환\n",
    "    \"\"\"\n",
    "    html_content = element[\"content\"][\"html\"]\n",
    "    return html_to_markdown(html_content)\n",
    "\n",
    "\n",
    "def process_page(page, state, page_number, text_splitter):\n",
    "    \"\"\"\n",
    "    페이지를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        page (dict): 페이지 정보\n",
    "        state (dict): 현재 상태\n",
    "        page_number (str): 페이지 번호\n",
    "        text_splitter (RecursiveCharacterTextSplitter): 텍스트 분할기\n",
    "\n",
    "    Returns:\n",
    "        tuple: 마크다운 문자열 리스트와 문서 객체 리스트\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    docs = []\n",
    "    hypo_docs = []\n",
    "    page_texts = []\n",
    "\n",
    "    for element in page[\"elements\"]:\n",
    "        if element[\"category\"] == \"figure\":\n",
    "            markdown, element_docs, hypo_doc = process_image_element(\n",
    "                element, state, page_number\n",
    "            )\n",
    "            markdowns.append(markdown)\n",
    "            docs.extend(element_docs)\n",
    "            hypo_docs.extend(hypo_doc)\n",
    "        elif element[\"category\"] == \"table\":\n",
    "            markdown, element_docs, hypo_doc = process_table_element(\n",
    "                element, state, page_number\n",
    "            )\n",
    "            markdowns.append(markdown)\n",
    "            docs.extend(element_docs)\n",
    "            hypo_docs.extend(hypo_doc)\n",
    "        else:\n",
    "            text = process_text_element(element)\n",
    "            markdowns.append(text)\n",
    "            page_texts.append(text)\n",
    "\n",
    "    page_text = \"\\n\".join(page_texts)\n",
    "    split_texts = text_splitter.split_text(page_text)\n",
    "\n",
    "    text_summary = state[\"text_summary\"][page_number]\n",
    "\n",
    "    docs.append(\n",
    "        create_document(\n",
    "            text_summary,\n",
    "            metadata={\n",
    "                \"type\": \"page_summary\",\n",
    "                \"page\": page_number,\n",
    "                \"source\": state[\"filepath\"],\n",
    "                \"text\": page_text,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for text in split_texts:\n",
    "        docs.append(\n",
    "            create_document(\n",
    "                text,\n",
    "                metadata={\n",
    "                    \"type\": \"text\",\n",
    "                    \"page\": page_number,\n",
    "                    \"source\": state[\"filepath\"],\n",
    "                    \"summary\": text_summary,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return markdowns, docs, hypo_docs\n",
    "\n",
    "\n",
    "def process_document(state):\n",
    "    \"\"\"\n",
    "    전체 문서를 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        tuple: 마크다운 문자열 리스트와 문서 객체 리스트\n",
    "    \"\"\"\n",
    "    markdowns = []\n",
    "    docs = []\n",
    "    hypo_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    for page_number, page in state[\"page_elements\"].items():\n",
    "        page_markdowns, page_docs, page_hypo_docs = process_page(\n",
    "            page, state, page_number, text_splitter\n",
    "        )\n",
    "        markdowns.extend(page_markdowns)\n",
    "        docs.extend(page_docs)\n",
    "        hypo_docs.extend(page_hypo_docs)\n",
    "\n",
    "    return markdowns, docs, hypo_docs\n",
    "\n",
    "\n",
    "markdowns, docs, hypo_docs = process_document(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown 파일로 텍스트 저장\n",
    "with open(FILEPATH.replace(\".pdf\", \".md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(markdowns))\n",
    "\n",
    "print(f\"텍스트가 '{FILEPATH.replace('.pdf', '.md')}' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hypo_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[12].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print(i, d.metadata[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[24].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[3].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[20].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[23].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_docs[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_docs[2].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 범위 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs + hypo_docs\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# bm25 retriever와 faiss retriever를 초기화합니다.\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    all_docs,\n",
    ")\n",
    "bm25_retriever.k = 5  # BM25Retriever의 검색 결과 개수를 1로 설정합니다.\n",
    "\n",
    "embedding = OpenAIEmbeddings()  # OpenAI 임베딩을 사용합니다.\n",
    "\n",
    "chroma_vectorstore = Chroma.from_documents(\n",
    "    all_docs,\n",
    "    embedding,\n",
    ")\n",
    "chroma_vectorstore = chroma_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 앙상블 retriever를 초기화합니다.\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, chroma_vectorstore],\n",
    "    weights=[0.7, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevance Checker 로직을 활용한 중요 정보 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 데이터 모델\n",
    "class GradeRetrievalQuestion(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved documents to the question.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Whether the retrieved context is relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 데이터 모델\n",
    "class GradeRetrievalAnswer(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved documents to the answer.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Whether the retrieved context is relevant to the answer, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class OpenAIRelevanceGrader:\n",
    "    \"\"\"\n",
    "    OpenAI 기반의 관련성 평가기 클래스입니다.\n",
    "\n",
    "    이 클래스는 검색된 문서가 주어진 질문이나 답변과 얼마나 관련이 있는지 평가합니다.\n",
    "    'retrieval-question' 또는 'retrieval-answer' 두 가지 모드로 작동할 수 있습니다.\n",
    "\n",
    "    Attributes:\n",
    "        llm: 사용할 언어 모델 인스턴스\n",
    "        structured_llm_grader: 구조화된 출력을 생성하는 LLM 인스턴스\n",
    "        grader_prompt: 평가에 사용될 프롬프트 템플릿\n",
    "\n",
    "    Args:\n",
    "        llm: 사용할 언어 모델 인스턴스\n",
    "        target (str): 평가 대상 ('retrieval-question' 또는 'retrieval-answer')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, target=\"retrieval-question\"):\n",
    "        \"\"\"\n",
    "        OpenAIRelevanceGrader 클래스의 초기화 메서드입니다.\n",
    "\n",
    "        Args:\n",
    "            llm: 사용할 언어 모델 인스턴스\n",
    "            target (str): 평가 대상 ('retrieval-question' 또는 'retrieval-answer')\n",
    "\n",
    "        Raises:\n",
    "            ValueError: 유효하지 않은 target 값이 제공될 경우 발생\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "\n",
    "        if target == \"retrieval-question\":\n",
    "            self.structured_llm_grader = llm.with_structured_output(\n",
    "                GradeRetrievalQuestion\n",
    "            )\n",
    "        elif target == \"retrieval-answer\":\n",
    "            self.structured_llm_grader = llm.with_structured_output(\n",
    "                GradeRetrievalAnswer\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid target: {target}\")\n",
    "\n",
    "        # 프롬프트\n",
    "        target_variable = (\n",
    "            \"user question\" if target == \"retrieval-question\" else \"answer\"\n",
    "        )\n",
    "        system = f\"\"\"You are a grader assessing relevance of a retrieved document to a {target_variable}. \\n \n",
    "            It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "            If the document contains keyword(s) or semantic meaning related to the {target_variable}, grade it as relevant. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to {target_variable}.\"\"\"\n",
    "\n",
    "        grade_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    f\"Retrieved document: \\n\\n {{context}} \\n\\n {target_variable}: {{input}}\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.grader_prompt = grade_prompt\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"\n",
    "        관련성 평가기를 생성하고 반환합니다.\n",
    "\n",
    "        Returns:\n",
    "            관련성 평가를 수행할 수 있는 체인 객체\n",
    "        \"\"\"\n",
    "\n",
    "        retrieval_grader_oai = self.grader_prompt | self.structured_llm_grader\n",
    "        return retrieval_grader_oai\n",
    "\n",
    "\n",
    "class GroundnessQuestionScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the answer is relevant to the question else answer 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GroundnessAnswerRetrievalScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the answer is relevant to the retrieved document else answer 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GroundnessQuestionRetrievalScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the question is relevant to the retrieved document else answer 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GroundednessChecker:\n",
    "    \"\"\"\n",
    "    GroundednessChecker 클래스는 문서의 정확성을 평가하는 클래스입니다.\n",
    "\n",
    "    이 클래스는 주어진 문서가 정확한지 여부를 평가합니다.\n",
    "    'yes' 또는 'no' 두 가지 중 하나를 반환합니다.\n",
    "\n",
    "    Attributes:\n",
    "        llm (BaseLLM): 사용할 언어 모델 인스턴스\n",
    "        target (str): 평가 대상 ('retrieval-answer', 'question-answer' 또는 'question-retrieval')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, target=\"retrieval-answer\"):\n",
    "        \"\"\"\n",
    "        GroundednessChecker 클래스의 생성자입니다.\n",
    "\n",
    "        Args:\n",
    "            llm (BaseLLM): 사용할 언어 모델 인스턴스\n",
    "            target (str): 평가 대상 ('retrieval-answer', 'question-answer' 또는 'question-retrieval')\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.target = target\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"\n",
    "        정확성 평가를 위한 체인을 생성합니다.\n",
    "\n",
    "        Returns:\n",
    "            Chain: 정확성 평가를 수행할 수 있는 체인 객체\n",
    "        \"\"\"\n",
    "        # 파서\n",
    "        if self.target == \"retrieval-answer\":\n",
    "            llm = self.llm.with_structured_output(GroundnessAnswerRetrievalScore)\n",
    "        elif self.target == \"question-answer\":\n",
    "            llm = self.llm.with_structured_output(GroundnessQuestionScore)\n",
    "        elif self.target == \"question-retrieval\":\n",
    "            llm = self.llm.with_structured_output(GroundnessQuestionRetrievalScore)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid target: {self.target}\")\n",
    "\n",
    "        # 프롬프트 선택\n",
    "        if self.target == \"retrieval-answer\":\n",
    "            template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "                Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "                Here is the answer: {answer} \\n\n",
    "                If the document contains keyword(s) or semantic meaning related to the user answer, grade it as relevant. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the retrieved document is relevant to the answer.\"\"\"\n",
    "            input_vars = [\"context\", \"answer\"]\n",
    "\n",
    "        elif self.target == \"question-answer\":\n",
    "            template = \"\"\"You are a grader assessing whether an answer appropriately addresses the given question. \\n\n",
    "                Here is the question: \\n\\n {question} \\n\\n\n",
    "                Here is the answer: {answer} \\n\n",
    "                If the answer directly addresses the question and provides relevant information, grade it as relevant. \\n\n",
    "                Consider both semantic meaning and factual accuracy in your assessment. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the answer is relevant to the question.\"\"\"\n",
    "            input_vars = [\"question\", \"answer\"]\n",
    "\n",
    "        elif self.target == \"question-retrieval\":\n",
    "            template = \"\"\"You are a grader assessing whether a retrieved document is relevant to the given question. \\n\n",
    "                Here is the question: \\n\\n {question} \\n\\n\n",
    "                Here is the retrieved document: \\n\\n {context} \\n\n",
    "                If the document contains information that could help answer the question, grade it as relevant. \\n\n",
    "                Consider both semantic meaning and potential usefulness for answering the question. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the retrieved document is relevant to the question.\"\"\"\n",
    "            input_vars = [\"question\", \"context\"]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid target: {self.target}\")\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=input_vars,\n",
    "        )\n",
    "\n",
    "        # 체인\n",
    "        chain = prompt | llm\n",
    "        return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundness Checker 생성\n",
    "groundedness_check = OpenAIRelevanceGrader(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"retrieval-question\"\n",
    ").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = ensemble_retriever.invoke(\"다중 회귀 분석 이론이 무엇인가요?\")\n",
    "retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_retrieved_documents(retrieved_documents):\n",
    "    clean_docs = []\n",
    "\n",
    "    for doc in retrieved_documents:\n",
    "        metadata = doc.metadata\n",
    "        new_metadata = {}\n",
    "        content = doc.page_content\n",
    "\n",
    "        # 문서 타입이 'page_summary' 또는 'text'인 경우\n",
    "        if metadata[\"type\"] in [\"page_summary\", \"text\"]:\n",
    "            # 페이지 번호와 소스 정보를 새 메타데이터에 추가\n",
    "            if \"page\" in metadata:\n",
    "                new_metadata[\"page\"] = metadata[\"page\"]\n",
    "            if \"source\" in metadata:\n",
    "                new_metadata[\"source\"] = metadata[\"source\"]\n",
    "            # 'text' 타입인 경우 요약 정보도 추가\n",
    "            if metadata[\"type\"] == \"text\":\n",
    "                # content += f'\\n\\n<summary>{metadata[\"summary\"]}</summary>'\n",
    "                new_metadata[\"summary\"] = metadata[\"summary\"]\n",
    "            clean_docs.append(Document(page_content=content, metadata=new_metadata))\n",
    "\n",
    "        # 문서 타입이 'image'인 경우\n",
    "        elif metadata[\"type\"] == \"image\":\n",
    "            image_path = metadata[\"image\"]\n",
    "            # 페이지 번호와 소스 정보를 새 메타데이터에 추가\n",
    "            if \"page\" in metadata:\n",
    "                new_metadata[\"page\"] = metadata[\"page\"]\n",
    "            if \"source\" in metadata:\n",
    "                new_metadata[\"source\"] = metadata[\"source\"]\n",
    "            # 내용을 마크다운 테이블 형식으로 변환\n",
    "            content = convert_to_markdown_table(content)\n",
    "\n",
    "            clean_docs.append(Document(page_content=content, metadata=new_metadata))\n",
    "\n",
    "        # 문서 타입이 'table'인 경우\n",
    "        elif metadata[\"type\"] == \"table\":\n",
    "            table_path = metadata[\"table\"]\n",
    "            table_markdown = metadata[\"markdown\"]\n",
    "            # 페이지 번호와 소스 정보를 새 메타데이터에 추가\n",
    "            if \"page\" in metadata:\n",
    "                new_metadata[\"page\"] = metadata[\"page\"]\n",
    "            if \"source\" in metadata:\n",
    "                new_metadata[\"source\"] = metadata[\"source\"]\n",
    "            # 내용을 마크다운 테이블 형식으로 변환하고 원본 마크다운과 결합\n",
    "            content = f\"{convert_to_markdown_table(content)}\\n\\n{table_markdown}\"\n",
    "\n",
    "            clean_docs.append(Document(page_content=content, metadata=new_metadata))\n",
    "\n",
    "        # 문서 타입이 'hypothetical_questions'인 경우\n",
    "        elif metadata[\"type\"] == \"hypothetical_questions\":\n",
    "            # 내용을 요약 정보로 대체\n",
    "            content = metadata[\"summary\"]\n",
    "            # 페이지 번호와 소스 정보를 새 메타데이터에 추가\n",
    "            if \"page\" in metadata:\n",
    "                new_metadata[\"page\"] = metadata[\"page\"]\n",
    "            if \"source\" in metadata:\n",
    "                new_metadata[\"source\"] = metadata[\"source\"]\n",
    "            clean_docs.append(Document(page_content=content, metadata=new_metadata))\n",
    "\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 사용 예시\n",
    "# 앙상블 리트리버를 사용하여 질문에 대한 문서 검색\n",
    "retrieved_documents = ensemble_retriever.invoke(\"다중 회귀 분석 이론이 무엇인가요?\")\n",
    "# 검색된 문서를 정제하여 깨끗한 형태로 변환\n",
    "cleaned_documents = clean_retrieved_documents(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in cleaned_documents:\n",
    "    print(doc.page_content)\n",
    "    print(\"---\" * 30)\n",
    "    print(doc.metadata)\n",
    "    print(\"===\" * 30, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_check(question, use_checker=True):\n",
    "    # 질문에 대한 문서를 검색합니다.\n",
    "    retrieved_documents = ensemble_retriever.invoke(question)\n",
    "\n",
    "    # 검색된 문서를 정제합니다.\n",
    "    cleaned_documents = clean_retrieved_documents(retrieved_documents)\n",
    "\n",
    "    filtered_documents = []\n",
    "    if use_checker:\n",
    "        # 검사기를 사용하는 경우, 각 문서의 내용과 질문을 입력으로 준비합니다.\n",
    "        checking_inputs = [\n",
    "            {\"context\": doc.page_content, \"input\": question}\n",
    "            for doc in cleaned_documents\n",
    "        ]\n",
    "\n",
    "        # 준비된 입력을 사용하여 일괄 검사를 수행합니다.\n",
    "        checked_results = groundedness_check.batch(checking_inputs)\n",
    "\n",
    "        # 검사 결과가 'yes'인 문서만 필터링합니다.\n",
    "        filtered_documents = [\n",
    "            doc\n",
    "            for doc, result in zip(cleaned_documents, checked_results)\n",
    "            if result.score == \"yes\"\n",
    "        ]\n",
    "    else:\n",
    "        # 검사기를 사용하지 않는 경우, 모든 정제된 문서를 그대로 사용합니다.\n",
    "        filtered_documents = cleaned_documents\n",
    "\n",
    "    # 필터링된 문서를 반환합니다.\n",
    "    return filtered_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_check(\n",
    "    \"프로그램을 일정한 크기로 나누고, 주기억장치도 동일한 크기로 구획하여, 나뉜 프로그램 조각(세그먼트)을 주기억장치의 각 구획(프레임)에 배치하여 실행하는 방식은 무엇인가요?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "# Direction:\n",
    "Make sure you understand the intent of the question and provide the most appropriate answer.\n",
    "- Ask yourself the context of the question and why the questioner asked it, think about the question, and provide an appropriate answer based on your understanding.\n",
    "2. Select the most relevant content (the key content that directly relates to the question) from the context in which it was retrieved to write your answer.\n",
    "3. Create a concise and logical answer. When creating your answer, don't just list your selections, but rearrange them to fit the context so they flow naturally into paragraphs.\n",
    "4. If you haven't searched for context for the question, or if you've searched for a document but its content isn't relevant to the question, you should say ‘I can't find an answer to that question in the materials I have’.\n",
    "5. Write your answer in a table of key points.\n",
    "6. Your answer must include all sources and page numbers.\n",
    "7. Your answer must be written in Korean.\n",
    "8. Be as detailed as possible in your answer.\n",
    "9. Begin your answer with ‘This answer is based on content found in the document **📚’ and end with ‘**📌 source**’.\n",
    "10. Page numbers should be whole numbers.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "###\n",
    "\n",
    "#Example Format:\n",
    "\n",
    "(brief summary of the answer)\n",
    "(include table if there is a table in the context related to the question)\n",
    "(include image explanation if there is a image in the context related to the question)\n",
    "(detailed answer to the question)\n",
    "\n",
    "출처:\n",
    "[here you only write filename(.pdf only), page]\n",
    "\n",
    "- 파일명.pdf, 192쪽\n",
    "- 파일명.pdf, 192쪽\n",
    "- ...\n",
    "\n",
    "###\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_and_check), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"지곡초등학교 급식실 창고면적이 어떻게 돼?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-loader-oJYlJ6ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
