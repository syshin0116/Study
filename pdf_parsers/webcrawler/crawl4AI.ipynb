{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INIT].... → Running post-installation setup...\u001b[0m\n",
      "\u001b[36m[INIT].... → Installing Playwright browsers...\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Playwright installation completed successfully.\u001b[0m\n",
      "\u001b[36m[INIT].... → Starting database initialization...\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Database initialization completed successfully.\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Post-installation setup completed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!crawl4ai-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
      "\u001b[36m[INIT].... → Crawl4AI 0.4.247\u001b[0m\n",
      "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
      "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 0.54s\u001b[0m\n",
      "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 3.14s\u001b[0m\n",
      "\u001b[36m[SCRAPE].. ◆ Processed https://crawl4ai.com... | Time: 13ms\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m3.15s\u001b[0m\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://labq.kr... | Status: True | Time: 0.07s\n",
      "[COMPLETE] ● https://labq.kr... | Status: True | Total: 0.07s\n",
      "[ ![...](https://labq.kr/assets2/images/logo/LABQ.svg) ](https://labq.kr/</>)\n",
      "[ ![...](https://labq.kr/assets2/images/logo/LABQ.svg) ](https://labq.kr/<index.html>)\n",
      "  * [ 비즈니스 ](https://labq.kr/<#business>)\n",
      "  * [ 서비스 ](https://labq.kr/<#service>)\n",
      "  * [ 솔루션 ](https://labq.kr/<#solution>)\n",
      "  * [ 프로젝트 ](https://labq.kr/<#project>)\n",
      "  * [ 고객사 ](https://labq.kr/<#clients>)\n",
      "  * [ 연혁 ](https://labq.kr/<#history>)\n",
      "  * [ Contact Us ](https://labq.kr/<#contact_>)\n",
      "\n",
      "\n",
      "#  |\n",
      "##  Our Business \n",
      "AI & Big Data\n",
      "기업 맞춤형 솔루션을 위한 추론형 AI와 파인튜닝(Fine-tuning),그리고 생성형 AI와 Retrieval Augmented Generation(RAG) 기술을 기반으로근거 있는 답변(based on retrieval evidence)을 생성하는 모델 개발 전문 기업입니다. \n",
      "![My Image](https://labq.kr/assets2/images/logo/aibuild.svg)\n",
      "##### 추론형·생성형 AI\n",
      "추론형 - 예측/탐지/분류/요약/검출\n",
      "생성형 - 문서작성/챗봇/이미지 생성\n",
      "[ 문의하기 ](https://labq.kr/<#contactus>)\n",
      "![My Image](https://labq.kr/assets2/images/logo/genai.svg)\n",
      "##### AI 솔루션 공급\n",
      "(영상 이상 탐지 모델/문서 요약 모델)\n",
      "[ 문의하기 ](https://labq.kr/<#contactus>)\n",
      "![My Image](https://labq.kr/assets2/images/logo/consulting.svg)\n",
      "##### 데이터 분석 및 IT 컨설팅 \n",
      "(분석/시각화/비즈니스 모델 수립)\n",
      "[ 문의하기 ](https://labq.kr/<#contactus>)\n",
      "##### 문의하기 \n",
      "필요한 서비스가 있으신가요?\n",
      "소속\n",
      "이름 *\n",
      "이메일 *\n",
      "연락처 *\n",
      "문의 내용 *\n",
      "취소 확인\n",
      "##  Service Offerings \n",
      "![My Image](https://labq.kr/assets2/images/제안GPT/proposal1.png)\n",
      "![My Image](https://labq.kr/assets2/images/제안GPT/proposal2.png)\n",
      "![My Image](https://labq.kr/assets2/images/제안GPT/proposal3.png)\n",
      "### PROP\n",
      "#### 제안서 작성 3일이면 끝낼 수 있어요.\n",
      "30분이면 제안서의 초안이 완성돼요. 완성된 초안으로 더 높은 수준의 제안서를 작성해보세요. 제안 요청서 · 회사 정보 · 참고 자료 · 검색 을 기반으로 작성해요. \n",
      "####  |\n",
      "연구개발 계획서에 맞춘 제안서도 작성할 수 있어요. 다양한 정부과제를 위한 자유형식 제안서도 활용해 보세요. \n",
      "#제안서초안 #정부과제제안서 #연구개발계획서 #LLM\n",
      "[ 서비스 이용하기 ](https://labq.kr/<https:/www.prop-ai.kr/>)\n",
      "![My Image](https://labq.kr/assets2/images/제안GPT/page_main.png)\n",
      "##  DeepQ Solution\n",
      "비즈니스에 최적화된 인공지능 솔루션을 제공합니다\n",
      "![My Image](https://labq.kr/assets2/images/logo/cam.svg)\n",
      "DeepQ Anomaly Detection \n",
      "이상징후 탐지 모델\n",
      "![My Image](https://labq.kr/assets2/images/logo/video.svg)\n",
      "DeepQ Recommendation Engine \n",
      "추천 모델\n",
      "![My Image](https://labq.kr/assets2/images/logo/chart.svg)\n",
      "DeepQ prediction Engine \n",
      "예측 모델\n",
      "![My Image](https://labq.kr/assets2/images/logo/data.svg)\n",
      "DeepQ Document Summary \n",
      "문서 요약 모델\n",
      "특허출원\n",
      "#### 정확도 99%\n",
      "이상징후 탐지 모델\n",
      "DeepQ AI Solution Suite(v1.0)은 영상 객체 및 이상탐지,시계열 예측 그리고 추천시스템 모델들의 최소 기능 제품(Minimum Viable Product)으로 구현한프로그램으로 AI 시뮬레이션을 특정 시나리오 기반으로 체험할 수 있는 프로그램입니다.\n",
      "영상 이상 탐지 \n",
      "*주거 및 공용 공간 주제의 공공데이터를 기반으로 주거지 침입, 위협 행위 등의 시나리오에 대한 이상행동 탐지\n",
      "  * CNN기반의 다양한 모델에서부터 최신 SOTA 모델의 도메인 중심 커스터마이징\n",
      "  * Realtime Inference를 위한 모델 경량화 및 최적화\n",
      "  * 데이터와 모델 버전 관리\n",
      "\n",
      "\n",
      "시계열 분석 및 예측\n",
      "*서울시 종로구 부암동의 수위계와 강우계 데이터를 기반으로 수위를 예측하는 시나리오에 대한 시뮬레이션\n",
      "  * 여러 통계적인 기법과 회귀적인 접근에서부터 Transformer 기반의 딥러닝 모델까지 자유로운 적용\n",
      "  * 시계열 데이터 분석에 적합한 시각화 기능을 제공\n",
      "  * 데이터 특징에 적합한 ML / DL 파이프라인 제공\n",
      "\n",
      "\n",
      "추천시스템\n",
      "*영화 시청 데이터를 기반으로 새로운 유저가 선호할 영화를 추천해주는 시나리오에 대한 시뮬레이션\n",
      "  * 다양한 모델과 기법을 적용한 Ensemble 방식의 모델 적용\n",
      "  * 유저의 행동 데이터를 통해 성능이 강화되는 ML / DL 파이프라인 제공 \n",
      "\n",
      "\n",
      "다양성과 편리성을 지향\n",
      "다양한 DNN 모델들을 적용할 수 있는 프로그램으로\n",
      "도메인 지식과 데이터를 기반으로 설계하여 쉬운 사용성을 지향합니다\n",
      "분석 툴과 성능 최적화\n",
      "다양한 모델 분석 보조 도구 제공하며 (전처리, EDA, 시각화, 성능 분석)\n",
      "하이퍼 파라미터를 쉽게 조절하거나\n",
      "최고 성능을 위한 하이퍼 파라미터를 자동 최적화합니다\n",
      "##  Our Projects \n",
      "![포스코E&C](https://labq.kr/assets2/images/project_logo/포스코.png)\n",
      "##### 하자관리 및 기술 추천 알고리즘 개발\n",
      "LLM 커스텀 임베딩 RAG 챗봇 웹\n",
      "프로젝트 소개\n",
      "하자발생 사전 예방과 보수 비용 절감을 위한 기술 추천 알고리즘 개발 및 하자관리 시스템 구축 \n",
      "![KADaP](https://labq.kr/assets2/images/project_logo/katech.png)\n",
      "##### 자료조사 및 보고서 생성 서비스 개발\n",
      "LLM RAG 챗봇 웹\n",
      "프로젝트 소개\n",
      "자동차 도메인 특화 대규모 언어 모델 활용 자료조사 및 보고서 생성 서비스 개발 및 운영 \n",
      "![화성시보건소](https://labq.kr/assets2/images/project_logo/화성시보건소.gif)\n",
      "##### 마음건강 상담 챗봇 서비스 개발\n",
      "LLM RAG 챗봇 웹\n",
      "프로젝트 소개\n",
      "정신건강 증진을 위한 인공지능 기반 상담 챗봇 서비스 개발 \n",
      "![정보통신기술사회](https://labq.kr/assets2/images/project_logo/정보통신기술사회.jpg)\n",
      "##### 정보통신 공사 지식 멘토\n",
      "LLM RAG 챗봇 웹\n",
      "프로젝트 소개\n",
      "FAQ와 정보 보호 법령 기반 신뢰할 수 있는 정보를 제공하는 RAG 기반 챗봇 개발\n",
      "![CareLogue](https://labq.kr/assets2/images/project_logo/carelogue.png)\n",
      "##### 고령자 맞춤형 돌봄 챗봇 서비스 개발\n",
      "LLM RAG 챗봇\n",
      "프로젝트 소개\n",
      "고령자 맞춤형 돌봄 서비스를 위한 LLM 기반 챗봇 서비스 개발 \n",
      "![그린텍](https://labq.kr/assets2/images/logo/그린텍.png)\n",
      "##### 하수처리시설 AI 솔루션 개발\n",
      "데이터 분석 ML 웹 SW품질 인증\n",
      "프로젝트 소개\n",
      "하수 처리 공정 데이터를 기반으로 수질 예측 및 이상치 탐지 솔루션 개발 \n",
      "![나보로스](https://labq.kr/assets2/images/project_logo/navoros.png)\n",
      "##### 드론 이상 비행 탐지 솔루션 개발\n",
      "데이터 분석 데이터 바우처\n",
      "프로젝트 소개\n",
      "드론 로그 데이터를 분석하여 이상 비행을 탐지하는 솔루션 개발\n",
      "![소망병원](https://labq.kr/assets2/images/logo/somang.gif)\n",
      "##### 실시간 위생 감지 및 출결 관리 솔루션 개발\n",
      "CV 웹\n",
      "프로젝트 소개\n",
      "실시간 영상 분석을 통해 객체를 탐지하고 위생/안전 규정 준수 자동 판별 및 출결 관리 솔루션 개발\n",
      "![섹타나인](https://labq.kr/assets2/images/project_logo/secta9ine.png)\n",
      "##### AIoT 기반 에너지 절감 서비스\n",
      "AIoT ML 웹\n",
      "프로젝트 소개\n",
      "AIoT 기술을 활용한 냉난방기 데이터 분석을 통해 효율적인 에너지 절감 서비스 개발\n",
      "![한국건설기술연구원](https://labq.kr/assets2/images/project_logo/한국건설기술연구원.jpg)\n",
      "##### 건축물 데이터 활용 예측 모델 개발\n",
      "데이터 분석 ML\n",
      "프로젝트 소개\n",
      "건축물 전과정평가 데이터 기반 탄소배출 주요 변수 예측 및 탄소배출량 예측모형 개발 \n",
      "![과학기술정보통신부](https://labq.kr/assets2/images/project_logo/과학기술정보통신부.jpg)\n",
      "##### 비점 오염원 유형별 이미지 분류 및 객체 탐지 모델 개발\n",
      "데이터 구축 CV\n",
      "프로젝트 소개\n",
      "비점 오염원 유형별 이미지 객체 탐지 모델/ 이미지 분류 모델 개발, 데이터 활용 계획에 따른 모델 검증 \n",
      "![소망병원](https://labq.kr/assets2/images/logo/somang.gif)\n",
      "##### 우울증 징후 탐지 및 정신건강 모니터링 앱 개발\n",
      "ML 앱\n",
      "서비스 소개\n",
      "인공지능 기반 우울증 징후 탐지 솔루션 개발, 정신건강 모니터링 시스템 앱 개발 및 운영 \n",
      "![...](https://labq.kr/assets2/images/project_logo/엔비디아.png)\n",
      "##### 이미지 및 영상 속 폭행/침입/추행/위기 탐지 시스템 개발\n",
      "CV\n",
      "프로젝트 소개\n",
      "이미지, 영상 데이터 기반 정상 범위에서 벗어나는 폭행/침입/추행/위기 탐지 시스템 개발 \n",
      "##  Our Clients \n",
      "[ ![포스코](https://labq.kr) ](https://labq.kr/<https:/www.poscoenc.com/ko/index.aspx>)\n",
      "[ ![화성시보건소](https://labq.kr) ](https://labq.kr/<https:/www.hscity.go.kr/health/index.do>)\n",
      "[ ![한국자동차연구원](https://labq.kr) ](https://labq.kr/<https:/www.katech.re.kr/>)\n",
      "[ ![그린텍](https://labq.kr) ](https://labq.kr/<https:/greentechinc.co.kr/>)\n",
      "[ ![케어로그](https://labq.kr) ](https://labq.kr/<https:/carelogue.co.kr/main/>)\n",
      "[ ![섹타나인](https://labq.kr) ](https://labq.kr/<http:/www.secta9ine.co.kr/>)\n",
      "[ ![소망](https://labq.kr) ](https://labq.kr/<http:/somangmh.com/>)\n",
      "[ ![나보로스](https://labq.kr) ](https://labq.kr/<https:/navoros.com/>)\n",
      "[ ![정보통신기술사회](https://labq.kr) ](https://labq.kr/<https:/itpe.or.kr/main/ko/index.html>)\n",
      "[ ![검빛경마](https://labq.kr) ](https://labq.kr/<https:/www.gumvit.com/>)\n",
      "[ ![한국건설기술연구원](https://labq.kr) ](https://labq.kr/<https:/www.kict.re.kr/>)\n",
      "[ ![싸이버원](https://labq.kr) ](https://labq.kr/<https:/www.cyberone.kr/>)\n",
      "[ ![전파진흥협회](https://labq.kr) ](https://labq.kr/<https:/www.rapa.or.kr/>)\n",
      "[ ![정보통신산업](https://labq.kr) ](https://labq.kr/<https:/www.nipa.kr/>)\n",
      "[ ![대전테크노파크](https://labq.kr) ](https://labq.kr/<https:/www.djtp.or.kr/>)\n",
      "##  History\n",
      "[2024](https://labq.kr/<#v-pills-2024>) [2023](https://labq.kr/<#v-pills-2023>) [2022](https://labq.kr/<#v-pills-2022>) [2021](https://labq.kr/<#v-pills-2021>) [2020](https://labq.kr/<#v-pills-2020>) [2019](https://labq.kr/<#v-pills-2019>)\n",
      "  * 12월 화성시 LLM 기반 정신건강 상담 서비스 개발\n",
      "  * 10월 케어로그 LLM 기반 시니어돌봄 서비스 개발\n",
      "  * 8월 그린텍아이엔씨 하수처리시설 AI 솔루션 개발\n",
      "  * 7월 섹타나인 민관협력 오픈이노베이션 AIoT 기반 냉난방기 에너지 절감 솔루션 개발 \n",
      "  * 3월 한국자동차연구원 자동차포털 내 LLM 기반 에이전트 개발\n",
      "  * 2월 포스코이앤씨 LLM 기반 하자예방 기술추천서비스 개발 사업 수주\n",
      "  * 1월 제안서 생성 AI 서비스 'PROP' 런칭\n",
      "\n",
      "\n",
      "  * 8월 기술혁신형 중소기업(Inno-Biz) 확인서 취득\n",
      "  * 6월 경영혁신형 중소기업(Inno-Biz) 확인서 취득\n",
      "  * 5월 데이터바우처 지원사업 3개 공급 과제 이행(AI데이터 가공) \n",
      "  * 2월 DeepQ AI Solution Suite v1.0 - GS 인증 1등급\n",
      "\n",
      "\n",
      "  * 5월 데이터바우처 지원사업 3개 과제 공급기업 선정(AI데이터 가공) \n",
      "  * 4월 AI바우처 지원사업 3개 과제 공급기업 선정(AI모델 개발) \n",
      "  * 4월 창업성장기술개발사업 선정(언어모델 기반 대용량 문서자동 요약) \n",
      "  * 3월 글로벌기업협업프로그램 선정(이상징후탐지 시스템)\n",
      "\n",
      "\n",
      "  * 12월 벤처기업 인증\n",
      "  * 12월 AI스마트팩토리 공급기업지정 (제조혁신추진단)\n",
      "  * 12월 NIA 인공지능 데이터활용 경진대회 우수상(AI 영상 자기소개서) \n",
      "  * 2월 데이터바우처 지원사업 6개 과제 공급기업 선정(AI데이터 가공) \n",
      "  * 1월 기업부설연구소 인증 \n",
      "\n",
      "\n",
      "  * 8월 서울시 공공서비스 AI 공모전 입상 (도서 개인화 추천 시스템) \n",
      "  * 6월 NIPA 인공지능 온라인 경진대회 입상(손글씨 탐지 OCR모델) \n",
      "  * 5월 대전테크노파크, 한국전파진흥협회와 MOU체결\n",
      "  * 1월 인공지능, 블록체인 기술융합 연구모임 LabABC 운영\n",
      "\n",
      "\n",
      "  * 12월 도로교통연구원 도로포장 빅데이터 통합관리체계연구\n",
      "  * 12월 (주)랩큐 대전 지사 설립\n",
      "  * 7월 (주)랩큐 법인 설립\n",
      "\n",
      "\n",
      "##  Contact Us\n",
      "###  연락처 \n",
      "Call. [02-6953-8053](https://labq.kr/<tel:02-6953-8053>)\n",
      "Email. [labq@labq.kr](https://labq.kr/<email:labq@labq.kr>)\n",
      "###  주소 \n",
      "07207 서울 영등포구 양평로21길 26, 2301호(아이에스비즈타워) \n",
      "![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/992/429.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/992/430.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/992/431.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/993/429.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/993/430.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/993/431.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/994/429.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/994/430.png)![](https://mts.daumcdn.net/api/v1/tile/PNGSD02/v05_zmeba/latest/4/994/431.png)\n",
      "![](https://t1.daumcdn.net/localimg/localimages/07/2018/pc/map/marker_map01.png)\n",
      "[아이에스BIZ타워](https://labq.kr/<https:/map.kakao.com/?urlX=476097.0&urlY=1121396.0&itemId=27050634&q=%EC%95%84%EC%9D%B4%EC%97%90%EC%8A%A4BIZ%ED%83%80%EC%9B%8C&srcid=27050634&map_type=TYPE_MAP&from=roughmap>)\n",
      "100m\n",
      "[![Kakao 맵으로 이동\\(새창열림\\)](https://t1.daumcdn.net/mapjsapi/images/m_bi_b.png)](https://labq.kr/<http:/map.kakao.com/>)\n",
      "확대축소초기화\n",
      "[![카카오맵](https://t1.daumcdn.net/localimg/localimages/07/2018/pc/common/logo_kakaomap.png)](https://labq.kr/<https:/map.kakao.com>)\n",
      "[로드뷰](https://labq.kr/<https:/map.kakao.com/?from=roughmap&srcid=27050634&confirmid=27050634&q=%EC%95%84%EC%9D%B4%EC%97%90%EC%8A%A4BIZ%ED%83%80%EC%9B%8C&rv=on>)[길찾기](https://labq.kr/<https:/map.kakao.com/?from=roughmap&eName=%EC%95%84%EC%9D%B4%EC%97%90%EC%8A%A4BIZ%ED%83%80%EC%9B%8C&eX=476097.0&eY=1121396.0>)[지도 크게 보기](https://labq.kr/<https:/map.kakao.com?map_type=TYPE_MAP&from=roughmap&srcid=27050634&itemId=27050634&q=%EC%95%84%EC%9D%B4%EC%97%90%EC%8A%A4BIZ%ED%83%80%EC%9B%8C&urlX=476097.0&urlY=1121396.0>)\n",
      "문의할 사항이 있으신가요?\n",
      "소속\n",
      "이름 *\n",
      "이메일 *\n",
      "연락처 *\n",
      "문의 내용 *\n",
      "확인\n",
      "(주)랩큐 07207 서울 영등포구 양평로21길 26, 2301호(아이에스비즈타워) 대표이사 김한수 /  ✉ labq@labq.kr  [ ☎ 02-6953-8053 ](https://labq.kr/<tel:02-6953-8053>) / 🖷 0504-140-8053 \n",
      "![...](https://labq.kr/assets2/images/logo/LABQ_white.svg)\n",
      "© LabQ. all rights reserved \n",
      "[](https://labq.kr/<#>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import *\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://labq.kr\",\n",
    "        )\n",
    "        print(result.markdown)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://labq.kr... | Status: True | Time: 0.03s\n",
      "[COMPLETE] ● https://labq.kr... | Status: True | Total: 0.04s\n",
      "Raw Markdown length: 9782\n",
      "Fit Markdown length: 2836\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a pruning filter\n",
    "prune_filter = PruningContentFilter(\n",
    "    user_query=\"only filter nav bars\",\n",
    "    # Lower → more content retained, higher → more content pruned\n",
    "    threshold=0.5,           \n",
    "    # \"fixed\" or \"dynamic\"\n",
    "    threshold_type=\"dynamic\",  \n",
    "    # Ignore nodes with <5 words\n",
    "    min_word_threshold=10\n",
    ")\n",
    "\n",
    "# Step 2: Insert it into a Markdown Generator\n",
    "md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n",
    "\n",
    "# Step 3: Pass it to CrawlerRunConfig\n",
    "config = CrawlerRunConfig(\n",
    "    # Content thresholds\n",
    "    word_count_threshold=10,  # Minimum words per block\n",
    "    # Tag exclusions\n",
    "    excluded_tags=[\"form\", \"header\", \"footer\", \"nav\"],\n",
    "    # Link filtering\n",
    "    exclude_external_links=True,\n",
    "    exclude_social_media_links=True,\n",
    "    # Block entire domains\n",
    "    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],\n",
    "    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n",
    "    # Media filtering\n",
    "    exclude_external_images=True,\n",
    ")\n",
    "\n",
    "async with AsyncWebCrawler() as crawler:\n",
    "    result = await crawler.arun(\n",
    "        url=\"https://labq.kr\", \n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    if result.success:\n",
    "        # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n",
    "        print(\"Raw Markdown length:\", len(result.markdown_v2.raw_markdown))\n",
    "        print(\"Fit Markdown length:\", len(result.markdown_v2.fit_markdown))\n",
    "    else:\n",
    "        print(\"Error:\", result.error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기업 맞춤형 솔루션을 위한 추론형 AI와 파인튜닝(Fine-tuning),그리고 생성형 AI와 Retrieval Augmented Generation(RAG) 기술을 기반으로근거 있는 답변(based on retrieval evidence)을 생성하는 모델 개발 전문 기업입니다. \n",
      "(영상 이상 탐지 모델/문서 요약 모델)\n",
      "##### 데이터 분석 및 IT 컨설팅 \n",
      "#### 제안서 작성 3일이면 끝낼 수 있어요.\n",
      "30분이면 제안서의 초안이 완성돼요. 완성된 초안으로 더 높은 수준의 제안서를 작성해보세요. · · · 을 기반으로 작성해요. \n",
      "에 맞춘 제안서도 작성할 수 있어요. 다양한 정부과제를 위한 도 활용해 보세요. \n",
      "비즈니스에 최적화된 인공지능 솔루션을 제공합니다\n",
      "DeepQ AI Solution Suite(v1.0)은 영상 객체 및 이상탐지,시계열 예측 그리고 추천시스템 모델들의 최소 기능 제품(Minimum Viable Product)으로 구현한프로그램으로 AI 시뮬레이션을 특정 시나리오 기반으로 체험할 수 있는 프로그램입니다.\n",
      "영상 이상 탐지 \n",
      "*주거 및 공용 공간 주제의 공공데이터를 기반으로 주거지 침입, 위협 행위 등의 시나리오에 대한 이상행동 탐지\n",
      "  * CNN기반의 다양한 모델에서부터 최신 SOTA 모델의 도메인 중심 커스터마이징\n",
      "  * Realtime Inference를 위한 모델 경량화 및 최적화\n",
      "\n",
      "\n",
      "*서울시 종로구 부암동의 수위계와 강우계 데이터를 기반으로 수위를 예측하는 시나리오에 대한 시뮬레이션\n",
      "  * 여러 통계적인 기법과 회귀적인 접근에서부터 Transformer 기반의 딥러닝 모델까지 자유로운 적용\n",
      "  * 시계열 데이터 분석에 적합한 시각화 기능을 제공\n",
      "  * 데이터 특징에 적합한 ML / DL 파이프라인 제공\n",
      "\n",
      "\n",
      "*영화 시청 데이터를 기반으로 새로운 유저가 선호할 영화를 추천해주는 시나리오에 대한 시뮬레이션\n",
      "  * 다양한 모델과 기법을 적용한 Ensemble 방식의 모델 적용\n",
      "  * 유저의 행동 데이터를 통해 성능이 강화되는 ML / DL 파이프라인 제공 \n",
      "\n",
      "\n",
      "다양한 DNN 모델들을 적용할 수 있는 프로그램으로\n",
      "도메인 지식과 데이터를 기반으로 설계하여 쉬운 사용성을 지향합니다\n",
      "다양한 모델 분석 보조 도구 제공하며 (전처리, EDA, 시각화, 성능 분석)\n",
      "최고 성능을 위한 하이퍼 파라미터를 자동 최적화합니다\n",
      "##### 하자관리 및 기술 추천 알고리즘 개발\n",
      "하자발생 사전 예방과 보수 비용 절감을 위한 기술 추천 알고리즘 개발 및 하자관리 시스템 구축 \n",
      "##### 자료조사 및 보고서 생성 서비스 개발\n",
      "자동차 도메인 특화 대규모 언어 모델 활용 자료조사 및 보고서 생성 서비스 개발 및 운영 \n",
      "##### 마음건강 상담 챗봇 서비스 개발\n",
      "정신건강 증진을 위한 인공지능 기반 상담 챗봇 서비스 개발 \n",
      "FAQ와 정보 보호 법령 기반 신뢰할 수 있는 정보를 제공하는 RAG 기반 챗봇 개발\n",
      "##### 고령자 맞춤형 돌봄 챗봇 서비스 개발\n",
      "고령자 맞춤형 돌봄 서비스를 위한 LLM 기반 챗봇 서비스 개발 \n",
      "데이터 분석 SW품질 인증\n",
      "하수 처리 공정 데이터를 기반으로 수질 예측 및 이상치 탐지 솔루션 개발 \n",
      "##### 드론 이상 비행 탐지 솔루션 개발\n",
      "데이터 분석 데이터 바우처\n",
      "드론 로그 데이터를 분석하여 이상 비행을 탐지하는 솔루션 개발\n",
      "##### 실시간 위생 감지 및 출결 관리 솔루션 개발\n",
      "실시간 영상 분석을 통해 객체를 탐지하고 위생/안전 규정 준수 자동 판별 및 출결 관리 솔루션 개발\n",
      "##### AIoT 기반 에너지 절감 서비스\n",
      "AIoT 기술을 활용한 냉난방기 데이터 분석을 통해 효율적인 에너지 절감 서비스 개발\n",
      "##### 건축물 데이터 활용 예측 모델 개발\n",
      "데이터 분석\n",
      "건축물 전과정평가 데이터 기반 탄소배출 주요 변수 예측 및 탄소배출량 예측모형 개발 \n",
      "##### 비점 오염원 유형별 이미지 분류 및 객체 탐지 모델 개발\n",
      "데이터 구축\n",
      "비점 오염원 유형별 이미지 객체 탐지 모델/ 이미지 분류 모델 개발, 데이터 활용 계획에 따른 모델 검증 \n",
      "##### 우울증 징후 탐지 및 정신건강 모니터링 앱 개발\n",
      "인공지능 기반 우울증 징후 탐지 솔루션 개발, 정신건강 모니터링 시스템 앱 개발 및 운영 \n",
      "##### 이미지 및 영상 속 폭행/침입/추행/위기 탐지 시스템 개발\n",
      "이미지, 영상 데이터 기반 정상 범위에서 벗어나는 폭행/침입/추행/위기 탐지 시스템 개발 \n",
      "  * 화성시 LLM 기반 정신건강 상담 서비스 개발\n",
      "  * 케어로그 LLM 기반 시니어돌봄 서비스 개발\n",
      "  * 그린텍아이엔씨 하수처리시설 AI 솔루션 개발\n",
      "  * 섹타나인 민관협력 오픈이노베이션 AIoT 기반 냉난방기 에너지 절감 솔루션 개발 \n",
      "  * 한국자동차연구원 자동차포털 내 LLM 기반 에이전트 개발\n",
      "  * 포스코이앤씨 LLM 기반 하자예방 기술추천서비스 개발 사업 수주\n",
      "  * 제안서 생성 AI 서비스 'PROP' 런칭\n",
      "\n",
      "\n",
      "  * 데이터바우처 지원사업 3개 공급 과제 이행(AI데이터 가공) \n",
      "  * DeepQ AI Solution Suite v1.0 - GS 인증 1등급\n",
      "\n",
      "\n",
      "  * 데이터바우처 지원사업 3개 과제 공급기업 선정(AI데이터 가공) \n",
      "  * AI바우처 지원사업 3개 과제 공급기업 선정(AI모델 개발) \n",
      "  * 창업성장기술개발사업 선정(언어모델 기반 대용량 문서자동 요약) \n",
      "\n",
      "\n",
      "  * NIA 인공지능 데이터활용 경진대회 우수상(AI 영상 자기소개서) \n",
      "  * 데이터바우처 지원사업 6개 과제 공급기업 선정(AI데이터 가공) \n",
      "\n",
      "\n",
      "  * 서울시 공공서비스 AI 공모전 입상 (도서 개인화 추천 시스템) \n",
      "  * NIPA 인공지능 온라인 경진대회 입상(손글씨 탐지 OCR모델) \n",
      "  * 인공지능, 블록체인 기술융합 연구모임 LabABC 운영\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "07207 서울 영등포구 양평로21길 26, 2301호(아이에스비즈타워) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.markdown_v2.fit_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://www.katech.re.kr/page/24839a36-773a-4809-9... | Status: True | Time: 1.04s\n",
      "[SCRAPE].. ◆ Processed https://www.katech.re.kr/page/24839a36-773a-4809-9... | Time: 34ms\n",
      "[COMPLETE] ● https://www.katech.re.kr/page/24839a36-773a-4809-9... | Status: True | Total: 1.09s\n",
      "Markdown: \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
    "\n",
    "\n",
    "async def main():\n",
    "    config = CrawlerRunConfig(\n",
    "        # css_selector=\"main.content\",\n",
    "        word_count_threshold=10,\n",
    "        excluded_tags=[\"nav\", \"footer\"],\n",
    "        exclude_external_links=True,\n",
    "        exclude_social_media_links=True,\n",
    "        exclude_external_images=True,\n",
    "        cache_mode=CacheMode.BYPASS,\n",
    "        # only_text=True,\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.katech.re.kr/page/24839a36-773a-4809-95ba-175d91a7a774\",\n",
    "            config=config,\n",
    "        )\n",
    "        # print(\"Cleaned HTML length:\", len(result.markdown))\n",
    "        # print(\"Cleaned HTML\", result.cleaned_html)\n",
    "        print(\"Markdown:\", result.markdown_v2.fit_markdown)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.247\n",
      "[FETCH]... ↓ https://www.katech.re.kr/page/14f39e35-6eaf-49fb-a... | Status: True | Time: 0.04s\n",
      "[COMPLETE] ● https://www.katech.re.kr/page/14f39e35-6eaf-49fb-a... | Status: True | Total: 0.06s\n",
      "Raw Markdown length: 22364\n",
      "Fit Markdown length: 468\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fit_markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Markdown length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mmarkdown_v2\u001b[38;5;241m.\u001b[39mraw_markdown))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFit Markdown length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mmarkdown_v2\u001b[38;5;241m.\u001b[39mfit_markdown))\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_markdown\u001b[49m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39merror_message)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'fit_markdown'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
    "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
    "\n",
    "\n",
    "# Step 1: Create a pruning filter\n",
    "prune_filter = PruningContentFilter(\n",
    "    # Lower → more content retained, higher → more content pruned\n",
    "    threshold=0.45,\n",
    "    # \"fixed\" or \"dynamic\"\n",
    "    threshold_type=\"dynamic\",\n",
    "    # Ignore nodes with <5 words\n",
    "    min_word_threshold=5,\n",
    ")\n",
    "\n",
    "# Step 2: Insert it into a Markdown Generator\n",
    "md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n",
    "\n",
    "# Step 3: Pass it to CrawlerRunConfig\n",
    "config = CrawlerRunConfig(markdown_generator=md_generator)\n",
    "\n",
    "async with AsyncWebCrawler() as crawler:\n",
    "    result = await crawler.arun(\n",
    "        url=\"https://www.katech.re.kr/page/14f39e35-6eaf-49fb-a912-9db866f55d5f\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    if result.success:\n",
    "        # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n",
    "        print(\"Raw Markdown length:\", len(result.markdown_v2.raw_markdown))\n",
    "        print(\"Fit Markdown length:\", len(result.markdown_v2.fit_markdown))\n",
    "        print(result.markdown.fit_markdown)\n",
    "    else:\n",
    "        print(\"Error:\", result.error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * \n",
      "\n",
      "[V2X 전자파 및 안테나 기술](https://www.katech.re.kr/page/</page/22cc6c2f-4da1-409f-ad1a-90782f90699d>)\n",
      "실차 계측에 기반한 전장품의 진동 가속 시험 기술\n",
      "  * 진동 가속시험법 개발을 위한 대규모 실차 주행 실측 작업 \n",
      "    * 8개 차종, 15개 전장품을 대상으로 각 차종별로 일반도로(2,200km), 내구시험 PG에서의 진동 데이터 확보 → 전장품별 입력 진동 특성 분류 및 가속 진동 Profile 형상 도출\n",
      "  * 연구용 샘플 및 실 전장품의 한계 내구 시험을 통한 가속 시험법 개발 \n",
      "    * 8종류의 연구용 샘플(총 200개PCB)의 한계 시험 결과를 이용한 가속 계수 결정\n",
      "    * 실전장품 한계 내구 시험으로, 신규 진동 시험법의 유효성 입증 \n",
      "      * 기존 3종류의 현대차 일반 전장품 진동 시험을 1종의 진동내구시험으로 통합\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    crawler = BeautifulSoupCrawler(\n",
    "        # Limit the crawl to max requests. Remove or increase it for crawling all links.\n",
    "        max_requests_per_crawl=10,\n",
    "    )\n",
    "\n",
    "    # Define the default request handler, which will be called for every request.\n",
    "    @crawler.router.default_handler\n",
    "    async def request_handler(context: BeautifulSoupCrawlingContext) -> None:\n",
    "        context.log.info(f\"Processing {context.request.url} ...\")\n",
    "\n",
    "        # Enqueue all links found on the page.\n",
    "        await context.enqueue_links()\n",
    "\n",
    "    # Run the crawler with the initial list of requests.\n",
    "    await crawler.run([\"https://crawlee.dev\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from crawl4ai import *\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n",
      "┌───────────────────────────────┬──────────┐\n",
      "│ requests_finished             │ 0        │\n",
      "│ requests_failed               │ 0        │\n",
      "│ retry_histogram               │ [0]      │\n",
      "│ request_avg_failed_duration   │ None     │\n",
      "│ request_avg_finished_duration │ None     │\n",
      "│ requests_finished_per_minute  │ 0        │\n",
      "│ requests_failed_per_minute    │ 0        │\n",
      "│ request_total_duration        │ 0.0      │\n",
      "│ requests_total                │ 0        │\n",
      "│ crawler_runtime               │ 0.005842 │\n",
      "└───────────────────────────────┴──────────┘\n",
      "[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 2; cpu = 0.0; mem = 0.0; event_loop = 0.0; client_info = 0.0\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Navigating to https://www.daebo.com ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://www.daebo.com ...\n",
      "[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n",
      "┌───────────────────────────────┬──────────┐\n",
      "│ requests_finished             │ 1        │\n",
      "│ requests_failed               │ 0        │\n",
      "│ retry_histogram               │ [1]      │\n",
      "│ request_avg_failed_duration   │ None     │\n",
      "│ request_avg_finished_duration │ 2.150954 │\n",
      "│ requests_finished_per_minute  │ 24       │\n",
      "│ requests_failed_per_minute    │ 0        │\n",
      "│ request_total_duration        │ 2.150954 │\n",
      "│ requests_total                │ 1        │\n",
      "│ crawler_runtime               │ 2.477006 │\n",
      "└───────────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from crawlee.crawlers import (\n",
    "    PlaywrightCrawler,\n",
    "    PlaywrightCrawlingContext,\n",
    "    PlaywrightPreNavCrawlingContext,\n",
    ")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    crawler = PlaywrightCrawler(\n",
    "        # Limit the crawl to max requests. Remove or increase it for crawling all links.\n",
    "        max_requests_per_crawl=10,\n",
    "        # Headless mode, set to False to see the browser in action.\n",
    "        headless=True,\n",
    "        # Browser types supported by Playwright.\n",
    "        browser_type=\"chromium\",\n",
    "    )\n",
    "\n",
    "    # Define the default request handler, which will be called for every request.\n",
    "    # The handler receives a context parameter, providing various properties and\n",
    "    # helper methods. Here are a few key ones we use for demonstration:\n",
    "    # - request: an instance of the Request class containing details such as the URL\n",
    "    #   being crawled and the HTTP method used.\n",
    "    # - page: Playwright's Page object, which allows interaction with the web page\n",
    "    #   (see https://playwright.dev/python/docs/api/class-page for more details).\n",
    "    @crawler.router.default_handler\n",
    "    async def request_handler(context: PlaywrightCrawlingContext) -> None:\n",
    "        context.log.info(f\"Processing {context.request.url} ...\")\n",
    "\n",
    "        # Extract data from the page using Playwright's API.\n",
    "        posts = await context.page.query_selector_all(\".athing\")\n",
    "        data = []\n",
    "\n",
    "        for post in posts:\n",
    "            # Get the HTML elements for the title and rank within each post.\n",
    "            title_element = await post.query_selector(\".title a\")\n",
    "            rank_element = await post.query_selector(\".rank\")\n",
    "\n",
    "            # Extract the data we want from the elements.\n",
    "            title = await title_element.inner_text() if title_element else None\n",
    "            rank = await rank_element.inner_text() if rank_element else None\n",
    "            href = await title_element.get_attribute(\"href\") if title_element else None\n",
    "\n",
    "            data.append({\"title\": title, \"rank\": rank, \"href\": href})\n",
    "\n",
    "        # Push the extracted data to the default dataset. In local configuration,\n",
    "        # the data will be stored as JSON files in ./storage/datasets/default.\n",
    "        await context.push_data(data)\n",
    "\n",
    "        # Find a link to the next page and enqueue it if it exists.\n",
    "        await context.enqueue_links(selector=\".morelink\")\n",
    "\n",
    "    # Define a hook that will be called each time before navigating to a new URL.\n",
    "    # The hook receives a context parameter, providing access to the request and\n",
    "    # browser page among other things. In this example, we log the URL being\n",
    "    # navigated to.\n",
    "    @crawler.pre_navigation_hook\n",
    "    async def log_navigation_url(context: PlaywrightPreNavCrawlingContext) -> None:\n",
    "        context.log.info(f\"Navigating to {context.request.url} ...\")\n",
    "\n",
    "    # Run the crawler with the initial list of URLs.\n",
    "    await crawler.run([\"https://www.daebo.com\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n",
      "┌───────────────────────────────┬──────────┐\n",
      "│ requests_finished             │ 0        │\n",
      "│ requests_failed               │ 0        │\n",
      "│ retry_histogram               │ [0]      │\n",
      "│ request_avg_failed_duration   │ None     │\n",
      "│ request_avg_finished_duration │ None     │\n",
      "│ requests_finished_per_minute  │ 0        │\n",
      "│ requests_failed_per_minute    │ 0        │\n",
      "│ request_total_duration        │ 0.0      │\n",
      "│ requests_total                │ 0        │\n",
      "│ crawler_runtime               │ 0.011197 │\n",
      "└───────────────────────────────┴──────────┘\n",
      "[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 2; cpu = 0.0; mem = 0.0; event_loop = 0.0; client_info = 0.0\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/python/ ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/examples ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/api/core ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/api/core/changelog ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/blog ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/next/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/3.11/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/3.8/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/3.10/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Processing https://crawlee.dev/docs/3.9/quick-start ...\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawler.stop() was called with following reason: The crawler has reached its limit of 10 requests per crawl. .\n",
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  The crawler will finish any remaining ongoing requests and shut down.\n",
      "[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n",
      "┌───────────────────────────────┬───────────┐\n",
      "│ requests_finished             │ 12        │\n",
      "│ requests_failed               │ 0         │\n",
      "│ retry_histogram               │ [12]      │\n",
      "│ request_avg_failed_duration   │ None      │\n",
      "│ request_avg_finished_duration │ 3.784188  │\n",
      "│ requests_finished_per_minute  │ 36        │\n",
      "│ requests_failed_per_minute    │ 0         │\n",
      "│ request_total_duration        │ 45.410259 │\n",
      "│ requests_total                │ 12        │\n",
      "│ crawler_runtime               │ 19.944672 │\n",
      "└───────────────────────────────┴───────────┘\n",
      "[crawlee.storages._dataset] WARN  Attempting to export an empty dataset - no file will be created\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    crawler = PlaywrightCrawler(\n",
    "        # Limit the crawl to max requests. Remove or increase it for crawling all links.\n",
    "        max_requests_per_crawl=10,\n",
    "    )\n",
    "\n",
    "    # Define the default request handler, which will be called for every request.\n",
    "    @crawler.router.default_handler\n",
    "    async def request_handler(context: PlaywrightCrawlingContext) -> None:\n",
    "        context.log.info(f\"Processing {context.request.url} ...\")\n",
    "\n",
    "        # Enqueue all links found on the page.\n",
    "        await context.enqueue_links()\n",
    "\n",
    "    # Run the crawler with the initial list of requests.\n",
    "    await crawler.run([\"https://crawlee.dev\"])\n",
    "    await crawler.export_data_json(path=\"results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-loader-oJYlJ6ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
